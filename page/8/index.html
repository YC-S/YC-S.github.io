<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Pisces',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Yuanchen&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/8/index.html">
<meta property="og:site_name" content="Yuanchen&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Yuanchen">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Yuanchen's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yuanchen's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/09/Learn-to-Code-by-Competitive-Programming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/09/Learn-to-Code-by-Competitive-Programming/" class="post-title-link" itemprop="url">Learn_to_Code_by_Competitive_Programming</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-09 22:20:51" itemprop="dateCreated datePublished" datetime="2020-02-09T22:20:51-05:00">2020-02-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-10 00:24:04" itemprop="dateModified" datetime="2020-02-10T00:24:04-05:00">2020-02-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>How do I Learn to Code? This is probably the most nagging question at the back of your mind, once you have decided that you want to learn programming. Like learning anything else, there is no standard process for learning to code. Of course there are guidelines, there are courses, there are ideologies and there are set traditions, but there is no one single correct way.</p>
<p>One school of thought which is very popular and fairly simple to begin with is <a href="http://en.wikipedia.org/wiki/Competitive_programming" target="_blank" rel="noopener">Competitive Programming</a>. Getting started with it is quite easy and if one devotes sufficient amount of time and effort, you can develop a very strong grasp of programming logic in relatively short amount of time.</p>
<hr>
<p>Here are some steps to get started and be good at it.</p>
<ul>
<li>Get comfortable writing code in either of one of these languages <strong>C, C++ or Java</strong>. Why only C, C++ or Java? Because these are the standard languages allowed in any programming competition.</li>
<li>If you are already good at C, it is suggested to <strong>learn C++</strong>. It is the most popular language among competitive programmers because of its speed and an excellent library in the form of STL (Standard Template Library).</li>
<li>Pick an online judge. Recommended ones are <a href="http://community.topcoder.com/tc" target="_blank" rel="noopener"><strong>Topcoder</strong></a> and <a href="http://codeforces.com/" target="_blank" rel="noopener"><strong>Codeforces</strong></a>. These sites have high quality of problems and also allow you to see other’s code post contest completion. These also categorize problems based on the topic. Some other popular judges include <a href="http://www.spoj.com/" target="_blank" rel="noopener">SPOJ</a>, <a href="http://codechef.com/" target="_blank" rel="noopener">CodeChef</a> (powered by SPOJ) and <a href="http://www.hackerearth.com/" target="_blank" rel="noopener">HackerEarth</a>.</li>
<li>To begin with, <strong>start with simple problems</strong> that typically require transforming English to code and does not require any knowledge on algorithms. Solving <a href="http://community.topcoder.com/tc?module=ProblemArchive&sr=&er=&sc=&sd=&class=&cat=&div1l=&div2l=1&mind1s=&mind2s=&maxd1s=&maxd2s=&wr=" target="_blank" rel="noopener">Div 2 250</a> (Division 2, 250 points) in Topcoder or Div 2 Problem A in Codeforces is a good start.</li>
<li>At the early stages of programming one tends to write long pieces of code, which is actually not required. Try to keep codes <strong>short and simple</strong>.</li>
<li><strong>Practice</strong> these problems until you become comfortable that you can submit it for 240 odd points on any day.</li>
<li>Start implementing basic(or standard) algorithms. It is suggested to read them from <a href="http://community.topcoder.com/tc?module=Static&d1=tutorials&d2=alg_index" target="_blank" rel="noopener">Topcoder tutorials</a> or <a href="http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844" target="_blank" rel="noopener">Introduction to algorithms</a>.</li>
</ul>
<hr>
<p>Some basic concepts that you should learn are</p>
<ol>
<li>Graph algorithms: Breadth first search(BFS), Depth first search(DFS), Strongly connected components(SCC), Dijkstra, Floyd-Warshall, Minimum spanning tree(MST), Topological sort.</li>
<li>Dynamic programming: Standard dynamic programming problems such as Rod Cutting, Knapsack, Matrix chain multiplication etc.</li>
<li>Number theory: Modular arithmetic, Fermat’s theorem, Chinese remainder theorem(CRT), Euclidian method for GCD, Logarithmic Exponentiation, Sieve of Eratosthenes, Euler’s totient function.</li>
<li>Greedy: Standard problems such as Activity selection.</li>
<li>Search techniques: Binary search, Ternary search and Meet in the middle.</li>
<li>Data structures (Basic): Stacks, Queues, Trees and Heaps.</li>
<li>Data structures (Advanced): Trie, Segment trees, Fenwick tree or Binary indexed tree(BIT), Disjoint data structures.</li>
<li>Strings: Knuth Morris Pratt(KMP), Z algorithm, Suffix arrays/Suffix trees. These are bit advanced algorithms.</li>
<li>Computational geometry: Graham-Scan for convex hull, Line sweep.</li>
<li>Game theory: Basic principles of Nim game, Grundy numbers, Sprague-Grundy theorem.</li>
</ol>
<p>The list is not complete but these are the ones that you encounter very frequently in the contests. There are other algorithms but are required very rarely in the contests.</p>
<p>You can find description and implementation of standard algorithms <a href="http://e-maxx.ru/algo/" target="_blank" rel="noopener">here</a>.</p>
<ul>
<li>Once you have sufficient knowledge of popular algorithms, you can start solving the medium level problems. That is Div 2 all problems in Topcoder and Codeforces. It is advisable not to go for Div 1 500 at this point.</li>
<li>Learning to code is all about practicing. <strong>Participate regularly</strong> in the programming contests. Solve the ones that you cannot solve in the contest, after the contest. Apart from Topcoder and Codeforces you can also look at <a href="http://www.hackerearth.com/challenges/" target="_blank" rel="noopener">HackerEarth Challenges</a> or <a href="http://www.codechef.com/contests" target="_blank" rel="noopener">Codechef contests</a>.</li>
<li><strong>Read the codes</strong> of high rated programmers. Compare your solution with them. You can observe that it is simple and shorter than your solution. Analyse how they have approached and improve your implementation skills.</li>
<li><strong>Read the editorials</strong> after the contest. You can learn how to solve the problems that you were not able to solve in the contest and learn alternative ways to solve the problems which you could solve.</li>
<li>Always <strong>practice the problems that you could solve in the contest</strong>. Suppose if you are able to solve Div 2 250 and 500 in the contest but not Div 2 1000 then practice as many Div 2 1000 problems as as you can.</li>
<li><strong>Do not spend too much time</strong> if you are not getting the solution or are stuck somewhere.</li>
<li>After you feel that you have spent enough time, look at the editorials. Understand the algorithm and code it. Do not look at the actual solution before you have attempted to write the code on your own.</li>
<li>Programming is a very practical and hands on skill. You have to continuously do it to be good at it. It’s not enough to solve the problem theoretically, <strong>you have to code it and get the solution accepted</strong>. Knowing which algorithm/logic to use and implementing it are two different things. It takes both to be good at programming.</li>
<li>Programming learning phase is going to take a lot of time and the key is <strong>practicing regularly</strong>. It takes some time before you can attempt Div 1 500 and other tough problems. Do not give up on reading the editorials and implementing them, even if it takes many hours/days. Remember everything requires practice to master it.</li>
</ul>
<p>It takes considerable amount of time before you get good at it. You have to keep yourself motivated throughout. Forming a team and practicing is a good choice. <strong>Not giving up is the key here</strong>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/09/%E8%AF%86%E5%88%AB%E8%A1%A8%E8%B1%A1%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9A%84%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/09/%E8%AF%86%E5%88%AB%E8%A1%A8%E8%B1%A1%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9A%84%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">识别表象和本质的方法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-09 10:32:13 / Modified: 11:42:41" itemprop="dateCreated datePublished" datetime="2020-02-09T10:32:13-05:00">2020-02-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="关于兴趣和投入"><a href="#关于兴趣和投入" class="headerlink" title="关于兴趣和投入"></a>关于兴趣和投入</h1><p>兴趣是学习的助燃剂。对一件事有兴趣是是否愿意对这件事投入更多的前提条件。因此，找到自己的兴趣点的确是非常关键的。不过，我们也能看到下面几点。</p>
<ul>
<li><p><strong>一方面，兴趣是需要保持的</strong>。有的人有的事就是三分钟的兴趣。刚开始兴趣十足，然而时间一长，兴趣因为各种原因不能保持，就会很快地“移情别恋”了。所以，不能持久的兴趣，或是一时兴起的兴趣，都无法让人投入下去。</p>
</li>
<li><p><strong>另一方面，兴趣其实也是可以培养出来的</strong>。有些人对计算机软件毫无兴趣，反而对物理世界里的很多东西非常有兴趣，比如无线电、原子能，或是飞行器之类的。但阴差阳错，最终考了个计算机软件专业，然后发现，自己越来越有兴趣，于是就到了今天。</p>
</li>
</ul>
<p>一个可以持久的兴趣，或是可以培养出来的兴趣，后面都有一个比较本质的东西，那就是你在做这个件事时的一种正反馈，其实就是成就感。也就是说，<strong>兴趣只是开始，而能让人不断投入时间和精力的则是正反馈，是成就感</strong>。</p>
<p>带娃的父母可能对此比较好理解。比如，小孩 3 岁的时候，买了一桶积木给她。她一开始只喜欢把积木胡乱堆，没玩一会就对这种抽象的玩具失去了兴趣，去玩别的更形象的玩具去了。于是，我就搭了一个小城堡给她看，她看完后兴趣就来了，也想自己搭一个。但是，不一会儿，她就受挫了，因为没有掌握好物体在构建时的平衡和支点的方法，所以搭出来的东西会倒。</p>
<p>有时倒了之后，她会从中有一点点的学习总结，但更多的时候总结不出来。于是，就上前帮她做调整，她很快就学会了，并且每一次都比上一次搭得更好……如此反复，最终，小孩玩积木上花的时间大大超过了其它的玩具，直到她无法从中得到成就感。</p>
<p>很显然，把孩子从“天性喜欢破坏的兴趣点”上拉到了“喜欢创造的兴趣点”上。因为创造能带来更多的成就感，不是吗？</p>
<p>所以，你对一件事的兴趣只是一种表象，而内在更多的是你做这件事的成就感是否可以持续。<strong>你需要找到让自己能够更有成就感的事情，兴趣总是可以培养的</strong>。</p>
<h1 id="关于学习和工作"><a href="#关于学习和工作" class="headerlink" title="关于学习和工作"></a>关于学习和工作</h1><p>学习一门语言或者一项技术是否只有找到了相应的工作才学得好。</p>
<p>学好一项技术和是否找到与之相匹配的工作有关联，但它们之间并不是强关联的，因为我们每个人的成长和学习有很多时候是在还没有参加工作的时候。但之所以，我们都觉得通过工作才让我们学习和成长得更快，主要有这些原因。</p>
<ul>
<li>工作中能为我们带来相应的场景和实际的问题，而不是空泛的学习。带着问题去学习，带着场景去解决问题，的确是一种很高效的学习方式。</li>
<li>在工作当中，有同事和高手帮助。和他们的交互和讨论，可以让你更快地学习和成长。</li>
</ul>
<p><strong>本质上来说，并不是只有找到了相应的工作我们才可以学好一项技术，而是，我们在通过解决实际问题，在和他人讨论，获得高手帮助的环境下，能更快更有效率地学习和成长。</strong></p>
<p>有时候，在工作中你反而学不到东西，那是因为你找的这个工作能够提供的场景不够丰富，需要解决的实际问题太过简单，以及你的同事对你的帮助不大。这时，这个工作反而限制了你的学习和成长。</p>
<p>所以，两点。</p>
<ul>
<li>找工作不只是找用这个技术的工作，更是要找场景，找实际问题，找团队。这些才是本质。一项技术很多公司都在用，然而，只有进入到有更多的场景、有挑战性的问题、有靠谱团队的公司，才对学习和成长更有帮助。</li>
<li>不要完全把自己的学习寄希望于找一份工作，才会学得好。在一些开源社区内，有助于学习的场景会更多，要解决的实际问题也更多，同时你能接触到的牛人也更多。特别是一些有大量公司和几万、几十万甚至上百万的开发人员在贡献代码的项目，可以让人成长很快。</li>
</ul>
<p><strong>总之，找到学习的方法，提升自己对新事物学习的能力，才是真正靠谱的。</strong></p>
<h1 id="关于技术和价值"><a href="#关于技术和价值" class="headerlink" title="关于技术和价值"></a>关于技术和价值</h1><p>后面，我们聊到了什么样的技术会是属于未来的技术，以及应该把时间花在什么样的技术上。一个问题：“你觉得，让人登月探索宇宙的技术价值大，还是造高铁的技术价值大？或者是科学种田的技术价值大？……”</p>
<p>是的，对于这个问题，从不同的角度上看，就会得到不同的结论。似乎，我们无法说明白哪项技术创造的价值更大，因为完全没法比较。</p>
<p>于是我又说了一个例子，在第一次工业革命的时候，也就是蒸汽机时代，除了蒸汽机之外还有其它一些技术含量更高的技术，比如化学、冶金、水泥、玻璃……但是，这么一个不起眼的技术引发了人类社会的变革。也许，那个时候，在技术圈中，很多技术专家还鄙视蒸汽机的技术含量太低呢。</p>
<p>我并不是想说高大上的技术无用，我想说的是，技术无贵贱，很多伟大的事就是通过一些不起眼的技术造就的。所以，我们应该关注的是：</p>
<ul>
<li>要用技术解决什么样的问题，场景非常重要；</li>
<li>如何降低技术的学习成本，提高易用性，从而可以让技术更为普及。</li>
</ul>
<p>另外。假设，我们今天没有电，忽然，有人说他发明了电。这个世界上的很多人都会觉得“电”这个东西没什么用，而只有等到“电灯”的发明，人们才明白发明“电”是多么牛。</p>
<p>所以，对于一些“基础技术”来说，通常会在某段时间内被人类社会低估。就像国内前几年低估“云计算”技术一样。基础技术就像是创新的引擎，其不断地成熟和完善会导致更上层的技术不断地衍生，越滚越大。</p>
<p>而在一个基础技术被广泛应用的过程中，如何规模化也会成为一个关键技术。这就好像发电厂一样，没有发电厂，电力就无法做到规模化。记得汽车发明的时候，要组装一个汽车的时间成本、人力成本、物力成本都非常高，所以完全无法做到规模化，而通过模块化分工、自动化生产等技术手段才释放了产能，从而普及。</p>
<p>所以，一项有价值的技术，并不在于这项技术是否有技术含量，而是在于：</p>
<ul>
<li>能否低成本高效率地解决实际问题；</li>
<li>是不是众多产品的基础技术；</li>
<li>是不是可以支持规模化的技术。</li>
</ul>
<p>对于搞计算机软件的人来说，也可以找到相对应的技术点。比如：</p>
<ul>
<li>低成本高效率地解决实际问题的技术，一定是自动化的技术。软件天生就是用来完成重复劳动的，天生就是用来做自动化的。而未来的 AI 和 IoT 也是在拼命数字化和自动化还没有自动化的领域。</li>
<li>基础技术总是枯燥和有价值的。数学、算法、网络、存储等基础技术吃得越透，就越容易服务上层的各种衍生技术或产品。</li>
<li>支持规模化的技术也是很有价值的。在软件行业中，也就是 PaaS 的相关技术。</li>
</ul>
<p>当然，我的意思并不是别的技术都没有价值了。重申一下，<strong>技术无贵贱。我只是想说，能规模化低成本高效率解决实际问题的技术及其基础技术，就算是很 low，也是很有价值的。</strong></p>
<h1 id="关于趋势和未来"><a href="#关于趋势和未来" class="headerlink" title="关于趋势和未来"></a>关于趋势和未来</h1><p>似乎有些规律也是有迹可寻的。</p>
<p><strong>这个世界的技术趋势和未来其实是被人控制的</strong>。就是被那些有权有势有钱的公司或国家来控制的。当然，他们控制的不是长期的未来，但短期的未来（3-5 年）一定是他们控制着的。</p>
<p>也就是说，技术的未来要去哪，主要是看这个世界的投入会到哪。基本上就是这个世界上的有钱有势的人把财富投到哪个领域，也就是这个世界的大公司或大国们的规划。一旦他们把大量的金钱投到某个领域，这个领域就会得到发展，那么发展之后，这个领域也就成为未来了。只要是有一堆公司在往一个方向上不间断地投资或者花钱，这个方向不想成为未来似乎都不可能。</p>
<p>听上去多少有点儿令人沮丧，但世界就是如此简单粗暴运作着的。</p>
<p>所以，对于在这个世界里排不上号的人来说，只能默默地跟随着这些大公司所引领的趋势和未来。对一些缺钱缺人的创业公司，唯一能够做的，也许只是两条路，一是用更为低的成本来提供和大公司相应的技术，另一条路是在细分垂直市场上做得比大公司更专更精。等着自己有一天长大后，也能加入第一梯队从而“引领”未来。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>在我们的生活和工作中，总是会有很多人混淆一些看似有联系，实则却关系不大的词和概念，分辨不清事物的表象和本质。</p>
<p>兴趣和投入。表面上，兴趣是决定一件事儿能否做持久的关键因素。而反观我们自己和他人的经历不难发现，兴趣充演的角色通常是敲门砖，它引发我们关注到某事某物。而真正能让我们坚持下去的，实际上是做一件事之后从中收获到的正反馈，也就是成就感。</p>
<p>同样，人们也经常搞错学习和工作之间的关系。多数人都会认为，在工作中学习和成长速度快。而细细观察下来，就会发现，工作不过是提供了一个能够解决实际问题，能跟人讨论，有高手帮助的环境。所以说，让我们成长的并不是工作本身，而是有利于学习的环境。也就是说，如果我们想学习，除了可以选择有助于学习的工作机会，开源社区提供的环境同样有助于我们的学习和提高，那里高手更多，实际问题不少。</p>
<p>还有，技术和价值。人们通常认为技术含量高的技术其价值会更高，而历史上无数的事实却告诉我们，能规模化、低成本、高效率地解决实际问题的技术及其基础技术，才发挥出了更为深远的影响，甚至其价值更是颠覆性的，难以估量。</p>
<p>趋势和未来也是被误解得很深的一对“孪生兄弟”。虽然大家通常会认为有什么样的技术趋势，必然带来什么样的未来。殊不知，所谓的趋势和未来，其实都是可以由人为控制的，特别是哪些有钱有势的人和公司。也就是，社会的资金和资源流向什么领域，这个领域势必会得到成长和发展，会逐渐形成趋势，进而成为未来。我们遵循这样的规律，就能很容易地判断出未来的，最起码是近几年的，技术流向了。</p>
<p>再如，加班和产出，努力和成功，速度和效率……加班等于高产出吗？显然不是。很努力就一定会成功吗？当然不是。速度快就是效率高吗？更加不是。可以枚举的还有很多，如干得多就等于干得好吗？等等。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/08/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/08/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%85%A5%E9%97%A8/" class="post-title-link" itemprop="url">分布式架构入门</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-08 14:56:10 / Modified: 15:58:49" itemprop="dateCreated datePublished" datetime="2020-02-08T14:56:10-05:00">2020-02-08</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>分布式系统涵盖的面非常广，如下：</p>
<ul>
<li><strong>服务调度</strong>，涉及服务发现、配置管理、弹性伸缩、故障恢复等。</li>
<li><strong>资源调度</strong>，涉及对底层资源的调度使用，如计算资源、网络资源和存储资源等。</li>
<li><strong>流量调度</strong>，涉及路由、负载均衡、流控、熔断等。</li>
<li><strong>数据调度</strong>，涉及数据复本、数据一致性、分布式事务、分库、分表等。</li>
<li><strong>容错处理</strong>，涉及隔离、幂等、重试、业务补偿、异步、降级等。</li>
<li><strong>自动化运维</strong>，涉及持续集成、持续部署、全栈监控、调用链跟踪等。</li>
</ul>
<p>所有这些形成了分布式架构的整体复杂度，也造就了分布式系统中的很多很多论文、图书以及很多很多的项目。要学好分布式系统及其架构，我们需要大量的时间和实践才能真正掌握这些技术。</p>
<p>这里有几点需要你注意一下。</p>
<ul>
<li><strong>分布式系统之所以复杂，就是因为其太容易也太经常出错了</strong>。这意味着，<strong>你要把处理错误的代码当成正常功能的代码来处理</strong>。</li>
<li><strong>开发一个健壮的分布式系统的成本是单体系统的几百倍甚至几万倍</strong>。这意味着，<strong>我们要自己开发一个，需要能力很强的开发人员</strong>。</li>
<li><strong>非常健壮的开源的分布式系统并不多，或者说基本没有</strong>。这意味着，<strong>如果你要用开源的，那么你需要 hold 得住其源码</strong>。</li>
<li><strong>管理或是协调多个服务或机器是非常难的</strong>。这意味着，<strong>我们要去读很多很多的分布式系统的论文</strong>。</li>
<li><strong>在分布式环境下，出了问题是很难 debug 的</strong>。这意味着，<strong>我们需要非常好的监控和跟踪系统，还需要经常做演练和测试</strong>。</li>
<li><strong>在分布式环境下，你需要更科学地分析和统计</strong>。这意味着，<strong>我们要用 P90 这样的统计指标，而不是平均值，我们还需要做容量计划和评估</strong>。</li>
<li><strong>在分布式环境下，需要应用服务化</strong>。这意味着，<strong>我们需要一个服务开发框架，比如 SOA 或微服务</strong>。</li>
<li><strong>在分布式环境下，故障不可怕，可怕的是影响面过大，时间过长</strong>。这意味着，<strong>我们需要花时间来开发我们的自动化运维平台</strong>。</li>
</ul>
<p>总之，在分布式环境下，一切都变得非常复杂。要进入这个领域，你需要有足够多的耐性和足够强的心态来接受各式各样的失败。当拥有丰富的实践和经验后，你才会有所建树。这并不是一日之功，你可能要在这个领域花费数年甚至数十年的时间。</p>
<h2 id="分布式架构入门"><a href="#分布式架构入门" class="headerlink" title="分布式架构入门"></a>分布式架构入门</h2><p>学习如何设计可扩展的架构将会有助于你成为一个更好的工程师。系统设计是一个很宽泛的话题。在互联网上，关于架构设计原则的资源也是多如牛毛。所以，你需要知道一些基本概念，对此，这里你先读一下下面两篇文章，都非常不错。</p>
<ul>
<li><a href="http://www.aosabook.org/en/distsys.html" target="_blank" rel="noopener">Scalable Web Architecture and Distributed Systems</a> ，这篇文章会给你一个大概的分布式架构是怎么来解决系统扩展性问题的粗略方法。</li>
<li><a href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns" target="_blank" rel="noopener">Scalability, Availability &amp; Stability Patterns</a> ，这个 PPT 能在扩展性、可用性、稳定性等方面给你一个非常大的架构设计视野和思想，可以让你感受一下大概的全景图。</li>
</ul>
<p>然后，强烈推荐 GitHub 上的一篇文档 - <a href="https://github.com/donnemartin/system-design-primer" target="_blank" rel="noopener">System Design Primer</a> ，这个仓库主要组织收集分布式系统的一些与扩展性相关的资源，它可以帮助你学习如何构建可扩展的架构。</p>
<p>目前这个仓库收集到了好些系统架构和设计的基本方法。其中包括：CAP 理论、一致性模型、可用性模式、DNS、CDN、负载均衡、反向代理、应用层的微服务和服务发现、关系型数据库和 NoSQL、缓存、异步通讯、安全等。</p>
<p>上面这几篇文章基本足够可以让你入门了，因为其中基本涵盖了所有与系统架构相关的技术。这些技术，足够这世上 90% 以上的公司用了，只有超级巨型的公司才有可能使用更高层次的技术。</p>
<h2 id="分布式理论"><a href="#分布式理论" class="headerlink" title="分布式理论"></a>分布式理论</h2><p>下面学习一下分布式方面的理论知识。</p>
<p>首先，你需要看一下 <a href="https://github.com/aphyr/distsys-class" target="_blank" rel="noopener">An introduction to distributed systems</a>。 这只是某个教学课程的提纲，几乎涵盖了分布式系统方面的所有知识点，而且辅以简洁并切中要害的说明文字，非常适合初学者提纲挈领地了解知识全貌，快速与现有知识结合，形成知识体系。这也是一个分布式系统的知识图谱，可以让你看到分布式系统的整体全貌。你可以根据这个知识图 Google 下去，然后你会学会所有的东西。</p>
<p>然后，你需要了解一下拜占庭将军问题（<a href="https://en.wikipedia.org/wiki/Byzantine_fault_tolerance" target="_blank" rel="noopener">Byzantine Generals Problem</a>）。这个问题是莱斯利·兰波特（Leslie Lamport）于 1982 年提出用来解释一致性问题的一个虚构模型（<a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/The-Byzantine-Generals-Problem.pdf" target="_blank" rel="noopener">论文地址</a>）。拜占庭是古代东罗马帝国的首都，由于地域宽广，守卫边境的多个将军（系统中的多个节点）需要通过信使来传递消息，达成某些一致的决定。但由于将军中可能存在叛徒（系统中节点出错），这些叛徒将努力向不同的将军发送不同的消息，试图会干扰一致性的达成。拜占庭问题即为在此情况下，如何让忠诚的将军们能达成行动的一致。</p>
<p>对于拜占庭问题来说，假如节点总数为 <code>N</code>，叛变将军数为 <code>F</code>，则当 <code>N &gt;= 3F + 1</code> 时，问题才有解，即拜占庭容错（Byzantine Fault Tolerant，BFT）算法。拜占庭容错算法解决的是，网络通信可靠但节点可能故障情况下一致性该如何达成的问题。</p>
<p>最早由卡斯特罗（Castro）和利斯科夫（Liskov）在 1999 年提出的实用拜占庭容错（Practical Byzantine Fault Tolerant，PBFT）算法，是第一个得到广泛应用的 BFT 算法。只要系统中有 2/3 的节点是正常工作的，则可以保证一致性。PBFT 算法包括三个阶段来达成共识：预准备（Pre-Prepare）、准备（Prepare）和提交（Commit）。</p>
<p>这里有几篇和这个问题相关的文章，推荐阅读。</p>
<ul>
<li><a href="http://www.drdobbs.com/cpp/the-byzantine-generals-problem/206904396" target="_blank" rel="noopener">Dr.Dobb’s - The Byzantine Generals Problem</a></li>
<li><a href="http://blog.jameslarisch.com/the-byzantine-generals-problem" target="_blank" rel="noopener">The Byzantine Generals Problem</a></li>
<li><a href="http://pmg.csail.mit.edu/papers/osdi99.pdf" target="_blank" rel="noopener">Practicle Byzantine Fault Tolerance</a></li>
</ul>
<p>拜占庭容错系统研究中有三个重要理论：CAP、FLP 和 DLS。</p>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="noopener">CAP 定理</a>，CAP 理论相信你应该听说过不下 N 次了。CAP 定理是分布式系统设计中最基础也是最为关键的理论。CAP 定理指出，分布式数据存储不可能同时满足以下三个条件：一致性（Consistency）、可用性（Availability）和 分区容忍（Partition tolerance）。 “在网络发生阻断（partition）时，你只能选择数据的一致性（consistency）或可用性（availability），无法两者兼得”。</p>
<p>论点比较直观：如果网络因阻断而分隔为二，在其中一边我送出一笔交易：“将我的十元给 A”；在另一半我送出另一笔交易：” 将我的十元给 B “。此时系统要不是，a）无可用性，即这两笔交易至少会有一笔交易不会被接受；要不就是，b）无一致性，一半看到的是 A 多了十元而另一半则看到 B 多了十元。要注意的是，CAP 理论和扩展性（scalability）是无关的，在分片（sharded）或非分片的系统皆适用。</p>
</li>
<li><p><a href="http://the-paper-trail.org/blog/a-brief-tour-of-flp-impossibility/" target="_blank" rel="noopener">FLP impossibility</a>- 在异步环境中，如果节点间的网络延迟没有上限，只要有一个恶意的节点存在，就没有算法能在有限的时间内达成共识。但值得注意的是， <a href="https://en.wikipedia.org/wiki/Las_Vegas_algorithm" target="_blank" rel="noopener">“Las Vegas” algorithms</a>（这个算法又叫撞大运算法，其保证结果正确，只是在运算时所用资源上进行赌博，一个简单的例子是随机快速排序，它的 pivot 是随机选的，但排序结果永远一致）在每一轮皆有一定机率达成共识，随着时间增加，机率会越趋近于 1。而这也是许多成功的共识算法会采用的解决问题的办法。</p>
</li>
<li><p>容错的上限 - 由 <a href="http://groups.csail.mit.edu/tds/papers/Lynch/jacm88.pdf" target="_blank" rel="noopener">DLS 论文</a> ，我们可以得到以下结论。</p>
<ul>
<li>在部分同步（partially synchronous）的网络环境中（即网络延迟有一定的上限，但我们无法事先知道上限是多少），协议可以容忍最多 1/3 的拜占庭故障（Byzantine fault）。</li>
<li>在异步（asynchronous）的网络环境中，具有确定性质的协议无法容忍任何错误，但这篇论文并没有提及 <a href="http://link.springer.com/chapter/10.1007%2F978-3-540-77444-0_7" target="_blank" rel="noopener">randomized algorithms</a>，在这种情况下可以容忍最多 1/3 的拜占庭故障。</li>
<li>在同步（synchronous）网络环境中（即网络延迟有上限且上限是已知的），协议可以容忍 100% 的拜占庭故障，但当超过 1/2 的节点为恶意节点时，会有一些限制条件。要注意的是，我们考虑的是 “ 具有认证特性的拜占庭模型（authenticated Byzantine）”，而不是 “ 一般的拜占庭模型 “；具有认证特性指的是将如今已经过大量研究且成本低廉的公私钥加密机制应用在我们的算法中。</li>
</ul>
</li>
</ul>
<p>当然，还有一个著名的“8 条荒谬的分布式假设（<a href="http://en.wikipedia.org/wiki/Fallacies_of_distributed_computing" target="_blank" rel="noopener">Fallacies of Distributed Computing</a>）”。</p>
<ol>
<li>网络是稳定的。</li>
<li>网络传输的延迟是零。</li>
<li>网络的带宽是无穷大。</li>
<li>网络是安全的。</li>
<li>网络的拓扑不会改变。</li>
<li>只有一个系统管理员。</li>
<li>传输数据的成本为零。</li>
<li>整个网络是同构的。</li>
</ol>
<p>阿尔农·罗特姆 - 盖尔 - 奥兹（Arnon Rotem-Gal-Oz）写了一篇长文 <a href="http://www.rgoarchitects.com/Files/fallacies.pdf" target="_blank" rel="noopener">Fallacies of Distributed Computing Explained</a> 来解释为什么这些观点是错误的。另外，<a href="http://blog.fogcreek.com/eight-fallacies-of-distributed-computing-tech-talk/" target="_blank" rel="noopener">加勒思·威尔逊（Gareth Wilson）的文章</a> 则用日常生活中的例子，对这些点做了通俗的解释。为什么我们深刻地认识到这 8 个错误？是因为，这要我们清楚地认识到——在分布式系统中错误是不可能避免的，我们在分布式系统中，能做的不是避免错误，而是要把错误的处理当成功能写在代码中。</p>
<p>下面分享几篇一致性方面的论文。</p>
<ul>
<li><p>当然，关于经典的 CAP 理论，也存在一些误导的地方，这个问题在 2012 年有一篇论文 <a href="https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed" target="_blank" rel="noopener">CAP Twelve Years Later: How the Rules Have Changed</a> （<a href="http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed" target="_blank" rel="noopener">中译版</a>）中做了一些讨论，主要是说，在 CAP 中最大的问题就是分区，也就是 P，在 P 发生的情况下，非常难以保证 C 和 A。然而，这是强一致性的情况。</p>
<p>其实，在很多时候，我们并不需要强一致性的系统，所以后来，人们争论关于数据一致性和可用性时，主要是集中在强一致性的 ACID 或最终一致性的 BASE。当时，BASE 还不怎么为世人所接受，主要是大家都觉得 ACID 是最完美的模型，大家很难接受不完美的 BASE。在 CAP 理论中，大家总是觉得需要 “ 三选二 “，也就是说，P 是必选项，那 “ 三选二 “ 的选择题不就变成数据一致性 (consistency)、服务可用性 (availability) 间的 “ 二选一 “ ？</p>
<p>然而，现实却是，P 很少遇到，而 C 和 A 这两个事，工程实践中一致性有不同程度，可用性也有不同等级，在保证分区容错性的前提下，放宽约束后可以兼顾一致性和可用性，两者不是非此即彼。其实，在一个时间可能允许的范围内是可以取舍并交替选择的。</p>
</li>
<li><p><a href="https://pdfs.semanticscholar.org/5015/8bc1a8a67295ab7bce0550886a9859000dc2.pdf" target="_blank" rel="noopener">Harvest, Yield, and Scalable Tolerant Systems</a> ，这篇论文是基于上面那篇 “CAP 12 年后 “ 的论文写的，它主要提出了 Harvest 和 Yield 概念，并把上面那篇论文中所讨论的东西讲得更为仔细了一些。</p>
</li>
<li><p><a href="https://queue.acm.org/detail.cfm?id=1394128" target="_blank" rel="noopener">Base: An Acid Alternative</a> （<a href="http://www.cnblogs.com/savorboard/p/base-an-acid-alternative.html" target="_blank" rel="noopener">中译版</a>），本文是 eBay 的架构师在 2008 年发表给 ACM 的文章，是一篇解释 BASE 原则，或者说最终一致性的经典文章。文中讨论了 BASE 与 ACID 原则的基本差异, 以及如何设计大型网站以满足不断增长的可伸缩性需求，其中有如何对业务做调整和折中，以及一些具体的折中技术的介绍。一个比较经典的话是——“在对数据库进行分区后, 为了可用性（Availability）牺牲部分一致性（Consistency）可以显著地提升系统的可伸缩性 (Scalability)”。</p>
</li>
<li><p><a href="https://www.allthingsdistributed.com/2008/12/eventually_consistent.html" target="_blank" rel="noopener">Eventually Consistent</a> ，这篇文章是 AWS 的 CTO 维尔纳·沃格尔（Werner Vogels）在 2008 年发布在 ACM Queue 上的一篇数据库方面的重要文章，阐述了 NoSQL 数据库的理论基石——最终一致性，对传统的关系型数据库（ACID，Transaction）做了较好的补充。</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/07/how-database-work/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/07/how-database-work/" class="post-title-link" itemprop="url">How database work</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-07 20:31:19 / Modified: 21:33:19" itemprop="dateCreated datePublished" datetime="2020-02-07T20:31:19-05:00">2020-02-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>When it comes to relational databases, I can’t help thinking that something is missing. They’re used everywhere. There are many different databases: from the small and useful SQLite to the powerful Teradata. But, there are only a few articles that explain how a database works. You can google by yourself “how does a relational database work” to see how few results there are. Moreover, those articles are short. Now, if you look for the last trendy technologies (Big Data, NoSQL or JavaScript), you’ll find more in-depth articles explaining how they work.</p>
<p>Are relational databases too old and too boring to be explained outside of university courses, research papers and books?</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/main_databases.jpg" target="_blank" rel="noopener"><img src="../../public/images/main_databases-20200207203251326.jpg" alt="logos of main databases"></a></p>
<p>As a developer, I HATE using something I don’t understand. And, if databases have been used for 40 years, there must be a reason. Over the years, I’ve spent hundreds of hours to really understand these weird black boxes I use every day. <strong>Relational Databases</strong> <strong>are</strong> very interesting because they’re <strong>based on useful and reusable concepts</strong>. If understanding a database interests you but you’ve never had the time or the will to dig into this wide subject, you should like this article.</p>
<p>Though the title of this article is explicit, <strong>the aim of this article is NOT to understand how to use a database</strong>. Therefore, <strong>you should already know how to write a simple join query and basic CRUD queries</strong>; otherwise you might not understand this article. This is the only thing you need to know, I’ll explain everything else.</p>
<p>I’ll start with some computer science stuff like time complexity. I know that some of you hate this concept but, without it, you can’t understand the cleverness inside a database. Since it’s a huge topic, <strong>I’ll focus on</strong> what I think is essential: <strong>the way a database handles an SQL query</strong>. I’ll only present <strong>the basic concepts behind a database</strong> so that at the end of the article you’ll have a good idea of what’s happening under the hood.</p>
<p>Since it’s a long and technical article that involves many algorithms and data structures, take your time to read it. Some concepts are more difficult to understand; you can skip them and still get the overall idea.</p>
<p>For the more knowledgeable of you, this article is more or less divided into 3 parts:</p>
<ul>
<li>An overview of low-level and high-level database components</li>
<li>An overview of the query optimization process</li>
<li>An overview of the transaction and buffer pool management</li>
</ul>
<p>Contents [<a href="http://coding-geek.com/how-databases-work/#" target="_blank" rel="noopener">show</a>]</p>
<h1 id="Back-to-basics"><a href="#Back-to-basics" class="headerlink" title="Back to basics"></a>Back to basics</h1><p>A long time ago (in a galaxy far, far away….), developers had to know exactly the number of operations they were coding. They knew by heart their algorithms and data structures because they couldn’t afford to waste the CPU and memory of their slow computers.</p>
<p>In this part, I’ll remind you about some of these concepts because they are essential to understand a database. I’ll also introduce the notion of <strong>database index</strong>.</p>
<h2 id="O-1-vs-O-n2"><a href="#O-1-vs-O-n2" class="headerlink" title="O(1) vs O(n2)"></a>O(1) vs O(n2)</h2><p>Nowadays, many developers don’t care about time complexity … and they’re right!</p>
<p>But when you deal with a large amount of data (I’m not talking about thousands) or if you’re fighting for milliseconds, it becomes critical to understand this concept. And guess what, databases have to deal with both situations! I won’t bore you a long time, just the time to get the idea. This will help us later to understand the concept of <strong>cost based optimization</strong>.</p>
<h3 id="The-concept"><a href="#The-concept" class="headerlink" title="The concept"></a>The concept</h3><p>The <strong>time complexity is used to see how long an algorithm will take for a given amount of data</strong>. To describe this complexity, computer scientists use the mathematical big O notation. This notation is used with a function that describes how many operations an algorithm needs for a given amount of input data.</p>
<p>For example, when I say “this algorithm is in O( some_function() )”, it means that for a certain amount of data the algorithm needs some_function(a_certain_amount_of_data) operations to do its job.</p>
<p><strong>What’s important is</strong> not the amount of data but <strong>the way the number of operations increases when the amount of data increases</strong>. The time complexity doesn’t give the exact number of operations but a good idea.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/TimeComplexity.png" target="_blank" rel="noopener"><img src="../../public/images/TimeComplexity-20200207203251326.png" alt="time complexity analysis"></a></p>
<p>In this figure, you can see the evolution of different types of complexities. I used a logarithmic scale to plot it. In other words, the number of data is quickly increasing from 1 to 1 billion. We can see that:</p>
<ul>
<li>The O(1) or constant complexity stays constant (otherwise it wouldn’t be called constant complexity).</li>
<li>The <strong>O(log(n)) stays low even with billions of data</strong>.</li>
<li>The worst complexity is the <strong>O(n2) where the number of operations quickly explodes</strong>.</li>
<li>The two other complexities are quickly increasing.</li>
</ul>
<h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><p>With a low amount of data, the difference between O(1) and O(n2) is negligible. For example, let’s say you have an algorithm that needs to process 2000 elements.</p>
<ul>
<li>An O(1) algorithm will cost you 1 operation</li>
<li>An O(log(n)) algorithm will cost you 7 operations</li>
<li>An O(n) algorithm will cost you 2 000 operations</li>
<li>An O(n*log(n)) algorithm will cost you 14 000 operations</li>
<li>An O(n2) algorithm will cost you 4 000 000 operations</li>
</ul>
<p>The difference between O(1) and O(n2) seems a lot (4 million) but you’ll lose at max 2 ms, just the time to blink your eyes. Indeed, current processors can handle <a href="https://en.wikipedia.org/wiki/Instructions_per_second" target="_blank" rel="noopener">hundreds of millions of operations per second</a>. This is why performance and optimization are not an issue in many IT projects.</p>
<p>As I said, it’s still important to know this concept when facing a huge number of data. If this time the algorithm needs to process 1 000 000 elements (which is not that big for a database):</p>
<ul>
<li>An O(1) algorithm will cost you 1 operation</li>
<li>An O(log(n)) algorithm will cost you 14 operations</li>
<li>An O(n) algorithm will cost you 1 000 000 operations</li>
<li>An O(n*log(n)) algorithm will cost you 14 000 000 operations</li>
<li>An O(n2) algorithm will cost you 1 000 000 000 000 operations</li>
</ul>
<p>I didn’t do the math but I’d say with the O(n2) algorithm you have the time to take a coffee (even a second one!). If you put another 0 on the amount of data, you’ll have the time to take a long nap.</p>
<h3 id="Going-deeper"><a href="#Going-deeper" class="headerlink" title="Going deeper"></a>Going deeper</h3><p>To give you an idea:</p>
<ul>
<li>A search in a good hash table gives an element in O(1)</li>
<li>A search in a well-balanced tree gives a result in O(log(n))</li>
<li>A search in an array gives a result in O(n)</li>
<li>The best sorting algorithms have an O(n*log(n)) complexity.</li>
<li>A bad sorting algorithm has an O(n2) complexity</li>
</ul>
<p>Note: In the next parts, we’ll see these algorithms and data structures.</p>
<p>There are multiple types of time complexity:</p>
<ul>
<li>the average case scenario</li>
<li>the best case scenario</li>
<li>and the worst case scenario</li>
</ul>
<p>The time complexity is often the worst case scenario.</p>
<p>I only talked about time complexity but complexity also works for:</p>
<ul>
<li>the memory consumption of an algorithm</li>
<li>the disk I/O consumption of an algorithm</li>
</ul>
<p>Of course there are worse complexities than n2, like:</p>
<ul>
<li>n4: that sucks! Some of the algorithms I’ll mention have this complexity.</li>
<li>3n: that sucks even more! One of the algorithms we’re going to see in the middle of this article has this complexity (and it’s really used in many databases).</li>
<li>factorial n : you’ll never get your results, even with a low amount of data.</li>
<li>nn: if you end-up with this complexity, you should ask yourself if IT is really your field…</li>
</ul>
<p>Note: I didn’t give you the real definition of the big O notation but just the idea. You can read this article on <a href="https://en.wikipedia.org/wiki/Big_O_notation" target="_blank" rel="noopener">Wikipedia</a> for the real (asymptotic) definition.</p>
<h2 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a>Merge Sort</h2><p>What do you do when you need to sort a collection? What? You call the sort() function … ok, good answer… But for a database you have to understand how this sort() function works.</p>
<p>There are several good sorting algorithms so I’ll focus on the most important one: <strong>the merge sort</strong>. You might not understand right now why sorting data is useful but you should after the part on query optimization. Moreover, understanding the merge sort will help us later to understand a common database join operation called the <strong>merge join</strong>.</p>
<h3 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h3><p>Like many useful algorithms, the merge sort is based on a trick: merging 2 sorted arrays of size N/2 into a N-element sorted array only costs N operations. This operation is called a <strong>merge</strong>.</p>
<p>Let’s see what this means with a simple example:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/merge_sort_3.png" target="_blank" rel="noopener"><img src="../../public/images/merge_sort_3-20200207203251311.png" alt="merge operation during merge sort algorithm"></a></p>
<p>You can see on this figure that to construct the final sorted array of 8 elements, you only need to iterate one time in the 2 4-element arrays. Since both 4-element arrays are already sorted:</p>
<ul>
<li>1) you compare both current elements in the 2 arrays (current=first for the first time)</li>
<li>2) then take the lowest one to put it in the 8-element array</li>
<li>3) and go to the next element in the array you took the lowest element</li>
<li>and repeat 1,2,3 until you reach the last element of one of the arrays.</li>
<li>Then you take the rest of the elements of the other array to put them in the 8-element array.</li>
</ul>
<p>This works because both 4-element arrays are sorted and therefore you don’t need to “go back” in these arrays.</p>
<p>Now that we’ve understood this trick, here is my pseudocode of the merge sort.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array mergeSort(array a)&#96;&#96;  &#96;&#96;if&#96;&#96;(length(a)&#x3D;&#x3D;&#96;&#96;1&#96;&#96;)&#96;&#96;   &#96;&#96;return&#96; &#96;a[&#96;&#96;0&#96;&#96;];&#96;&#96;  &#96;&#96;end &#96;&#96;if&#96; &#96;  &#96;&#96;&#x2F;&#x2F;recursive calls&#96;&#96;  &#96;&#96;[left_array right_array] :&#x3D; split_into_2_equally_sized_arrays(a);&#96;&#96;  &#96;&#96;array new_left_array :&#x3D; mergeSort(left_array);&#96;&#96;  &#96;&#96;array new_right_array :&#x3D; mergeSort(right_array);&#96; &#96;  &#96;&#96;&#x2F;&#x2F;merging the 2 small ordered arrays into a big one&#96;&#96;  &#96;&#96;array result :&#x3D; merge(new_left_array,new_right_array);&#96;&#96;  &#96;&#96;return&#96; &#96;result;</span><br></pre></td></tr></table></figure>

<p>The merge sort breaks the problem into smaller problems then finds the results of the smaller problems to get the result of the initial problem (note: this kind of algorithms is called divide and conquer). If you don’t understand this algorithm, don’t worry; I didn’t understand it the first time I saw it. If it can help you, I see this algorithm as a two-phase algorithm:</p>
<ul>
<li>The division phase where the array is divided into smaller arrays</li>
<li>The sorting phase where the small arrays are put together (using the merge) to form a bigger array.</li>
</ul>
<h3 id="Division-phase"><a href="#Division-phase" class="headerlink" title="Division phase"></a>Division phase</h3><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/merge_sort_1.png" target="_blank" rel="noopener"><img src="../../public/images/merge_sort_1-20200207203251327.png" alt="division phaseduring merge sort algorithm"></a></p>
<p>During the division phase, the array is divided into unitary arrays using 3 steps. The formal number of steps is log(N) (since N=8, log(N) = 3).</p>
<p>How do I know that?</p>
<p>I’m a genius! In one word: mathematics. The idea is that each step divides the size of the initial array by 2. The number of steps is the number of times you can divide the initial array by two. This is the exact definition of logarithm (in base 2).</p>
<h3 id="Sorting-phase"><a href="#Sorting-phase" class="headerlink" title="Sorting phase"></a>Sorting phase</h3><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/merge_sort_2.png" target="_blank" rel="noopener"><img src="../../public/images/merge_sort_2-20200207203251328.png" alt="sort phaseduring merge sort algorithm"></a></p>
<p>In the sorting phase, you start with the unitary arrays. During each step, you apply multiple merges and the overall cost is N=8 operations:</p>
<ul>
<li>In the first step you have 4 merges that cost 2 operations each</li>
<li>In the second step you have 2 merges that cost 4 operations each</li>
<li>In the third step you have 1 merge that costs 8 operations</li>
</ul>
<p>Since there are log(N) steps, <strong>the overall costs N * log(N) operations</strong>.</p>
<h3 id="The-power-of-the-merge-sort"><a href="#The-power-of-the-merge-sort" class="headerlink" title="The power of the merge sort"></a>The power of the merge sort</h3><p>Why this algorithm is so powerful?</p>
<p>Because:</p>
<ul>
<li>You can modify it in order to reduce the memory footprint, in a way that you don’t create new arrays but you directly modify the input array.</li>
</ul>
<p>Note: this kind of algorithms is called <a href="https://en.wikipedia.org/wiki/In-place_algorithm" target="_blank" rel="noopener">in-place</a>.</p>
<ul>
<li>You can modify it in order to use disk space and a small amount of memory at the same time without a huge disk I/O penalty. The idea is to load in memory only the parts that are currently processed. This is important when you need to sort a multi-gigabyte table with only a memory buffer of 100 megabytes.</li>
</ul>
<p>Note: this kind of algorithms is called <a href="https://en.wikipedia.org/wiki/External_sorting" target="_blank" rel="noopener">external sorting</a>.</p>
<ul>
<li>You can modify it to run on multiple processes/threads/servers.</li>
</ul>
<p>For example, the distributed merge sort is one of the key components of <a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Reducer.html" target="_blank" rel="noopener">Hadoop </a>(which is THE framework in Big Data).</p>
<ul>
<li>This algorithm can turn lead into gold (true fact!).</li>
</ul>
<p>This sorting algorithm is used in most (if not all) databases but it’s not the only one. If you want to know more, you can read this <a href="http://wwwlgis.informatik.uni-kl.de/archiv/wwwdvs.informatik.uni-kl.de/courses/DBSREAL/SS2005/Vorlesungsunterlagen/Implementing_Sorting.pdf" target="_blank" rel="noopener">research paper</a> that discusses the pros and cons of the common sorting algorithms in a database.</p>
<h2 id="Array-Tree-and-Hash-table"><a href="#Array-Tree-and-Hash-table" class="headerlink" title="Array, Tree and Hash table"></a>Array, Tree and Hash table</h2><p>Now that we understand the idea behind time complexity and sorting, I have to tell you about 3 data structures. It’s important because they’re <strong>the backbone of modern databases</strong>. I’ll also introduce the notion of <strong>database index</strong>.</p>
<h3 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h3><p>The two-dimensional array is the simplest data structure. A table can be seen as an array. For example:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/array.png" target="_blank" rel="noopener"><img src="../../public/images/array-20200207203251327.png" alt="array table in databases"></a></p>
<p>This 2-dimensional array is a table with rows and columns:</p>
<ul>
<li>Each row represents a subject</li>
<li>The columns the features that describe the subjects.</li>
<li>Each column stores a certain type of data (integer, string, date …).</li>
</ul>
<p>Though it’s great to store and visualize data, when you need to look for a specific value it sucks.</p>
<p>For example, <strong>if you want to find all the guys who work in the UK</strong>, you’ll have to look at each row to find if the row belongs to the UK. <strong>This will cost you N operations</strong> (N being the number of rows) which is not bad but could there be a faster way? This is where trees come into play.</p>
<p>Note: Most modern databases provide advanced arrays to store tables efficiently like heap-organized tables or index-organized tables. But it doesn’t change the problem of fast searching for a specific condition on a group of columns.</p>
<h3 id="Tree-and-database-index"><a href="#Tree-and-database-index" class="headerlink" title="Tree and database index"></a>Tree and database index</h3><p>A binary search tree is a binary tree with a special property, the key in each node must be:</p>
<ul>
<li>greater than all keys stored in the left sub-tree</li>
<li>smaller than all keys stored in the right sub-tree</li>
</ul>
<p>Let’s see what it means visually</p>
<h4 id="The-idea"><a href="#The-idea" class="headerlink" title="The idea"></a>The idea</h4><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/BST.png" target="_blank" rel="noopener"><img src="../../public/images/BST-20200207203251343.png" alt="binary search tree"></a></p>
<p>This tree has N=15 elements. Let’s say I’m looking for 208:</p>
<ul>
<li>I start with the root whose key is 136. Since 136&lt;208, I look at the right sub-tree of the node 136.</li>
<li>398&gt;208 so, I look at the left sub-tree of the node 398</li>
<li>250&gt;208 so, I look at the left sub-tree of the node 250</li>
<li>200&lt;208 so, I look at the right sub-tree of the node 200. But 200 doesn’t have a right subtree, <strong>the value doesn’t exist</strong> (because if it did exist it would be in the right subtree of 200)</li>
</ul>
<p>Now let’s say I’m looking for 40</p>
<ul>
<li>I start with the root whose key is 136. Since 136&gt;40, I look at the left sub-tree of the node 136.</li>
<li>80&gt;40 so, I look at the left sub-tree of the node 80</li>
<li>40= 40, <strong>the node exists</strong>. I extract the id of the row inside the node (it’s not in the figure) and look at the table for the given row id.</li>
<li>Knowing the row id let me know where the data is precisely on the table and therefore I can get it instantly.</li>
</ul>
<p>In the end, both searches cost me the number of levels inside the tree. If you read carefully the part on the merge sort you should see that there are log(N) levels. So the <strong>cost of the search is log(N)</strong>, not bad!</p>
<h4 id="Back-to-our-problem"><a href="#Back-to-our-problem" class="headerlink" title="Back to our problem"></a>Back to our problem</h4><p>But this stuff is very abstract so let’s go back to our problem. Instead of a stupid integer, imagine the string that represents the country of someone in the previous table. Suppose you have a tree that contains the column “country” of the table:</p>
<ul>
<li>If you want to know who is working in the UK</li>
<li>you look at the tree to get the node that represents the UK</li>
<li>inside the “UK node” you’ll find the locations of the rows of the UK workers.</li>
</ul>
<p>This search only costs you log(N) operations instead of N operations if you directly use the array. What you’ve just imagined was a <strong>database index</strong>.</p>
<p>You can build a tree index for any group of columns (a string, an integer, 2 strings, an integer and a string, a date …) as long as you have a function to compare the keys (i.e. the group of columns) so that you can establish an <strong>order</strong> <strong>among the keys</strong> (which is the case for any basic types in a database).</p>
<h4 id="B-Tree-Index"><a href="#B-Tree-Index" class="headerlink" title="B+Tree Index"></a>B+Tree Index</h4><p>Although this tree works well to get a specific value, there is a BIG problem when you need to <strong>get multiple elements</strong> <strong>between two values</strong>. It will cost O(N) because you’ll have to look at each node in the tree and check if it’s between these 2 values (for example, with an in-order traversal of the tree). Moreover this operation is not disk I/O friendly since you’ll have to read the full tree. We need to find a way to efficiently do a <strong>range query</strong>. To answer this problem, modern databases use a modified version of the previous tree called B+Tree. In a B+Tree:</p>
<ul>
<li>only the lowest nodes (the leaves) <strong>store information</strong> (the location of the rows in the associated table)</li>
<li>the other nodes are just here <strong>to route</strong> to the right node <strong>during the search</strong>.</li>
</ul>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/database_index.png" target="_blank" rel="noopener"><img src="../../public/images/database_index-20200207203251357.png" alt="B+Tree index in databases"></a></p>
<p>As you can see, there are more nodes (twice more). Indeed, you have additional nodes, the “decision nodes” that will help you to find the right node (that stores the location of the rows in the associated table). But the search complexity is still in O(log(N)) (there is just one more level). The big difference is that <strong>the lowest nodes are linked to their successors</strong>.</p>
<p>With this B+Tree, if you’re looking for values between 40 and 100:</p>
<ul>
<li>You just have to look for 40 (or the closest value after 40 if 40 doesn’t exist) like you did with the previous tree.</li>
<li>Then gather the successors of 40 using the direct links to the successors until you reach 100.</li>
</ul>
<p>Let’s say you found M successors and the tree has N nodes. The search for a specific node costs log(N) like the previous tree. But, once you have this node, you get the M successors in M operations with the links to their successors. <strong>This search only costs M + log(N)</strong> operations vs N operations with the previous tree. Moreover, you don’t need to read the full tree (just M + log(N) nodes), which means less disk usage. If M is low (like 200 rows) and N large (1 000 000 rows) it makes a BIG difference.</p>
<p>But there are new problems (again!). If you add or remove a row in a database (and therefore in the associated B+Tree index):</p>
<ul>
<li>you have to keep the order between nodes inside the B+Tree otherwise you won’t be able to find nodes inside the mess.</li>
<li>you have to keep the lowest possible number of levels in the B+Tree otherwise the time complexity in O(log(N)) will become O(N).</li>
</ul>
<p>I other words, the B+Tree needs to be self-ordered and self-balanced. Thankfully, this is possible with smart deletion and insertion operations. But this comes with a cost: the insertion and deletion in a B+Tree are in O(log(N)). This is why some of you have heard that <strong>using too many indexes is not a good idea.</strong> Indeed, <strong>you’re slowing down the fast insertion/update/deletion of a row</strong> in a table since the database needs to update the indexes of the table with a costly O(log(N)) operation per index. Moreover, adding indexes means more workload for the <strong>transaction manager</strong> (we will see this manager at the end of the article).</p>
<p>For more details, you can look at the Wikipedia <a href="https://en.wikipedia.org/wiki/B%2B_tree" target="_blank" rel="noopener">article about B+Tree</a>. If you want an example of a B+Tree implementation in a database, look at <a href="http://blog.jcole.us/2013/01/07/the-physical-structure-of-innodb-index-pages/" target="_blank" rel="noopener">this article</a> and <a href="http://blog.jcole.us/2013/01/10/btree-index-structures-in-innodb/" target="_blank" rel="noopener">this article</a> from a core developer of MySQL. They both focus on how innoDB (the engine of MySQL) handles indexes.</p>
<p>Note: I was told by a reader that, because of low-level optimizations, the B+Tree needs to be fully balanced.</p>
<h3 id="Hash-table"><a href="#Hash-table" class="headerlink" title="Hash table"></a>Hash table</h3><p>Our last important data structure is the hash table. It’s very useful when you want to quickly look for values.  Moreover, understanding the hash table will help us later to understand a common database join operation called the <strong>hash join</strong>. This data structure is also used by a database to store some internal stuff (like the <strong>lock table</strong> or the <strong>buffer pool</strong>, we’ll see both concepts later)</p>
<p>The hash table is a data structure that quickly finds an element with its key. To build a hash table you need to define:</p>
<ul>
<li><strong>a key</strong> for your elements</li>
<li><strong>a hash function</strong> for the keys. The computed hashes of the keys give the locations of the elements (called <strong>buckets</strong>).</li>
<li><strong>a function to compare the keys</strong>. Once you found the right bucket you have to find the element you’re looking for inside the bucket using this comparison.</li>
</ul>
<h4 id="A-simple-example"><a href="#A-simple-example" class="headerlink" title="A simple example"></a>A simple example</h4><p>Let’s have a visual example:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/hash_table.png" target="_blank" rel="noopener"><img src="../../public/images/hash_table-20200207203251356.png" alt="hash table"></a></p>
<p>This hash table has 10 buckets. Since I’m lazy I only drew 5 buckets but I know you’re smart so I let you imagine the 5 others. The Hash function I used is the modulo 10 of the key. In other words I only keep the last digit of the key of an element to find its bucket:</p>
<ul>
<li>if the last digit is 0 the element ends up in the bucket 0,</li>
<li>if the last digit is 1 the element ends up in the bucket 1,</li>
<li>if the last digit is 2 the element ends up in the bucket 2,</li>
<li>…</li>
</ul>
<p>The compare function I used is simply the equality between 2 integers.</p>
<p>Let’s say you want to get the element 78:</p>
<ul>
<li>The hash table computes the hash code for 78 which is 8.</li>
<li>It looks in the bucket 8, and the first element it finds is 78.</li>
<li>It gives you back the element 78</li>
<li><strong>The</strong> <strong>search only costs 2 operations</strong> (1 for computing the hash value and the other for finding the element inside the bucket).</li>
</ul>
<p>Now, let’s say you want to get the element 59:</p>
<ul>
<li>The hash table computes the hash code for 59 which is 9.</li>
<li>It looks in the bucket 9, and the first element it finds is 99. Since 99!=59, element 99 is not the right element.</li>
<li>Using the same logic, it looks at the second element (9), the third (79), … , and the last (29).</li>
<li>The element doesn’t exist.</li>
<li><strong>The search costs 7 operations</strong>.</li>
</ul>
<h4 id="A-good-hash-function"><a href="#A-good-hash-function" class="headerlink" title="A good hash function"></a>A good hash function</h4><p>As you can see, depending on the value you’re looking for, the cost is not the same!</p>
<p>If I now change the hash function with the modulo 1 000 000 of the key (i.e. taking the last 6 digits), the second search only costs 1 operation because there are no elements in the bucket 000059. <strong>The real challenge is to find a good hash function that will create buckets that contain a very small amount of elements</strong>.</p>
<p>In my example, finding a good hash function is easy. But this is a simple example, finding a good hash function is more difficult when the key is:</p>
<ul>
<li>a string (for example the last name of a person)</li>
<li>2 strings (for example the last name and the first name of a person)</li>
<li>2 strings and a date (for example the last name, the first name and the birth date of a person)</li>
<li>…</li>
</ul>
<p><strong>With a good hash function,</strong> <strong>the search in a hash table is in O(1)</strong>.</p>
<h4 id="Array-vs-hash-table"><a href="#Array-vs-hash-table" class="headerlink" title="Array vs hash table"></a>Array vs hash table</h4><p>Why not using an array?</p>
<p>Hum, you’re asking a good question.</p>
<ul>
<li>A hash table can be <strong>half loaded in memory</strong> and the other buckets can stay on disk.</li>
<li>With an array you have to use a contiguous space in memory. If you’re loading a large table it’s <strong>very difficult to have enough contiguous space</strong>.</li>
<li>With a hash table you can <strong>choose the key you want</strong> (for example the country AND the last name of a person).</li>
</ul>
<p>For more information, you can read my article on the <a href="http://coding-geek.com/how-does-a-hashmap-work-in-java/" target="_blank" rel="noopener">Java HashMap</a> which is an efficient hash table implementation; you don’t need to understand Java to understand the concepts inside this article.</p>
<h1 id="Global-overview"><a href="#Global-overview" class="headerlink" title="Global overview"></a>Global overview</h1><p>We’ve just seen the basic components inside a database. We now need to step back to see the big picture.</p>
<p>A database is a collection of information that can easily be accessed and modified. But a simple bunch of files could do the same. In fact, the simplest databases like SQLite are nothing more than a bunch of files. But SQLite is a well-crafted bunch of files because it allows you to:</p>
<ul>
<li>use transactions that ensure data are safe and coherent</li>
<li>quickly process data even when you’re dealing with millions of data</li>
</ul>
<p>More generally, a database can be seen as the following figure:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/global_overview.png" target="_blank" rel="noopener"><img src="../../public/images/global_overview-20200207203251364.png" alt="global overview of a database"></a></p>
<p>Before writing this part, I’ve read multiple books/papers and every source had its on way to represent a database. So, don’t focus too much on how I organized this database or how I named the processes because I made some choices to fit the plan of this article. What matters are the different components; the overall idea is that <strong>a database is divided into multiple components that interact with each other</strong>.</p>
<p>The core components:</p>
<ul>
<li><strong>The process manager</strong>: Many databases have a <strong>pool of processes/threads</strong> that needs to be managed. Moreover, in order to gain nanoseconds, some modern databases use their own threads instead of the Operating System threads.</li>
<li><strong>The network manager</strong>: Network I/O is a big issue, especially for distributed databases. That’s why some databases have their own manager.</li>
<li><strong>File system manager</strong>: <strong>Disk I/O is the first bottleneck of a database</strong>. Having a manager that will perfectly handle the Operating System file system or even replace it is important.</li>
<li><strong>The memory manager</strong>: To avoid the disk I/O penalty a large quantity of ram is required. But if you handle a large amount of memory, you need an efficient memory manager. Especially when you have many queries using memory at the same time.</li>
<li><strong>Security Manager</strong>: for managing the authentication and the authorizations of the users</li>
<li><strong>Client manager</strong>: for managing the client connections</li>
<li>…</li>
</ul>
<p>The tools:</p>
<ul>
<li><strong>Backup manager</strong>: for saving and restoring a database.</li>
<li><strong>Recovery manager</strong>: for restarting the database in a <strong>coherent state</strong> after a crash</li>
<li><strong>Monitor manager</strong>: for logging the activity of the database and providing tools to monitor a database</li>
<li><strong>Administration manager</strong>: for storing metadata (like the names and the structures of the tables) and providing tools to manage databases, schemas, tablespaces, …</li>
<li>…</li>
</ul>
<p>The query Manager:</p>
<ul>
<li><strong>Query parser</strong>: to check if a query is valid</li>
<li><strong>Query rewriter</strong>: to pre-optimize a query</li>
<li><strong>Query optimizer</strong>: to optimize a query</li>
<li><strong>Query executor</strong>: to compile and execute a query</li>
</ul>
<p>The data manager:</p>
<ul>
<li><strong>Transaction manager</strong>: to handle transactions</li>
<li><strong>Cache manager</strong>: to put data in memory before using them and put data in memory before writing them on disk</li>
<li><strong>Data access manager</strong>: to access data on disk</li>
</ul>
<p>For the rest of this article, I’ll focus on how a database manages an SQL query through the following processes:</p>
<ul>
<li>the client manager</li>
<li>the query manager</li>
<li>the data manager (I’ll also include the recovery manager in this part)</li>
</ul>
<h1 id="Client-manager"><a href="#Client-manager" class="headerlink" title="Client manager"></a>Client manager</h1><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/client_manager.png" target="_blank" rel="noopener"><img src="../../public/images/client_manager-20200207203251358.png" alt="client manager in databases"></a></p>
<p>The client manager is the part that handles the communications with the client. The client can be a (web) server or an end-user/end-application. The client manager provides different ways to access the database through a set of well-known APIs: JDBC, ODBC, OLE-DB …</p>
<p>It can also provide proprietary database access APIs.</p>
<p>When you connect to a database:</p>
<ul>
<li>The manager first checks your <strong>authentication</strong> (your login and password) and then checks if you have the <strong>authorizations</strong> to use the database. These access rights are set by your DBA.</li>
<li>Then, it checks if there is a process (or a thread) available to manage your query.</li>
<li>It also checks if the database if not under heavy load.</li>
<li>It can wait a moment to get the required resources. If this wait reaches a timeout, it closes the connection and gives a readable error message.</li>
<li>Then it <strong>sends your query to the query manager</strong> and your query is processed</li>
<li>Since the query processing is not an “all or nothing” thing, as soon as it gets data from the query manager, it <strong>stores</strong> <strong>the partial results in a buffer and start sending</strong> them to you.</li>
<li>In case of problem, it stops the connection, gives you a <strong>readable explanation</strong> and releases the resources.</li>
</ul>
<h1 id="Query-manager"><a href="#Query-manager" class="headerlink" title="Query manager"></a>Query manager</h1><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/query_manager.png" target="_blank" rel="noopener"><img src="../../public/images/query_manager-20200207203251359.png" alt="query manager in databases"></a></p>
<p><strong>This part is where the power of a database lies</strong>. During this part, an ill-written query is transformed into a <strong>fast</strong> executable code. The code is then executed and the results are returned to the client manager. It’s a multiple-step operation:</p>
<ul>
<li>the query is first <strong>parsed</strong> to see if it’s valid</li>
<li>it’s then <strong>rewritten</strong> to remove useless operations and add some pre-optimizations</li>
<li>it’s then <strong>optimized</strong> to improve the performances and transformed into an execution and data access plan.</li>
<li>then the plan is <strong>compiled</strong></li>
<li>at last, it’s <strong>executed</strong></li>
</ul>
<p>In this part, I won’t talk a lot about the last 2 points because they’re less important.</p>
<p>After reading this part, if you want a better understanding I recommend reading:</p>
<ul>
<li>The initial research paper (1979) on cost based optimization: <a href="http://www.cs.berkeley.edu/~brewer/cs262/3-selinger79.pdf" target="_blank" rel="noopener">Access Path Selection in a Relational Database Management System</a>. This article is only 12 pages and understandable with an average level in computer science.</li>
<li>A very good and in-depth presentation on how DB2 9.X optimizes queries <a href="http://infolab.stanford.edu/~hyunjung/cs346/db2-talk.pdf" target="_blank" rel="noopener">here</a></li>
<li>A very good presentation on how PostgreSQL optimizes queries <a href="http://momjian.us/main/writings/pgsql/optimizer.pdf" target="_blank" rel="noopener">here</a>. It’s the most accessible document since it’s more a presentation on “let’s see what query plans PostgreSQL gives in these situations“ than a “let’s see the algorithms used by PostgreSQL”.</li>
<li>The official <a href="https://www.sqlite.org/optoverview.html" target="_blank" rel="noopener">SQLite documentation</a> about optimization. It’s “easy” to read because SQLite uses simple rules. Moreover, it’s the only official documentation that really explains how it works.</li>
<li>A good presentation on how SQL Server 2005 optimizes queries <a href="https://blogs.msdn.com/cfs-filesystemfile.ashx/__key/communityserver-components-postattachments/00-08-50-84-93/QPTalk.pdf" target="_blank" rel="noopener">here</a></li>
<li>A white paper about optimization in Oracle 12c <a href="http://www.oracle.com/technetwork/database/bi-datawarehousing/twp-optimizer-with-oracledb-12c-1963236.pdf" target="_blank" rel="noopener">here</a></li>
<li>2 theoretical courses on query optimization from the authors of the book “<em>DATABASE SYSTEM CONCEPTS”</em> <a href="http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch12.ppt" target="_blank" rel="noopener">here</a> and t<a href="http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch13.ppt" target="_blank" rel="noopener">here</a>. A good read that focuses on disk I/O cost but a good level in CS is required.</li>
<li>Another <a href="https://www.informatik.hu-berlin.de/de/forschung/gebiete/wbi/teaching/archive/sose05/dbs2/slides/09_joins.pdf" target="_blank" rel="noopener">theoretical course</a> that I find more accessible but that only focuses on join operators and disk I/O.</li>
</ul>
<h2 id="Query-parser"><a href="#Query-parser" class="headerlink" title="Query parser"></a>Query parser</h2><p>Each SQL statement is sent to the parser where it is checked for correct syntax. If you made a mistake in your query the parser will reject the query. For example, if you wrote “SLECT …” instead of “SELECT …”,  the story ends here.</p>
<p>But this goes deeper. It also checks that the keywords are used in the right order. For example a WHERE before a SELECT will be rejected.</p>
<p>Then, the tables and the fields inside the query are analyzed. The parser uses the metadata of the database to check:</p>
<ul>
<li>If the <strong>tables exist</strong></li>
<li>If the <strong>fields</strong> of the tables exist</li>
<li>If the <strong>operations</strong> for the types of the fields <strong>are possible</strong> (for example you can’t compare an integer with a string, you can’t use a substring() function on an integer)</li>
</ul>
<p>Then it checks if you have the <strong>authorizations</strong> to read (or write) the tables in the query. Again, these access rights on tables are set by your DBA.</p>
<p>During this parsing, the SQL query is transformed into an internal representation (often a tree)</p>
<p>If everything is ok then the internal representation is sent to the query rewriter.</p>
<h2 id="Query-rewriter"><a href="#Query-rewriter" class="headerlink" title="Query rewriter"></a>Query rewriter</h2><p>At this step, we have an internal representation of a query. The aim of the rewriter is:</p>
<ul>
<li>to pre-optimize the query</li>
<li>to avoid unnecessary operations</li>
<li>to help the optimizer to find the best possible solution</li>
</ul>
<p>The rewriter executes a list of known rules on the query. If the query fits a pattern of a rule, the rule is applied and the query is rewritten. Here is a non-exhaustive list of (optional) rules:</p>
<ul>
<li><strong>View merging:</strong> If you’re using a view in your query, the view is transformed with the SQL code of the view.</li>
<li><strong>Subquery flattening</strong>: Having subqueries is very difficult to optimize so the rewriter will try to modify a query with a subquery to remove the subquery.</li>
</ul>
<p>For example</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;PERSON.*&#96;&#96;FROM&#96; &#96;PERSON&#96;&#96;WHERE&#96; &#96;PERSON.person_key &#96;&#96;IN&#96;&#96;(&#96;&#96;SELECT&#96; &#96;MAILS.person_key&#96;&#96;FROM&#96; &#96;MAILS&#96;&#96;WHERE&#96; &#96;MAILS.mail &#96;&#96;LIKE&#96; &#96;&#39;christophe%&#39;&#96;&#96;);</span><br></pre></td></tr></table></figure>

<p>Will be replaced by</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;PERSON.*&#96;&#96;FROM&#96; &#96;PERSON, MAILS&#96;&#96;WHERE&#96; &#96;PERSON.person_key &#x3D; MAILS.person_key&#96;&#96;and&#96; &#96;MAILS.mail &#96;&#96;LIKE&#96; &#96;&#39;christophe%&#39;&#96;&#96;;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Removal of unnecessary operators</strong>: For example if you use a DISTINCT whereas you have a UNIQUE constraint that prevents the data from being non-unique, the DISTINCT keyword is removed.</li>
<li><strong>Redundant join elimination:</strong> If you have twice the same join condition because one join condition is hidden in a view or if by transitivity there is a useless join, it’s removed.</li>
<li><strong>Constant arithmetic evaluation:</strong> If you write something that requires a calculus, then it’s computed once during the rewriting. For example WHERE AGE &gt; 10+2 is transformed into WHERE AGE &gt; 12 and TODATE(“some date”) is transformed into the date in the datetime format</li>
<li><strong>(**</strong>Advanced) Partition Pruning:** If you’re using a partitioned table, the rewriter is able to find what partitions to use.</li>
<li><strong>(Advanced) Materialized view rewrite</strong>: If you have a materialized view that matches a subset of the predicates in your query, the rewriter checks if the view is up to date and modifies the query to use the materialized view instead of the raw tables.</li>
<li><strong>(Advanced) Custom rules:</strong> If you have custom rules to modify a query (like Oracle policies), then the rewriter executes these rules</li>
<li><strong>(Advanced) Olap transformations</strong>: analytical/windowing functions, star joins, rollup … are also transformed (but I’m not sure if it’s done by the rewriter or the optimizer, since both processes are very close it must depends on the database).</li>
</ul>
<p>This rewritten query is then sent to the query optimizer where the fun begins!</p>
<h2 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h2><p>Before we see how a database optimizes a query we need to speak about <strong>statistics</strong> because <strong>without them</strong> <strong>a database is stupid</strong>. If you don’t tell the database to analyze its own data, it will not do it and it will make (very) bad assumptions.</p>
<p>But what kind of information does a database need?</p>
<p>I have to (briefly) talk about how databases and Operating systems store data. They’re using a minimum unit called <strong>a</strong> <strong>page</strong> or a block (4 or 8 kilobytes by default). This means that if you only need 1 Kbytes it will cost you one page anyway. If the page takes 8 Kbytes then you’ll waste 7 Kbytes.</p>
<p>Back to the statistics! When you ask a database to gather statistics, it computes values like:</p>
<ul>
<li>The number of rows/pages in a table</li>
<li>For each column in a table:<ul>
<li>distinct data values</li>
<li>the length of data values (min, max, average)</li>
<li>data range information (min, max, average)</li>
</ul>
</li>
<li>Information on the indexes of the table.</li>
</ul>
<p><strong>These statistics will help the optimizer to estimate the</strong> <strong>disk I/O, CPU and memory usages of the query.</strong></p>
<p>The statistics for each column are very important. For example if a table PERSON needs to be joined on 2 columns: LAST_NAME, FIRST_NAME. With the statistics, the database knows that there are only 1 000 different values on FIRST_NAME and 1 000 000 different values on LAST_NAME. Therefore, the database will join the data on LAST_NAME, FIRST_NAME instead of FIRST_NAME,LAST_NAME because it produces way less comparisons since the LAST_NAME are unlikely to be the same so most of the time a comparison on the 2 (or 3) first characters of the LAST_NAME is enough.</p>
<p>But these are basic statistics. You can ask a database to compute advanced statistics called <strong>histograms</strong>. Histograms are statistics that inform about the distribution of the values inside the columns. For example</p>
<ul>
<li>the most frequent values</li>
<li>the quantiles</li>
<li>…</li>
</ul>
<p>These extra statistics will help the database to find an even better query plan. Especially for equality predicate (ex: WHERE AGE = 18 ) or range predicates (ex: WHERE AGE &gt; 10 and AGE &lt;40 ) because the database will have a better idea of the number rows concerned by these predicates (note: the technical word for this concept is selectivity).</p>
<p>The statistics are stored in the metadata of the database. For example you can see the statistics for the (non-partitioned) tables:</p>
<ul>
<li>in USER/ALL/DBA_TABLES and USER/ALL/DBA_TAB_COLUMNS for Oracle</li>
<li>in SYSCAT.<em>TABLES</em> and <em>SYSCAT.COLUMNS for DB2</em>.</li>
</ul>
<p>The <strong>statistics have to be up to date</strong>. There is nothing worse than a database thinking a table has only 500 rows whereas it has 1 000 000 rows. The only drawback of the statistics is that <strong>it takes time to compute them</strong>. This is why they’re not automatically computed by default in most databases. It becomes difficult with millions of data to compute them. In this case, you can choose to compute only the basics statistics or to compute the stats on a sample of the database.</p>
<p>For example, when I was working on a project dealing with hundreds of millions rows in each tables, I chose to compute the statistics on only 10%, which led to a huge gain in time. For the story it turned out to be a bad decision because occasionally the 10% chosen by Oracle 10G for a specific column of a specific table were very different from the overall 100% (which is very unlikely to happen for a table with 100M rows). This wrong statistic led to a query taking occasionally 8 hours instead of 30 seconds; a nightmare to find the root cause. This example shows how important the statistics are.</p>
<p>Note: Of course, there are more advanced statistics specific for each database. If you want to know more, read the documentations of the databases. That being said, I’ve tried to understand how the statistics are used and the best official documentation I found was the <a href="http://www.postgresql.org/docs/9.4/static/row-estimation-examples.html" target="_blank" rel="noopener">one from PostgreSQL</a>.</p>
<h2 id="Query-optimizer"><a href="#Query-optimizer" class="headerlink" title="Query optimizer"></a>Query optimizer</h2><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/15-McDonalds_CBO.jpg" target="_blank" rel="noopener"><img src="../../public/images/15-McDonalds_CBO-20200207203251382.jpg" alt="CBO"></a></p>
<p>All modern databases are using a <strong>Cost Based Optimization</strong> (or <strong>CBO</strong>) to optimize queries. The idea is to put a cost an every operation and find the best way to reduce the cost of the query by using the cheapest chain of operations to get the result.</p>
<p>To understand how a cost optimizer works I think it’s good to have an example to “feel” the complexity behind this task. In this part I’ll present you the 3 common ways to join 2 tables and we will quickly see that even a simple join query is a nightmare to optimize. After that, we’ll see how real optimizers do this job.</p>
<p>For these joins, I’ll focus on their time complexity but <strong>a</strong> <strong>database optimizer computes</strong> their <strong>CPU cost, disk I/O cost and memory requirement</strong>. The difference between time complexity and CPU cost is that time cost is very approximate (it’s for lazy guys like me). For the CPU cost, I should count every operation like an addition, an “if statement”, a multiplication, an iteration … Moreover:</p>
<ul>
<li>Each high level code operation has a specific number of low level CPU operations.</li>
<li>The cost of a CPU operation is not the same (in terms of CPU cycles) whether you’re using an Intel Core i7, an Intel Pentium 4, an AMD Opteron…. In other words it depends on the CPU architecture.</li>
</ul>
<p>Using the time complexity is easier (at least for me) and with it we can still get the concept of CBO. I’ll sometimes speak about disk I/O since it’s an important concept. Keep in mind that <strong>the bottleneck is most of the time the disk I/O and not the CPU usage</strong>.</p>
<h3 id="Indexes"><a href="#Indexes" class="headerlink" title="Indexes"></a>Indexes</h3><p>We talked about indexes when we saw the B+Trees. Just remember that these <strong>indexes are already sorted</strong>.</p>
<p>FYI, there are other types of indexes like <strong>bitmap indexes</strong>. They don’t offer the same cost in terms of CPU, disk I/O and memory than B+Tree indexes.</p>
<p>Moreover, many modern databases can <strong>dynamically create temporary indexes</strong> just for the current query if it can improve the cost of the execution plan.</p>
<h3 id="Access-Path"><a href="#Access-Path" class="headerlink" title="Access Path"></a>Access Path</h3><p>Before applying your join operators, you first need to get your data. Here is how you can get your data.</p>
<p>Note: Since the real problem with all the access paths is the disk I/O, I won’t talk a lot about time complexity.</p>
<h4 id="Full-scan"><a href="#Full-scan" class="headerlink" title="Full scan"></a>Full scan</h4><p>If you’ve ever read an execution plan you must have seen the word <strong>full scan</strong> (or just scan). A full scan is simply the database reading a table or an index entirely. <strong>In terms of disk I/O, a table full scan is obviously more expensive than an index full scan</strong>.</p>
<h4 id="Range-Scan"><a href="#Range-Scan" class="headerlink" title="Range Scan"></a>Range Scan</h4><p>There are other types of scan like <strong>index range scan</strong>. It is used for example when you use a predicate like “WHERE AGE &gt; 20 AND AGE &lt;40”.</p>
<p>Of course you need have an index on the field AGE to use this index range scan.</p>
<p>We already saw in the first part that the time cost of a range query is something like log(N) +M, where N is the number of data in this index and M an estimation of the number of rows inside this range. <strong>Both N and M values are known thanks to the statistics</strong> (Note: M is the selectivity for the predicate AGE &gt;20 AND AGE&lt;40). Moreover, for a range scan you don’t need to read the full index so it’s <strong>less expensive in terms of disk I/O than a full scan</strong>.</p>
<h4 id="Unique-scan"><a href="#Unique-scan" class="headerlink" title="Unique scan"></a>Unique scan</h4><p>If you only need one value from an index you can use the <strong>unique scan</strong>.</p>
<h4 id="Access-by-row-id"><a href="#Access-by-row-id" class="headerlink" title="Access by row id"></a>Access by row id</h4><p>Most of the time, if the database uses an index, it will have to look for the rows associated to the index. To do so it will use an access by row id.</p>
<p>For example, if you do something like</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;LASTNAME, FIRSTNAME &#96;&#96;from&#96; &#96;PERSON &#96;&#96;WHERE&#96; &#96;AGE &#x3D; 28</span><br></pre></td></tr></table></figure>

<p>If you have an index for person on column age, the optimizer will use the index to find all the persons who are 28 then it will ask for the associate rows in the table because the index only has information about the age and you want to know the lastname and the firstname.</p>
<p>But, if now you do something like</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;TYPE_PERSON.CATEGORY &#96;&#96;from&#96; &#96;PERSON ,TYPE_PERSON&#96;&#96;WHERE&#96; &#96;PERSON.AGE &#x3D; TYPE_PERSON.AGE</span><br></pre></td></tr></table></figure>

<p>The index on PERSON will be used to join with TYPE_PERSON but the table PERSON will not be accessed by row id since you’re not asking information on this table.</p>
<p>Though it works great for a few accesses, the real issue with this operation is the disk I/O. If you need too many accesses by row id the database might choose a full scan.</p>
<h4 id="Others-paths"><a href="#Others-paths" class="headerlink" title="Others paths"></a>Others paths</h4><p>I didn’t present all the access paths. If you want to know more, you can read the <a href="https://docs.oracle.com/database/121/TGSQL/tgsql_optop.htm" target="_blank" rel="noopener">Oracle documentation</a>. The names might not be the same for the other databases but the concepts behind are the same.</p>
<h3 id="Join-operators"><a href="#Join-operators" class="headerlink" title="Join operators"></a>Join operators</h3><p>So, we know how to get our data, let’s join them!</p>
<p>I’ll present the 3 common join operators: Merge Join, Hash Join and Nested Loop Join. But before that, I need to introduce new vocabulary: <strong>inner relation</strong> and <strong>outer relation</strong>. A relation can be:</p>
<ul>
<li>a table</li>
<li>an index</li>
<li>an intermediate result from a previous operation (for example the result of a previous join)</li>
</ul>
<p>When you’re joining two relations, the join algorithms manage the two relations differently. In the rest of the article, I’ll assume that:</p>
<ul>
<li>the outer relation is the left data set</li>
<li>the inner relation is the right data set</li>
</ul>
<p>For example, A JOIN B is the join between A and B where A is the outer relation and B the inner relation.</p>
<p>Most of the time, <strong>the cost of A JOIN B is not the same as the cost of B JOIN A.</strong></p>
<p><strong>In this part, I’ll also assume that the outer relation has N elements</strong> <strong>and the inner relation M elements</strong>. Keep in mind that a real optimizer knows the values of N and M with the statistics.</p>
<p>Note: N and M are the cardinalities of the relations.</p>
<h4 id="Nested-loop-join"><a href="#Nested-loop-join" class="headerlink" title="Nested loop join"></a>Nested loop join</h4><p>The nested loop join is the easiest one.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/nested_loop_join.png" target="_blank" rel="noopener"><img src="../../public/images/nested_loop_join-20200207203251391.png" alt="nested loop join in databases"></a></p>
<p>Here is the idea:</p>
<ul>
<li>for each row in the outer relation</li>
<li>you look at all the rows in the inner relation to see if there are rows that match</li>
</ul>
<p>Here is a pseudo code:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nested_loop_join(array outer, array inner)&#96;&#96; &#96;&#96;for&#96; &#96;each row a in outer&#96;&#96;  &#96;&#96;for&#96; &#96;each row b in inner&#96;&#96;   &#96;&#96;if&#96; &#96;(match_join_condition(a,b))&#96;&#96;    &#96;&#96;write_result_in_output(a,b)&#96;&#96;   &#96;&#96;end &#96;&#96;if&#96;&#96;  &#96;&#96;end &#96;&#96;for&#96;&#96;  &#96;&#96;end &#96;&#96;for</span><br></pre></td></tr></table></figure>

<p>Since it’s a double iteration, the <strong>time complexity is O(N*M)</strong></p>
<p>In term of disk I/O, for each of the N rows in the outer relation, the inner loop needs to read M rows from the inner relation. This algorithm needs to read N + N<em>M rows from disk. But, if the inner relation is small enough, you can put the relation in memory and just have M +N reads. With this modification, *</em>the inner relation must be the smallest one** since it has more chance to fit in memory.</p>
<p>In terms of time complexity it makes no difference but in terms of disk I/O it’s way better to read only once both relations.   </p>
<p>Of course, the inner relation can be replaced by an index, it will be better for the disk I/O.</p>
<p>Since this algorithm is very simple, here is another version that is more disk I/O friendly if the inner relation is too big to fit in memory. Here is the idea:</p>
<ul>
<li>instead of reading both relation row by row,</li>
<li>you read them bunch by bunch and keep 2 bunches of rows (from each relation) in memory,</li>
<li>you compare the rows inside the two bunches and keep the rows that match,</li>
<li>then you load new bunches from disk and compare them</li>
<li>and so on until there are no bunches to load.</li>
</ul>
<p>Here is a possible algorithm:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; improved version to reduce the disk I&#x2F;O.&#96;&#96;nested_loop_join_v2(file outer, file inner)&#96;&#96; &#96;&#96;for&#96; &#96;each bunch ba in outer&#96;&#96; &#96;&#96;&#x2F;&#x2F; ba is now in memory&#96;&#96;  &#96;&#96;for&#96; &#96;each bunch bb in inner&#96;&#96;    &#96;&#96;&#x2F;&#x2F; bb is now in memory&#96;&#96;    &#96;&#96;for&#96; &#96;each row a in ba&#96;&#96;     &#96;&#96;for&#96; &#96;each row b in bb&#96;&#96;      &#96;&#96;if&#96; &#96;(match_join_condition(a,b))&#96;&#96;       &#96;&#96;write_result_in_output(a,b)&#96;&#96;      &#96;&#96;end &#96;&#96;if&#96;&#96;     &#96;&#96;end &#96;&#96;for&#96;&#96;    &#96;&#96;end &#96;&#96;for&#96;&#96;  &#96;&#96;end &#96;&#96;for&#96;&#96;  &#96;&#96;end &#96;&#96;for</span><br></pre></td></tr></table></figure>



<p><strong>With this version, the time complexity remains the same, but the number of disk access decreases</strong>:</p>
<ul>
<li>With the previous version, the algorithm needs N + N*M accesses (each access gets one row).</li>
<li>With this new version, the number of disk accesses becomes number_of_bunches_for(outer)+ number_of_ bunches_for(outer)* number_of_ bunches_for(inner).</li>
<li>If you increase the size of the bunch you reduce the number of disk accesses.</li>
</ul>
<p>Note: Each disk access gathers more data than the previous algorithm but it doesn’t matter since they’re sequential accesses (the real issue with mechanical disks is the time to get the first data).</p>
<h4 id="Hash-join"><a href="#Hash-join" class="headerlink" title="Hash join"></a>Hash join</h4><p>The hash join is more complicated but gives a better cost than a nested loop join in many situations.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/hash_join.png" target="_blank" rel="noopener"><img src="../../public/images/hash_join-20200207203251391.png" alt="hash join in a database"></a></p>
<p>The idea of the hash join is to:</p>
<ul>
<li>1) Get all elements from the inner relation</li>
<li>2) Build an in-memory hash table</li>
<li>3) Get all elements of the outer relation one by one</li>
<li>4) Compute the hash of each element (with the hash function of the hash table) to find the associated bucket of the inner relation</li>
<li>5) find if there is a match between the elements in the bucket and the element of the outer table</li>
</ul>
<p>In terms of time complexity I need to make some assumptions to simplify the problem:</p>
<ul>
<li>The inner relation is divided into X buckets</li>
<li>The hash function distributes hash values almost uniformly for both relations. In other words the buckets are equally sized.</li>
<li>The matching between an element of the outer relation and all elements inside a bucket costs the number of elements inside the buckets.</li>
</ul>
<p>The time complexity is (M/X) * N + cost_to_create_hash_table(M) + cost_of_hash_function*N</p>
<p>If the Hash function creates enough small-sized buckets then <strong>the time complexity is O(M+N)</strong></p>
<p>Here is another version of the hash join which is more memory friendly but less disk I/O friendly. This time:</p>
<ul>
<li>1) you compute the hash tables for both the inner and outer relations</li>
<li>2) then you put them on disk</li>
<li>3) then you compare the 2 relations bucket by bucket (with one loaded in-memory and the other read row by row)</li>
</ul>
<h4 id="Merge-join"><a href="#Merge-join" class="headerlink" title="Merge join"></a>Merge join</h4><p><strong>The merge join is the only join that produces a sorted result.</strong></p>
<p>Note: In this simplified merge join, there are no inner or outer tables; they both play the same role. But real implementations make a difference, for example, when dealing with duplicates.</p>
<p>The merge join can be divided into of two steps:</p>
<ol>
<li>(Optional) Sort join operations: Both the inputs are sorted on the join key(s).</li>
<li>Merge join operation: The sorted inputs are merged together.</li>
</ol>
<p>Sort</p>
<p>We already spoke about the merge sort, in this case a merge sort in a good algorithm (but not the best if memory is not an issue).</p>
<p>But sometimes the data sets are already sorted, for example:</p>
<ul>
<li>If the table is natively ordered, for example an index-organized table on the join condition</li>
<li>If the relation is an index on the join condition</li>
<li>If this join is applied on an intermediate result already sorted during the process of the query</li>
</ul>
<p>Merge join</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/merge_join.png" target="_blank" rel="noopener"><img src="../../public/images/merge_join-20200207203251391.png" alt="merge join in a database"></a></p>
<p>This part is very similar to the merge operation of the merge sort we saw. But this time, instead of picking every element from both relations, we only pick the elements from both relations that are equals. Here is the idea:</p>
<ul>
<li>1) you compare both current elements in the 2 relations (current=first for the first time)</li>
<li>2) if they’re equal, then you put both elements in the result and you go to the next element for both relations</li>
<li>3) if not, you go to the next element for the relation with the lowest element (because the next element might match)</li>
<li>4) and repeat 1,2,3 until you reach the last element of one of the relation.</li>
</ul>
<p>This works because both relations are sorted and therefore you don’t need to “go back” in these relations.</p>
<p>This algorithm is a simplified version because it doesn’t handle the case where the same data appears multiple times in both arrays (in other words a multiple matches). The real version is more complicated “just” for this case; this is why I chose a simplified version.</p>
<p>If both relations are already sorted then <strong>the time complexity is O(N+M)</strong></p>
<p>If both relations need to be sorted then the time complexity is the cost to sort both relations: <strong>O(N*Log(N) + M*Log(M))</strong></p>
<p>For the CS geeks, here is a possible algorithm that handles the multiple matches (note: I’m not 100% sure about my algorithm):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mergeJoin(relation a, relation b)&#96;&#96; &#96;&#96;relation output&#96;&#96; &#96;&#96;integer a_key:&#x3D;&#96;&#96;0&#96;&#96;;&#96;&#96; &#96;&#96;integer b_key:&#x3D;&#96;&#96;0&#96;&#96;;&#96;&#96; &#96; &#96; &#96;&#96;while&#96; &#96;(a[a_key]!&#x3D;&#96;&#96;null&#96; &#96;or b[b_key]!&#x3D;&#96;&#96;null&#96;&#96;)&#96;&#96;  &#96;&#96;if&#96; &#96;(a[a_key] &lt; b[b_key])&#96;&#96;   &#96;&#96;a_key++;&#96;&#96;  &#96;&#96;else&#96; &#96;if&#96; &#96;(a[a_key] &gt; b[b_key])&#96;&#96;   &#96;&#96;b_key++;&#96;&#96;  &#96;&#96;else&#96; &#96;&#x2F;&#x2F;Join predicate satisfied&#96;&#96;  &#96;&#96;&#x2F;&#x2F;i.e. a[a_key] &#x3D;&#x3D; b[b_key]&#96; &#96;   &#96;&#96;&#x2F;&#x2F;count the number of duplicates in relation a&#96;&#96;   &#96;&#96;integer nb_dup_in_a &#x3D; &#96;&#96;1&#96;&#96;:&#96;&#96;   &#96;&#96;while&#96; &#96;(a[a_key]&#x3D;&#x3D;a[a_key+nb_dup_in_a])&#96;&#96;    &#96;&#96;nb_dup_in_a++;&#96;&#96;    &#96; &#96;   &#96;&#96;&#x2F;&#x2F;count the number of duplicates in relation b&#96;&#96;   &#96;&#96;integer dup_in_b &#x3D; &#96;&#96;1&#96;&#96;:&#96;&#96;   &#96;&#96;while&#96; &#96;(b[b_key]&#x3D;&#x3D;b[b_key+nb_dup_in_b])&#96;&#96;    &#96;&#96;nb_dup_in_b++;&#96;&#96;    &#96; &#96;   &#96;&#96;&#x2F;&#x2F;write the duplicates in output&#96;&#96;    &#96;&#96;for&#96; &#96;(&#96;&#96;int&#96; &#96;i &#x3D; &#96;&#96;0&#96; &#96;; i&lt; nb_dup_in_a ; i++)&#96;&#96;     &#96;&#96;for&#96; &#96;(&#96;&#96;int&#96; &#96;j &#x3D; &#96;&#96;0&#96; &#96;; i&lt; nb_dup_in_b ; i++)   &#96;&#96;      &#96;&#96;write_result_in_output(a[a_key+i],b[b_key+j])&#96;&#96;      &#96; &#96;   &#96;&#96;a_key&#x3D;a_key + nb_dup_in_a-&#96;&#96;1&#96;&#96;;&#96;&#96;   &#96;&#96;b_key&#x3D;b_key + nb_dup_in_b-&#96;&#96;1&#96;&#96;;&#96; &#96;  &#96;&#96;end &#96;&#96;if&#96;&#96; &#96;&#96;end &#96;&#96;while</span><br></pre></td></tr></table></figure>



<h4 id="Which-one-is-the-best"><a href="#Which-one-is-the-best" class="headerlink" title="Which one is the best?"></a>Which one is the best?</h4><p>If there was a best type of joins, there wouldn’t be multiple types. This question is very difficult because many factors come into play like:</p>
<ul>
<li>The <strong>amount of free memory</strong>: without enough memory you can say goodbye to the powerful hash join (at least the full in-memory hash join)</li>
<li>The <strong>size of the 2 data sets</strong>. For example if you have a big table with a very small one, a nested loop join will be faster than a hash join because the hash join has an expensive creation of hashes. If you have 2 very large tables the nested loop join will be very CPU expensive.</li>
<li>The <strong>presence</strong> <strong>of</strong> <strong>indexes</strong>. With 2 B+Tree indexes the smart choice seems to be the merge join</li>
<li>If <strong>the result need to be sorted</strong>: Even if you’re working with unsorted data sets, you might want to use a costly merge join (with the sorts) because at the end the result will be sorted and you’ll be able to chain the result with another merge join (or maybe because the query asks implicitly/explicitly for a sorted result with an ORDER BY/GROUP BY/DISTINCT operation)</li>
<li>If <strong>the relations are already sorted</strong>: In this case the merge join is the best candidate</li>
<li>The type of joins you’re doing: is it an <strong>equijoin</strong> (i.e.: tableA.col1 = tableB.col2)? Is it an <strong>inner join</strong>, an <strong>outer join,</strong> a <strong>cartesian product</strong> or a <strong>self-join</strong>? Some joins can’t work in certain situations.</li>
<li>The <strong>distribution of data</strong>. If the data on the join condition are <strong>skewed</strong> (For example you’re joining people on their last name but many people have the same), using a hash join will be a disaster because the hash function will create ill-distributed buckets.</li>
<li>If you want the join to be executed by <strong>multiple threads/process</strong></li>
</ul>
<p>For more information, you can read the <a href="https://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.admin.perf.doc/doc/c0005311.html" target="_blank" rel="noopener">DB2</a>, <a href="http://docs.oracle.com/cd/B28359_01/server.111/b28274/optimops.htm#i76330" target="_blank" rel="noopener">ORACLE</a> or <a href="https://technet.microsoft.com/en-us/library/ms191426(v=sql.105).aspx" target="_blank" rel="noopener">SQL Server</a> documentations.</p>
<h3 id="Simplified-example"><a href="#Simplified-example" class="headerlink" title="Simplified example"></a>Simplified example</h3><p>We’ve just seen 3 types of join operations.</p>
<p>Now let’s say we need to join 5 tables to have a full view of a person. A PERSON can have:</p>
<ul>
<li>multiple MOBILES</li>
<li>multiple MAILS</li>
<li>multiple ADRESSES</li>
<li>multiple BANK_ACCOUNTS</li>
</ul>
<p>In other words we need a quick answer for the following query:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;* &#96;&#96;from&#96; &#96;PERSON, MOBILES, MAILS,ADRESSES, BANK_ACCOUNTS&#96;&#96;WHERE&#96;&#96;PERSON.PERSON_ID &#x3D; MOBILES.PERSON_ID&#96;&#96;AND&#96; &#96;PERSON.PERSON_ID &#x3D; MAILS.PERSON_ID&#96;&#96;AND&#96; &#96;PERSON.PERSON_ID &#x3D; ADRESSES.PERSON_ID&#96;&#96;AND&#96; &#96;PERSON.PERSON_ID &#x3D; BANK_ACCOUNTS.PERSON_ID</span><br></pre></td></tr></table></figure>

<p>As a query optimizer, I have to find the best way to process the data. But there are 2 problems:</p>
<ul>
<li>What kind of join should I use for each join?</li>
</ul>
<p>I have 3 possible joins (Hash Join, Merge Join, Nested Join) with the possibility to use 0,1 or 2 indexes (not to mention that there are different types of indexes).</p>
<ul>
<li>What order should I choose to compute the join?</li>
</ul>
<p>For example, the following figure shows different possible plans for only 3 joins on 4 tables</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/join_ordering_problem.png" target="_blank" rel="noopener"><img src="../../public/images/join_ordering_problem-20200207203251393.png" alt="join ordering optimization problem in a database"></a></p>
<p>So here are my possibilities:</p>
<ul>
<li>1) I use a brute force approach</li>
</ul>
<p>Using the database statistics, I <strong>compute the cost for every possible plan</strong> and I keep the best one. But there are many possibilities. For a given order of joins, each join has 3 possibilities: HashJoin, MergeJoin, NestedJoin. So, for a given order of joins there are 34 possibilities. The join ordering is a <a href="https://en.wikipedia.org/wiki/Catalan_number" target="_blank" rel="noopener">permutation problem on a binary tree</a> and there are (2<em>4)!/(4+1)! possible orders. For this very simplified problem, I end up with 34</em>(2*4)!/(4+1)! possibilities.</p>
<p>In non-geek terms, it means 27 216 possible plans. If I now add the possibility for the merge join to take 0,1 or 2 B+Tree indexes, the number of possible plans becomes 210 000. Did I forget to mention that this query is VERY SIMPLE?</p>
<ul>
<li>2) I cry and quit this job</li>
</ul>
<p>It’s very tempting but you wouldn’t get your result and I need money to pay the bills.</p>
<ul>
<li>3) I only try a few plans and take the one with the lowest cost.</li>
</ul>
<p>Since I’m not superman, I can’t compute the cost of every plan. Instead, I can <strong>arbitrary choose a subset of all the possible plans</strong>, compute their costs and give you the best plan of this subset.</p>
<ul>
<li>4) I apply smart <strong>rules to reduce the number of possible plans</strong>.</li>
</ul>
<p>There are 2 types of rules:</p>
<p>I can use “logical” rules that will remove useless possibilities but they won’t filter a lot of possible plans. For example: “the inner relation of the nested loop join must be the smallest data set”</p>
<p>I accept not finding the best solution and apply more aggressive rules to reduce a lot the number of possibilities. For example “If a relation is small, use a nested loop join and never use a merge join or a hash join”</p>
<p>In this simple example, I end up with many possibilities. But <strong>a real query can have other relational operators</strong> like OUTER JOIN, CROSS JOIN, GROUP BY, ORDER BY, PROJECTION, UNION, INTERSECT, DISTINCT … <strong>which means even more possibilities</strong>.</p>
<p>So, how a database does it?</p>
<h3 id="Dynamic-programming-greedy-algorithm-and-heuristic"><a href="#Dynamic-programming-greedy-algorithm-and-heuristic" class="headerlink" title="Dynamic programming, greedy algorithm and heuristic"></a>Dynamic programming, greedy algorithm and heuristic</h3><p>A relational database tries the multiple approaches I’ve just said. The real job of an optimizer is to find a good solution on a limited amount of time.</p>
<p><strong>Most of the time an optimizer doesn’t find the best solution but a “good” one</strong>.</p>
<p>For small queries, doing a brute force approach is possible. But there is a way to avoid unnecessary computations so that even medium queries can use the brute force approach. This is called dynamic programming.</p>
<h4 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a>Dynamic Programming</h4><p>The idea behind these 2 words is that many executions plan are very similar. If you look at the following plans:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/overlapping_trees.png" target="_blank" rel="noopener"><img src="../../public/images/overlapping_trees-20200207203251396.png" alt="overlapping trees optimization dynamic programming"></a></p>
<p>They share the same (A JOIN B) subtree. So, instead of computing the cost of this subtree in every plan, we can compute it once, save the computed cost and reuse it when we see this subtree again. More formally, we’re facing an overlapping problem. To avoid the extra-computation of the partial results we’re using memoization.</p>
<p>Using this technique, instead of having a (2<em>N)!/(N+1)! time complexity, we “just” have 3N. In our previous example with 4 joins, it means passing from 336 ordering to 81. If you take a bigger *</em>query with 8 joins** (which is not big)<strong>, it means passing from 57 657 600 to 6561</strong>.</p>
<p>For the CS geeks, here is an algorithm I found on the <a href="http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch13.ppt" target="_blank" rel="noopener">formal course I already gave you</a>. I won’t explain this algorithm so read it only if you already know dynamic programming or if you’re good with algorithms (you’ve been warned!):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">procedure findbestplan(S)&#96;&#96;if&#96; &#96;(bestplan[S].cost infinite)&#96;&#96;  &#96;&#96;return&#96; &#96;bestplan[S]&#96;&#96;&#x2F;&#x2F; else bestplan[S] has not been computed earlier, compute it now&#96;&#96;if&#96; &#96;(S contains only &#96;&#96;1&#96; &#96;relation)&#96;&#96;     &#96;&#96;set bestplan[S].plan and bestplan[S].cost based on the best way&#96;&#96;     &#96;&#96;of accessing S &#96;&#96;&#x2F;* Using selections on S and indices on S *&#x2F;&#96;&#96;   &#96;&#96;else&#96; &#96;for&#96; &#96;each non-empty subset S1 of S such that S1 !&#x3D; S&#96;&#96;  &#96;&#96;P1&#x3D; findbestplan(S1)&#96;&#96;  &#96;&#96;P2&#x3D; findbestplan(S - S1)&#96;&#96;  &#96;&#96;A &#x3D; best algorithm &#96;&#96;for&#96; &#96;joining results of P1 and P2&#96;&#96;  &#96;&#96;cost &#x3D; P1.cost + P2.cost + cost of A&#96;&#96;  &#96;&#96;if&#96; &#96;cost &lt; bestplan[S].cost&#96;&#96;    &#96;&#96;bestplan[S].cost &#x3D; cost&#96;&#96;   &#96;&#96;bestplan[S].plan &#x3D; “execute P1.plan; execute P2.plan;&#96;&#96;         &#96;&#96;join results of P1 and P2 using A”&#96;&#96;return&#96; &#96;bestplan[S]</span><br></pre></td></tr></table></figure>



<p>For bigger queries you can still do a dynamic programming approach but with extra rules (or <strong>heuristics</strong>) to remove possibilities:</p>
<ul>
<li>If we analyze only a certain type of plan (for example: the left-deep trees) we end up with n*2n instead of 3n</li>
</ul>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/left-deep-tree.png" target="_blank" rel="noopener"><img src="../../public/images/left-deep-tree-20200207203251418.png" alt="left deep tree example"></a></p>
<ul>
<li>If we add logical rules to avoid plans for some patterns (like “if a table as an index for the given predicate, don’t try a merge join on the table but only on the index”) it will reduce the number of possibilities without hurting to much the best possible solution.</li>
<li>If we add rules on the flow (like “perform the join operations BEFORE all the other relational operations”) it also reduces a lot of possibilities.</li>
<li>…</li>
</ul>
<h4 id="Greedy-algorithms"><a href="#Greedy-algorithms" class="headerlink" title="Greedy algorithms"></a>Greedy algorithms</h4><p>But for a very big query or to have a very fast answer (but not a very fast query), another type of algorithms is used, the greedy algorithms.</p>
<p>The idea is to follow a rule (or <strong>heuristic</strong>) to build a query plan in an incremental way. With this rule, a greedy algorithm finds the best solution to a problem one step at a time. The algorithm starts the query plan with one JOIN. Then, at each step, the algorithm adds a new JOIN to the query plan using the same rule.</p>
<p>Let’s take a simple example. Let’s say we have a query with 4 joins on 5 tables (A, B, C, D and E). To simplify the problem we just take the nested join as a possible join. Let’s use the rule “use the join with the lowest cost”</p>
<ul>
<li>we arbitrary start on one of the 5 tables (let’s choose A)</li>
<li>we compute the cost of every join with A (A being the inner or outer relation).</li>
<li>we find that A JOIN B gives the lowest cost.</li>
<li>we then compute the cost of every join with the result of A JOIN B (A JOIN B being the inner or outer relation).</li>
<li>we find that (A JOIN B) JOIN C gives the best cost.</li>
<li>we then compute the cost of every join with the result of the (A JOIN B) JOIN C …</li>
<li>….</li>
<li>At the end we find the plan (((A JOIN B) JOIN C) JOIN D) JOIN E)</li>
</ul>
<p>Since we arbitrary started with A, we can apply the same algorithm for B, then C then D then E. We then keep the plan with the lowest cost.</p>
<p>By the way, this algorithm has a name: it’s called the <a href="https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm" target="_blank" rel="noopener">Nearest neighbor algorithm</a>.</p>
<p>I won’t go into details, but with a good modeling and a sort in N<em>log(N) this problem can <a href="http://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&cad=rja&uact=8&ved=0CE0QFjAEahUKEwjR8OLUmv3GAhUJuxQKHdU-DAA&url=http%3A%2F%2Fwww.cs.bu.edu%2F~steng%2Fteaching%2FSpring2004%2Flectures%2Flecture3.ppt&ei=hyK3VZGRAYn2UtX9MA&usg=AFQjCNGL41kMNkG5cH" target="_blank" rel="noopener">easily be solved</a>. The *</em>cost of this algorithm is in O(N*log(N)) vs O(3N) for the full dynamic programming version**. If you have a big query with 20 joins, it means 26 vs 3 486 784 401, a BIG difference!</p>
<p>The problem with this algorithm is that we assume that finding the best join between 2 tables will give us the best cost if we keep this join and add a new join. But:</p>
<ul>
<li>even if A JOIN B gives the best cost between A, B and C</li>
<li>(A JOIN C) JOIN B might give a better result than (A JOIN B) JOIN C.</li>
</ul>
<p>To improve the result, you can run multiple greedy algorithms using different rules and keep the best plan.</p>
<h4 id="Other-algorithms"><a href="#Other-algorithms" class="headerlink" title="Other algorithms"></a>Other algorithms</h4><p>[If you’re already fed up with algorithms, skip to the next part, what I’m going to say is not important for the rest of the article]</p>
<p>The problem of finding the best possible plan is an active research topic for many CS researchers. They often try to find better solutions for more precise problems/patterns. For example,</p>
<ul>
<li>if the query is a star join (it’s a certain type of multiple-join query), some databases will use a specific algorithm.</li>
<li>if the query is a parallel query, some databases will use a specific algorithm</li>
<li>…</li>
</ul>
<p>Other algorithms are also studied to replace dynamic programming for large queries. Greedy algorithms belong to larger family called <strong>heuristic algorithms</strong>. A greedy algorithm follows a rule (or heuristic), keeps the solution it found at the previous step and “appends” it to find the solution for the current step. Some algorithms follow a rule and apply it in a step-by-step way but don’t always keep the best solution found in the previous step. They are called heuristic algorithms.</p>
<p>For example, <strong>genetic algorithms</strong> follow a rule but the best solution of the last step is not often kept:</p>
<ul>
<li>A solution represents a possible full query plan</li>
<li>Instead of one solution (i.e. plan) there are P solutions (i.e. plans) kept at each step.</li>
<li>0) P query plans are randomly created</li>
<li>1) Only the plans with the best costs are kept</li>
<li>2) These best plans are mixed up to produce P news plans</li>
<li>3) Some of the P new plans are randomly modified</li>
<li>4) The step 1,2,3 are repeated T times</li>
<li>5) Then you keep the best plan from the P plans of the last loop.</li>
</ul>
<p>The more loops you do the better the plan will be.</p>
<p>Is it magic? No, it’s the laws of nature: only the fittest survives!</p>
<p>FYI, genetic algorithms are implemented in <a href="http://www.postgresql.org/docs/9.4/static/geqo-intro.html" target="_blank" rel="noopener">PostgreSQL</a> but I wasn’t able to find if they’re used by default.</p>
<p>There are other heuristic algorithms used in databases like Simulated Annealing, Iterative Improvement, Two-Phase Optimization… But I don’t know if they’re currently used in enterprise databases or if they’re only used in research databases.</p>
<p>For more information, you can read the following research article that presents more possible algorithms: <a href="http://www.acad.bg/rismim/itc/sub/archiv/Paper6_1_2009.PDF" target="_blank" rel="noopener">Review of Algorithms for the Join Ordering Problem in Database Query Optimization</a></p>
<h3 id="Real-optimizers"><a href="#Real-optimizers" class="headerlink" title="Real optimizers"></a>Real optimizers</h3><p>[You can skip to the next part, what I’m going to say is not important]</p>
<p>But, all this blabla is very theoretical. Since I’m a developer and not a researcher, I like <strong>concrete examples</strong>.</p>
<p>Let’s see how the <a href="https://www.sqlite.org/optoverview.html" target="_blank" rel="noopener">SQLite optimizer</a> works. It’s a light database so it uses a simple optimization based on a greedy algorithm with extra-rules to limit the number of possibilities:</p>
<ul>
<li>SQLite chooses to never reorder tables in a CROSS JOIN operator</li>
<li><strong>joins are implemented as nested joins</strong></li>
<li>outer joins are always evaluated in the order in which they occur</li>
<li>…</li>
<li>Prior to version 3.8.0, <strong>SQLite uses the “Nearest Neighbor” greedy algorithm when searching for the best query plan</strong></li>
</ul>
<p>Wait a minute … we’ve already seen this algorithm! What a coincidence!</p>
<ul>
<li>Since version 3.8.0 (released in 2015), SQLite uses the “<a href="https://www.sqlite.org/queryplanner-ng.html" target="_blank" rel="noopener">N Nearest Neighbors</a>” <strong>greedy algorithm</strong> when searching for the best query plan</li>
</ul>
<p>Let’s see how another optimizer does his job. IBM DB2 is like all the enterprise databases but I’ll focus on this one since it’s the last one I’ve really used before switching to Big Data.</p>
<p>If we look at the <a href="https://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.admin.perf.doc/doc/r0005278.html" target="_blank" rel="noopener">official documentation</a>, we learn that the DB2 optimizer let you use 7 different levels of optimization:</p>
<ul>
<li>Use greedy algorithms for the joins<ul>
<li>0 – minimal optimization, use index scan and nested-loop join and avoid some Query Rewrite</li>
<li>1 – low optimization</li>
<li>2 – full optimization</li>
</ul>
</li>
<li>Use dynamic programming for the joins<ul>
<li>3 – moderate optimization and rough approximation</li>
<li>5 – full optimization, uses all techniques with heuristics</li>
<li>7 – full optimization similar to 5, without heuristics</li>
<li>9 – maximal optimization spare no effort/expense <strong>considers all possible join orders, including Cartesian products</strong></li>
</ul>
</li>
</ul>
<p>We can see that <strong>DB2 uses greedy algorithms and dynamic programming</strong>. Of course, they don’t share the heuristics they use since the query optimizer is the main power of a database.</p>
<p>FYI, <strong>the default level is 5.</strong> By default the optimizer uses the following characteristics:</p>
<ul>
<li><p><strong>All available statistics</strong>, including frequent-value and quantile statistics, are used.</p>
</li>
<li><p><strong>All query rewrite rules</strong> (including materialized query table routing) are applied, except computationally intensive rules that are applicable only in very rare cases.</p>
</li>
<li><p>Dynamic programming join enumeration</p>
</li>
</ul>
<p>  is used, with:</p>
<ul>
<li>Limited use of composite inner relation</li>
<li>Limited use of Cartesian products for star schemas involving lookup tables</li>
</ul>
<ul>
<li>A wide range of access methods is considered, including list prefetch (note: will see what is means), index ANDing (note: a special operation with indexes), and materialized query table routing.</li>
</ul>
<p>By default, <strong>DB2 uses dynamic programming limited by heuristics for the join ordering</strong>.</p>
<p>The others conditions (GROUP BY, DISTINCT…) are handled by simple rules.</p>
<h3 id="Query-Plan-Cache"><a href="#Query-Plan-Cache" class="headerlink" title="Query Plan Cache"></a>Query Plan Cache</h3><p>Since the creation of a plan takes time, most databases store the plan into a <strong>query plan cache</strong> to avoid useless re-computations of the same query plan. It’s kind of a big topic since the database needs to know when to update the outdated plans. The idea is to put a threshold and if the statistics of a table have changed above this threshold then the query plan involving this table is purged from the cache.</p>
<h2 id="Query-executor"><a href="#Query-executor" class="headerlink" title="Query executor"></a>Query executor</h2><p>At this stage we have an optimized execution plan. This plan is compiled to become an executable code. Then, if there are enough resources (memory, CPU) it is executed by the query executor. The operators in the plan (JOIN, SORT BY …) can be executed in a sequential or parallel way; it’s up to the executor. To get and write its data, the query executor interacts with the data manager, which is the next part of the article.</p>
<h1 id="Data-manager"><a href="#Data-manager" class="headerlink" title="Data manager"></a>Data manager</h1><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/data_manager.png" target="_blank" rel="noopener"><img src="../../public/images/data_manager-20200207203251432.png" alt="data manager in databases"></a></p>
<p>At this step, the query manager is executing the query and needs the data from the tables and indexes. It asks the data manager to get the data, but there are 2 problems:</p>
<ul>
<li>Relational databases use a transactional model. So, you can’t get any data at any time because someone else might be using/modifying the data at the same time.</li>
<li><strong>Data retrieval is the slowest operation in a database</strong>, therefore the data manager needs to be smart enough to get and keep data in memory buffers.</li>
</ul>
<p>In this part, we’ll see how relational databases handle these 2 problems. I won’t talk about the way the data manager gets its data because it’s not the most important (and this article is long enough!).</p>
<h2 id="Cache-manager"><a href="#Cache-manager" class="headerlink" title="Cache manager"></a>Cache manager</h2><p>As I already said, the main bottleneck of databases is disk I/O. To improve performance, modern databases use a cache manager.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/cache_manager.png" target="_blank" rel="noopener"><img src="../../public/images/cache_manager-20200207203251433.png" alt="cache manager in databases"></a></p>
<p>Instead of directly getting the data from the file system, the query executor asks for the data to the cache manager. The cache manager has an in-memory cache called <strong>buffer pool</strong>. <strong>Getting data from memory dramatically speeds up a database</strong>. It’s difficult to give an order of magnitude because it depends on the operation you need to do:</p>
<ul>
<li>sequential access (ex: full scan) vs random access (ex: access by row id),</li>
<li>read vs write</li>
</ul>
<p>and the type of disks used by the database:</p>
<ul>
<li>7.2k/10k/15k rpm HDD</li>
<li>SSD</li>
<li>RAID 1/5/…</li>
</ul>
<p>but I’d say <strong>memory is 100 to 100k times faster than disk</strong>.</p>
<p>But, this leads to another problem (as always with databases…). The cache manager needs to get the data in memory BEFORE the query executor uses them; otherwise the query manager has to wait for the data from the slow disks.</p>
<h3 id="Prefetching"><a href="#Prefetching" class="headerlink" title="Prefetching"></a>Prefetching</h3><p>This problem is called prefetching. A query executor knows the data it’ll need because it knows the full flow of the query and has knowledge of the data on disk with the statistics. Here is the idea:</p>
<ul>
<li>When the query executor is processing its first bunch of data</li>
<li>It asks the cache manager to pre-load the second bunch of data</li>
<li>When it starts processing the second bunch of data</li>
<li>It asks the CM to pre-load the third bunch and informs the CM that the first bunch can be purged from cache.</li>
<li>…</li>
</ul>
<p>The CM stores all these data in its buffer pool. In order to know if a data is still needed, the cache manager adds an extra-information about the cached data (called a <strong>latch</strong>).</p>
<p>Sometimes the query executor doesn’t know what data it’ll need and some databases don’t provide this functionality. Instead, they use a speculative prefetching (for example: if the query executor asked for data 1,3,5 it’ll likely ask for 7,9,11 in a near future) or a sequential prefetching (in this case the CM simply loads from disks the next contiguous data after the ones asked).</p>
<p>To monitor how well the prefetching is working, modern databases provide a metric called <strong>buffer/cache hit ratio</strong>. The hit ratio shows how often a requested data has been found in the buffer cache without requiring disk access.</p>
<p>Note: a poor cache hit ratio doesn’t always mean that the cache is ill-working. For more information, you can read the <a href="http://docs.oracle.com/database/121/TGDBA/tune_buffer_cache.htm" target="_blank" rel="noopener">Oracle documentation</a>.</p>
<p>But, a buffer is a <strong>limited</strong> amount of memory. Therefore, it needs to remove some data to be able to load new ones. Loading and purging the cache has a cost in terms of disk and network I/O. If you have a query that is often executed, it wouldn’t be efficient to always load then purge the data used by this query. To handle this problem, modern databases use a buffer replacement strategy.</p>
<h3 id="Buffer-Replacement-strategies"><a href="#Buffer-Replacement-strategies" class="headerlink" title="Buffer-Replacement strategies"></a>Buffer-Replacement strategies</h3><p>Most modern databases (at least SQL Server, MySQL, Oracle and DB2) use an LRU algorithm.</p>
<h4 id="LRU"><a href="#LRU" class="headerlink" title="LRU"></a>LRU</h4><p><strong>LRU</strong> stands for <strong>L</strong>east <strong>R</strong>ecently <strong>U</strong>sed. The idea behind this algorithm is to keep in the cache the data that have been recently used and, therefore, are more likely to be used again.</p>
<p>Here is a visual example:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/LRU.png" target="_blank" rel="noopener"><img src="../../public/images/LRU-20200207203251433.png" alt="LRU algorithm in a database"></a></p>
<p>For the sake of comprehension, I’ll assume that the data in the buffer are not locked by latches (and therefore can be removed). In this simple example the buffer can store 3 elements:</p>
<ul>
<li>1: the cache manager uses the data 1 and puts the data into the empty buffer</li>
<li>2: the CM uses the data 4 and puts the data into the half-loaded buffer</li>
<li>3: the CM uses the data 3 and puts the data into the half-loaded buffer</li>
<li>4: the CM uses the data 9. The buffer is full so <strong>data 1 is removed</strong> <strong>since it’s the last recently used data</strong>. Data 9 is added into the buffer</li>
<li>5: the CM uses the data 4. <strong>Data 4 is already in the buffer therefore it becomes the first recently used data again</strong>.</li>
<li>6: the CM uses the data 1. The buffer is full so <strong>data 9 is removed</strong> <strong>since it’s the last recently used data</strong>. Data 1 is added into the buffer</li>
<li>…</li>
</ul>
<p>This algorithm works well but there are some limitations. What if there is a full scan on a large table? In other words, what happens when the size of the table/index is above the size of the buffer? Using this algorithm will remove all the previous values in the cache whereas the data from the full scan are likely to be used only once.</p>
<h4 id="Improvements"><a href="#Improvements" class="headerlink" title="Improvements"></a>Improvements</h4><p>To prevent this to happen, some databases add specific rules. For example according to <a href="http://docs.oracle.com/database/121/CNCPT/memory.htm#i10221" target="_blank" rel="noopener">Oracle documentation</a>:</p>
<blockquote>
<p>“For very large tables, the database typically uses a direct path read, which loads blocks directly […], to avoid populating the buffer cache. For medium size tables, the database may use a direct read or a cache read. If it decides to use a cache read, then the database places the blocks at the end of the LRU list to prevent the scan from effectively cleaning out the buffer cache.”</p>
</blockquote>
<p>There are other possibilities like using an advanced version of LRU called LRU-K. For example SQL Server uses LRU-K for K =2.</p>
<p>This idea behind this algorithm is to take into account more history. With the simple LRU (which is also LRU-K for K=1), the algorithm only takes into account the last time the data was used. With the LRU-K:</p>
<ul>
<li>It takes into account the <strong>K last times the data was used</strong>.</li>
<li><strong>A weight is put</strong> on the number of times the data was used</li>
<li>If a bunch of new data is loaded into the cache, the old but often used data are not removed (because their weights are higher).</li>
<li>But the algorithm can’t keep old data in the cache if they aren’t used anymore.</li>
<li>So the <strong>weights decrease</strong> <strong>over time if the data is not used</strong>.</li>
</ul>
<p>The computation of the weight is costly and this is why SQL Server only uses K=2. This value performs well for an acceptable overhead.</p>
<p>For a more in-depth knowledge of LRU-K, you can read the original research paper (1993): <a href="http://www.cs.cmu.edu/~christos/courses/721-resources/p297-o_neil.pdf" target="_blank" rel="noopener">The LRU-K page replacement algorithm for database disk buffering</a>.</p>
<h4 id="Other-algorithms-1"><a href="#Other-algorithms-1" class="headerlink" title="Other algorithms"></a>Other algorithms</h4><p>Of course there are other algorithms to manage cache like</p>
<ul>
<li>2Q (a LRU-K like algorithm)</li>
<li>CLOCK (a LRU-K like algorithm)</li>
<li>MRU (most recently used, uses the same logic than LRU but with another rule)</li>
<li>LRFU (Least Recently and Frequently Used)</li>
<li>…</li>
</ul>
<p>Some databases let the possibility to use another algorithm than the default one.</p>
<h3 id="Write-buffer"><a href="#Write-buffer" class="headerlink" title="Write buffer"></a>Write buffer</h3><p>I only talked about read buffers that load data before using them. But in a database you also have write buffers that store data and flush them on disk by bunches instead of writing data one by one and producing many single disk accesses.</p>
<p>Keep in mind that buffers store <strong>pages</strong> (the smallest unit of data) and not rows (which is a logical/human way to see data). A page in a buffer pool is <strong>dirty</strong> if the page has been modified and not written on disk. There are multiple algorithms to decide the best time to write the dirty pages on disk but it’s highly linked to the notion of transaction, which is the next part of the article.</p>
<h2 id="Transaction-manager"><a href="#Transaction-manager" class="headerlink" title="Transaction manager"></a>Transaction manager</h2><p>Last but not least, this part is about the transaction manager. We’ll see how this process ensures that each query is executed in its own transaction. But before that, we need to understand the concept of ACID transactions.</p>
<h3 id="I’m-on-acid"><a href="#I’m-on-acid" class="headerlink" title="I’m on acid"></a>I’m on acid</h3><p>An ACID transaction is a <strong>unit of work</strong> that ensures 4 things:</p>
<ul>
<li><strong>Atomicity</strong>: the transaction is “all or nothing”, even if it lasts 10 hours. If the transaction crashes, the state goes back to before the transaction (the transaction is <strong>rolled back</strong>).</li>
<li><strong>Isolation</strong>: if 2 transactions A and B run at the same time, the result of transactions A and B must be the same whether A finishes before/after/during transaction B.</li>
<li><strong>Durability</strong>: once the transaction is <strong>committed</strong> (i.e. ends successfully), the data stay in the database no matter what happens (crash or error).</li>
<li><strong>Consistency</strong>: only valid data (in terms of relational constraints and functional constraints) are written to the database. The consistency is related to atomicity and isolation.</li>
</ul>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/dollar_low.jpg" target="_blank" rel="noopener"><img src="../../public/images/dollar_low-20200207203251437.jpg" alt="one dollar"></a></p>
<p>During the same transaction, you can run multiple SQL queries to read, create, update and delete data. The mess begins when two transactions are using the same data. The classic example is a money transfer from an account A to an account B. Imagine you have 2 transactions:</p>
<ul>
<li>Transaction 1 that takes 100$ from account A and gives them to account B</li>
<li>Transaction 2 that takes 50$ from account A and gives them to account B</li>
</ul>
<p>If we go back to the <strong>ACID</strong> properties:</p>
<ul>
<li><p><strong>Atomicity</strong> ensures that no matter what happens during T1 (a server crash, a network failure …), you can’t end up in a situation where the 100$ are withdrawn from A and not given to B (this case is an inconsistent state).</p>
</li>
<li><p>I<strong>solation</strong> ensures that if T1 and T2 happen at the same time, in the end A will be taken 150$ and B given 150$ and not, for example, A taken 150$ and B given just $50 because T2 has partially erased the actions of T1 (this case is also an inconsistent state).</p>
</li>
<li><p><strong>Durability</strong> ensures that T1 won’t disappear into thin air if the database crashes just after T1 is committed.</p>
</li>
<li><p><strong>Consistency</strong> ensures that no money is created or destroyed in the system.</p>
</li>
</ul>
<p>[You can skip to the next part if you want, what I’m going to say is not important for the rest of the article]</p>
<p>Many modern databases don’t use a pure isolation as a default behavior because it comes with a huge performance overhead. The SQL norm defines 4 levels of isolation:</p>
<ul>
<li><p><strong>Serializable</strong> (default behaviour in SQLite): The highest level of isolation. Two transactions happening at the same time are 100% isolated. Each transaction has its own “world”.</p>
</li>
<li><p><strong>Repeatable read</strong> (default behavior in MySQL): Each transaction has its own “world” except in one situation. If a transaction ends up successfully and adds new data, these data will be visible in the other and still running transactions. But if A modifies a data and ends up successfully, the modification won’t be visible in the still running transactions. So, this break of isolation between transactions is only about new data, not the existing ones.</p>
</li>
</ul>
<p>For example, if a transaction A does a “SELECT count(1) from TABLE_X” and then a new data is added and committed in TABLE_X by Transaction B, if transaction A does again a count(1) the value won’t be the same.</p>
<p>This is called a <strong>phantom read</strong>.</p>
<ul>
<li><strong>Read committed</strong> (default behavior in Oracle, PostgreSQL and SQL Server): It’s a repeatable read + a new break of isolation. If a transaction A reads a data D and then this data is modified (or deleted) and committed by a transaction B, if A reads data D again it will see the modification (or deletion) made by B on the data.</li>
</ul>
<p>This is called a <strong>non-repeatable read</strong>.</p>
<ul>
<li><strong>Read uncommitted</strong>: the lowest level of isolation. It’s a read committed + a new break of isolation. If a transaction A reads a data D and then this data D is modified by a transaction B (that is not committed and still running), if A reads data D again it will see the modified value. If transaction B is rolled back, then data D read by A the second time doesn’t make no sense since it has been modified by a transaction B that never happened (since it was rolled back).</li>
</ul>
<p>This is called a <strong>dirty read</strong>.</p>
<p>Most databases add their own custom levels of isolation (like the snapshot isolation used by PostgreSQL, Oracle and SQL Server). Moreover, most databases don’t implement all the levels of the SQL norm (especially the read uncommitted level).</p>
<p>The default level of isolation can be overridden by the user/developer at the beginning of the connection (it’s a very simple line of code to add).</p>
<h3 id="Concurrency-Control"><a href="#Concurrency-Control" class="headerlink" title="Concurrency Control"></a>Concurrency Control</h3><p>The real issue to ensure isolation, coherency and atomicity is the <strong>write operations on the same data</strong> (add, update and delete):</p>
<ul>
<li>if all transactions are only reading data, they can work at the same time without modifying the behavior of another transaction.</li>
<li>if (at least) one of the transactions is modifying a data read by other transactions, the database needs to find a way to hide this modification from the other transactions. Moreover, it also needs to ensure that this modification won’t be erased by another transaction that didn’t see the modified data.</li>
</ul>
<p>This problem is a called <strong>concurrency control</strong>.</p>
<p>The easiest way to solve this problem is to run each transaction one by one (i.e. sequentially). But that’s not scalable at all and only one core is working on the multi-processor/core server, not very efficient…</p>
<p>The ideal way to solve this problem is, every time a transaction is created or cancelled:</p>
<ul>
<li>to monitor all the operations of all the transactions</li>
<li>to check if the parts of 2 (or more) transactions are in conflict because they’re reading/modifying the same data.</li>
<li>to reorder the operations inside the conflicting transactions to reduce the size of the conflicting parts</li>
<li>to execute the conflicting parts in a certain order (while the non-conflicting transactions are still running concurrently).</li>
<li>to take into account that a transaction can be cancelled.</li>
</ul>
<p>More formally it’s a scheduling problem with conflicting schedules. More concretely, it’s a very difficult and CPU-expensive optimization problem. Enterprise databases can’t afford to wait hours to find the best schedule for each new transaction event. Therefore, they use less ideal approaches that lead to more time wasted between conflicting transactions.</p>
<h3 id="Lock-manager"><a href="#Lock-manager" class="headerlink" title="Lock manager"></a>Lock manager</h3><p>To handle this problem, most databases are using <strong>locks</strong> and/or <strong>data versioning</strong>. Since it’s a big topic, I’ll focus on the locking part then I’ll speak a little bit about data versioning.</p>
<h4 id="Pessimistic-locking"><a href="#Pessimistic-locking" class="headerlink" title="Pessimistic locking"></a>Pessimistic locking</h4><p>The idea behind locking is:</p>
<ul>
<li>if a transaction needs a data,</li>
<li>it locks the data</li>
<li>if another transaction also needs this data,</li>
<li>it’ll have to wait until the first transaction releases the data.</li>
</ul>
<p>This is called an <strong>exclusive lock</strong>.</p>
<p>But using an exclusive lock for a transaction that only needs to read a data is very expensive since <strong>it forces other transactions that only want to read the same data to wait</strong>. This is why there is another type of lock, the <strong>shared lock</strong>.</p>
<p>With the shared lock:</p>
<ul>
<li>if a transaction needs only to read a data A,</li>
<li>it “shared locks” the data and reads the data</li>
<li>if a second transaction also needs only to read data A,</li>
<li>it “shared locks” the data and reads the data</li>
<li>if a third transaction needs to modify data A,</li>
<li>it “exclusive locks” the data but it has to wait until the 2 other transactions release their shared locks to apply its exclusive lock on data A.</li>
</ul>
<p>Still, if a data as an exclusive lock, a transaction that just needs to read the data will have to wait the end of the exclusive lock to put a shared lock on the data.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/lock_manager.png" target="_blank" rel="noopener"><img src="../../public/images/lock_manager-20200207203251437.png" alt="lock manager in a database"></a></p>
<p>The lock manager is the process that gives and releases locks. Internally, it stores the locks in a hash table (where the key is the data to lock) and knows for each data:</p>
<ul>
<li>which transactions are locking the data</li>
<li>which transactions are waiting for the data</li>
</ul>
<h4 id="Deadlock"><a href="#Deadlock" class="headerlink" title="Deadlock"></a>Deadlock</h4><p>But the use of locks can lead to a situation where 2 transactions are waiting forever for a data:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/deadlock.png" target="_blank" rel="noopener"><img src="../../public/images/deadlock-20200207203251451.png" alt="deadlock with database transactions"></a></p>
<p>In this figure:</p>
<ul>
<li>transaction A has an exclusive lock on data1 and is waiting to get data2</li>
<li>transaction B has an exclusive lock on data2 and is waiting to get data1</li>
</ul>
<p>This is called a <strong>deadlock</strong>.</p>
<p>During a deadlock, the lock manager chooses which transaction to cancel (rollback) in order to remove the deadlock. This decision is not easy:</p>
<ul>
<li>Is it better to kill the transaction that modified the least amount of data (and therefore that will produce the least expensive rollback)?</li>
<li>Is it better to kill the least aged transaction because the user of the other transaction has waited longer?</li>
<li>Is it better to kill the transaction that will take less time to finish (and avoid a possible starvation)?</li>
<li>In case of rollback, how many transactions will be impacted by this rollback?</li>
</ul>
<p>But before making this choice, it needs to check if there are deadlocks.</p>
<p>The hash table can be seen as a graph (like in the previous figures). There is a deadlock if there is a cycle in the graph. Since it’s expensive to check for cycles (because the graph with all the locks is quite big), a simpler approach is often used: using a <strong>timeout</strong>. If a lock is not given within this timeout, the transaction enters a deadlock state.</p>
<p>The lock manager can also check before giving a lock if this lock will create a deadlock. But again it’s computationally expensive to do it perfectly. Therefore, these pre-checks are often a set of basic rules.</p>
<h4 id="Two-phase-locking"><a href="#Two-phase-locking" class="headerlink" title="Two-phase locking"></a>Two-phase locking</h4><p>The <strong>simplest way</strong> to ensure a pure isolation is if a lock is acquired at the beginning of the transaction and released at the end of the transaction. This means that a transaction has to wait for all its locks before it starts and the locks held by a transaction are released when the transaction ends. It works but it <strong>produces a lot of time wasted</strong> to wait for all locks.</p>
<p>A faster way is the <strong>Two-Phase Locking Protocol</strong> (used by DB2 and SQL Server) where a transaction is divided into 2 phases:</p>
<ul>
<li>the <strong>growing phase</strong> where a transaction can obtain locks, but can’t release any lock.</li>
<li>the <strong>shrinking phase</strong> where a transaction can release locks (on the data it has already processed and won’t process again), but can’t obtain new locks.</li>
</ul>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/two-phase-locking.png" target="_blank" rel="noopener"><img src="../../public/images/two-phase-locking-20200207203251473.png" alt="a problem avoided with two phase locking"></a></p>
<p>The idea behind these 2 simple rules is:</p>
<ul>
<li>to release the locks that aren’t used anymore to reduce the wait time of other transactions waiting for these locks</li>
<li>to prevent from cases where a transaction gets data modified after the transaction started and therefore aren’t coherent with the first data the transaction acquired.</li>
</ul>
<p>This protocol works well except if a transaction that modified a data and released the associated lock is cancelled (rolled back). You could end up in a case where another transaction reads the modified value whereas this value is going to be rolled back. To avoid this problem, <strong>all the exclusive locks must be released at the end of the transaction</strong>.</p>
<h4 id="A-few-words"><a href="#A-few-words" class="headerlink" title="A few words"></a>A few words</h4><p>Of course a real database uses a more sophisticated system involving more types of locks (like intention locks) and more granularities (locks on a row, on a page, on a partition, on a table, on a tablespace) but the idea remains the same.</p>
<p>I only presented the pure lock-based approach. <strong>Data versioning is another way to deal with this problem</strong>.</p>
<p>The idea behind versioning is that:</p>
<ul>
<li>every transaction can modify the same data at the same time</li>
<li>each transaction has its own copy (or version) of the data</li>
<li>if 2 transactions modify the same data, only one modification will be accepted, the other will be refused and the associated transaction will be rolled back (and maybe re-run).</li>
</ul>
<p>It increases the performance since:</p>
<ul>
<li><strong>reader transactions don’t block writer transactions</strong></li>
<li><strong>writer transactions don’t block reader transactions</strong></li>
<li>there is no overhead from the “fat and slow” lock manager</li>
</ul>
<p>Everything is better than locks except when 2 transactions write the same data. Moreover, you can quickly end up with a huge disk space overhead.</p>
<p>Data versioning and locking are two different visions: <strong>optimistic locking vs pessimistic locking</strong>. They both have pros and cons; it really depends on the use case (more reads vs more writes). For a presentation on data versioning, I recommend <a href="http://momjian.us/main/writings/pgsql/mvcc.pdf" target="_blank" rel="noopener">this very good presentation</a> on how PostgreSQL implements multiversion concurrency control.</p>
<p>Some databases like DB2 (until DB2 9.7) and SQL Server (except for snapshot isolation) are only using locks. Other like PostgreSQL, MySQL and Oracle use a mixed approach involving locks and data versioning. I’m not aware of a database using only data versioning (if you know a database based on a pure data versioning, feel free to tell me).</p>
<p>[UPDATE 08/20/2015] I was told by a reader that:</p>
<blockquote>
<p>Firebird and Interbase use versioning without record locking.<br>Versioning has an interesting effect on indexes: sometimes a unique index contains duplicates, the index can have more entries than the table has rows, etc.</p>
</blockquote>
<p>If you read the part on the different levels of isolation, when you increase the isolation level you increase the number of locks and therefore the time wasted by transactions to wait for their locks. This is why most databases don’t use the highest isolation level (Serializable) by default.</p>
<p>As always, you can check by yourself in the documentation of the main databases (for example <a href="http://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-model.html" target="_blank" rel="noopener">MySQL</a>, <a href="http://www.postgresql.org/docs/9.4/static/mvcc.html" target="_blank" rel="noopener">PostgreSQL</a> or <a href="http://docs.oracle.com/cd/B28359_01/server.111/b28318/consist.htm#i5337" target="_blank" rel="noopener">Oracle</a>).</p>
<h3 id="Log-manager"><a href="#Log-manager" class="headerlink" title="Log manager"></a>Log manager</h3><p>We’ve already seen that to increase its performances, a database stores data in memory buffers. But if the server crashes when the transaction is being committed, you’ll lose the data still in memory during the crash, which breaks the Durability of a transaction.</p>
<p>You can write everything on disk but if the server crashes, you’ll end up with the data half written on disk, which breaks the Atomicity of a transaction.</p>
<p><strong>Any modification written by a transaction must be undone or finished</strong>.</p>
<p>To deal with this problem, there are 2 ways:</p>
<ul>
<li><strong>Shadow copies/pages</strong>: Each transaction creates its own copy of the database (or just a part of the database) and works on this copy. In case of error, the copy is removed. In case of success, the database switches instantly the data from the copy with a filesystem trick then it removes the “old” data.</li>
<li><strong>Transaction log</strong>: A transaction log is a storage space. Before each write on disk, the database writes an info on the transaction log so that in case of crash/cancel of a transaction, the database knows how to remove (or finish) the unfinished transaction.</li>
</ul>
<h4 id="WAL"><a href="#WAL" class="headerlink" title="WAL"></a>WAL</h4><p>The shadow copies/pages creates a huge disk overhead when used on large databases involving many transactions. That’s why modern databases use a <strong>transaction log</strong>. The transaction log must be stored on a <strong>stable storage</strong>. I won’t go deeper on storage technologies but using (at least) RAID disks is mandatory to prevent from a disk failure.</p>
<p>Most databases (at least Oracle, <a href="https://technet.microsoft.com/en-us/library/ms186259(v=sql.105).aspx" target="_blank" rel="noopener">SQL Server</a>, <a href="http://www.ibm.com/developerworks/data/library/techarticle/0301kline/0301kline.html" target="_blank" rel="noopener">DB2</a>, <a href="http://www.postgresql.org/docs/9.4/static/wal.html" target="_blank" rel="noopener">PostgreSQL</a>, MySQL and <a href="https://www.sqlite.org/wal.html" target="_blank" rel="noopener">SQLite</a>) deal with the transaction log using the <strong>Write-Ahead Logging protocol</strong> (WAL). The WAL protocol is a set of 3 rules:</p>
<ul>
<li>1) Each modification into the database produces a log record, and <strong>the log record must be written into the transaction log before the data is written on disk</strong>.</li>
<li>2) The log records must be written in order; a log record A that happens before a log record B must but written before B</li>
<li>3) When a transaction is committed, the commit order must be written on the transaction log before the transaction ends up successfully.</li>
</ul>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/log_manager.png" target="_blank" rel="noopener"><img src="../../public/images/log_manager-20200207203251465.png" alt="log manager in a database"></a></p>
<p>This job is done by a log manager. An easy way to see it is that between the cache manager and the data access manager (that writes data on disk) the log manager writes every update/delete/create/commit/rollback on the transaction log before they’re written on disk. Easy, right?</p>
<p>WRONG ANSWER! After all we’ve been through, you should know that everything related to a database is cursed by the “database effect”. More seriously, the problem is to find a way to write logs while keeping good performances. If the writes on the transaction log are too slow they will slow down everything.</p>
<h4 id="ARIES"><a href="#ARIES" class="headerlink" title="ARIES"></a>ARIES</h4><p>In 1992, IBM researchers “invented” an enhanced version of WAL called ARIES. ARIES is more or less used by most modern databases. The logic might not be the same but the concepts behind ARIES are used everywhere. I put the quotes on invented because, according to this <a href="http://db.csail.mit.edu/6.830/lectures/lec15-notes.pdf" target="_blank" rel="noopener">MIT course</a>, the IBM researchers did “nothing more than writing the good practices of transaction recovery”. Since I was 5 when the ARIES paper was published, I don’t care about this old gossip from bitter researchers. In fact, I only put this info to give you a break before we start this last technical part. I’ve read a huge part of the <a href="http://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf" target="_blank" rel="noopener">research paper on ARIES</a> and I find it very interesting! In this part I’ll only give you an overview of ARIES but I strongly recommend to read the paper if you want a real knowledge.</p>
<p>ARIES stands for <strong>A</strong>lgorithms for <strong>R</strong>ecovery and <strong>I</strong>solation <strong>E</strong>xploiting <strong>S</strong>emantics.</p>
<p>The aim of this technique is double:</p>
<ul>
<li>1) Having <strong>good performances when writing logs</strong></li>
<li>2) Having a fast and <strong>reliable recovery</strong></li>
</ul>
<p>There are multiple reasons a database has to rollback a transaction:</p>
<ul>
<li>Because the user cancelled it</li>
<li>Because of server or network failures</li>
<li>Because the transaction has broken the integrity of the database (for example you have a UNIQUE constraint on a column and the transaction adds a duplicate)</li>
<li>Because of deadlocks</li>
</ul>
<p>Sometimes (for example, in case of network failure), the database can recover the transaction.</p>
<p>How is that possible? To answer this question, we need to understand the information stored in a log record.</p>
<h5 id="The-logs"><a href="#The-logs" class="headerlink" title="The logs"></a>The logs</h5><p>Each <strong>operation (add/remove/modify) during a transaction produces a log</strong>. This log record is composed of:</p>
<ul>
<li><strong>LSN:</strong> A unique <strong>L</strong>og <strong>S</strong>equence <strong>N</strong>umber. This LSN is given in a chronological order*. This means that if an operation A happened before an operation B the LSN of log A will be lower than the LSN of log B.</li>
<li><strong>TransID:</strong> the id of the transaction that produced the operation.</li>
<li><strong>PageID:</strong> the location on disk of the modified data. The minimum amount of data on disk is a page so the location of the data is the location of the page that contains the data.</li>
<li><strong>PrevLSN:</strong> A link to the previous log record produced by the same transaction.</li>
<li><strong>UNDO:</strong> a way to remove the effect of the operation</li>
</ul>
<p>For example, if the operation is an update, the UNDO will store either the value/state of the updated element before the update (physical UNDO) or the reverse operation to go back at the previous state (logical UNDO)**.</p>
<ul>
<li><strong>REDO</strong>: a way replay the operation</li>
</ul>
<p>Likewise, there are 2 ways to do that. Either you store the value/state of the element after the operation or the operation itself to replay it.</p>
<ul>
<li>…: (FYI, an ARIES log has 2 others fields: the UndoNxtLSN and the Type).</li>
</ul>
<p>Moreover, each page on disk (that stores the data, not the log) has id of the log record (LSN) of the last operation that modified the data.</p>
<p>*The way the LSN is given is more complicated because it is linked to the way the logs are stored. But the idea remains the same.</p>
<p>**ARIES uses only logical UNDO because it’s a real mess to deal with physical UNDO.</p>
<p>Note: From my little knowledge, only PostgreSQL is not using an UNDO. It uses instead a garbage collector daemon that removes the old versions of data. This is linked to the implementation of the data versioning in PostgreSQL.</p>
<p>To give you a better idea, here is a visual and simplified example of the log records produced by the query “UPDATE FROM PERSON SET AGE = 18;”. Let’s say this query is executed in transaction 18.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/ARIES_logs.png" target="_blank" rel="noopener"><img src="../../public/images/ARIES_logs-20200207203251473.png" alt="simplified logs of ARIES protocole"></a></p>
<p>Each log has a unique LSN. The logs that are linked belong to the same transaction. The logs are linked in a chronological order (the last log of the linked list is the log of the last operation).</p>
<h5 id="Log-Buffer"><a href="#Log-Buffer" class="headerlink" title="Log Buffer"></a>Log Buffer</h5><p>To avoid that log writing becomes a major bottleneck, a <strong>log buffer</strong> is used.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/ARIES_log_writing.png" target="_blank" rel="noopener"><img src="../../public/images/ARIES_log_writing-20200207203251473.png" alt="log writing process in databases"></a></p>
<p>When the query executor asks for a modification:</p>
<ul>
<li>1) The cache manager stores the modification in its buffer.</li>
<li>2) The log manager stores the associated log in its buffer.</li>
<li>3) At this step, the query executor considers the operation is done (and therefore can ask for other modifications)</li>
<li>4) Then (later) the log manager writes the log on the transaction log. The decision when to write the log is done by an algorithm.</li>
<li>5) Then (later) the cache manager writes the modification on disk. The decision when to write data on disk is done by an algorithm.</li>
</ul>
<p><strong>When a transaction is committed, it means that for every operation in the transaction the steps 1, 2, 3,4,5 are done</strong>. Writing in the transaction log is fast since it’s just “adding a log somewhere in the transaction log” whereas writing data on disk is more complicated because it’s “writing the data in a way that it’s fast to read them”.</p>
<h5 id="STEAL-and-FORCE-policies"><a href="#STEAL-and-FORCE-policies" class="headerlink" title="STEAL and FORCE policies"></a>STEAL and FORCE policies</h5><p>For performance reasons the <strong>step 5 might be done after the commit</strong> because in case of crashes it’s still possible to recover the transaction with the REDO logs. This is called a <strong>NO-FORCE policy</strong>.</p>
<p>A database can choose a FORCE policy (i.e. step 5 must be done before the commit) to lower the workload during the recovery.</p>
<p>Another issue is to choose whether <strong>the data are written step-by-step on disk (STEAL policy)</strong> or if the buffer manager needs to wait until the commit order to write everything at once (NO-STEAL). The choice between STEAL and NO-STEAL depends on what you want: fast writing with a long recovery using UNDO logs or fast recovery?</p>
<p>Here is a summary of the impact of these policies on recovery:</p>
<ul>
<li><strong>STEAL/NO-FORCE</strong> <strong>needs UNDO and REDO</strong>: <strong>highest performances</strong> but gives more complex logs and recovery processes (like ARIES). <strong>This is the choice made by most databases</strong>. Note: I read this fact on multiple research papers and courses but I couldn’t find it (explicitly) on the official documentations.</li>
<li>STEAL/ FORCE needs only UNDO.</li>
<li>NO-STEAL/NO-FORCE needs only REDO.</li>
<li>NO-STEAL/FORCE needs nothing: <strong>worst performances</strong> and a huge amount of ram is needed.</li>
</ul>
<h5 id="The-recovery-part"><a href="#The-recovery-part" class="headerlink" title="The recovery part"></a>The recovery part</h5><p>Ok, so we have nice logs, let’s use them!</p>
<p>Let’s say the new intern has crashed the database (rule n°1: it’s always the intern’s fault). You restart the database and the recovery process begins.</p>
<p>ARIES recovers from a crash in three passes:</p>
<ul>
<li><strong>1) The Analysis pass</strong>: The recovery process reads the full transaction log* to recreate the timeline of what was happening during the crash. It determines which transactions to rollback (all the transactions without a commit order are rolled back) and which data needed to be written on disk at the time of the crash.</li>
<li><strong>2) The Redo pass</strong>: This pass starts from a log record determined during analysis, and uses the REDO to update the database to the state it was before the crash.</li>
</ul>
<p>During the redo phase, the REDO logs are processed in a chronological order (using the LSN).</p>
<p>For each log, the recovery process reads the LSN of the page on disk containing the data to modify.</p>
<p>If LSN(page_on_disk)&gt;=LSN(log_record), it means that the data has already been written on disk before the crash (but the value was overwritten by an operation that happened after the log and before the crash) so nothing is done.</p>
<p>If LSN(page_on_disk)&lt;LSN(log_record) then the page on disk is updated.</p>
<p>The redo is done even for the transactions that are going to be rolled back because it simplifies the recovery process (but I’m sure modern databases don’t do that).</p>
<ul>
<li><strong>3) The Undo pass</strong>: This pass rolls back all transactions that were incomplete at the time of the crash. The rollback starts with the last logs of each transaction and processes the UNDO logs in an anti-chronological order (using the PrevLSN of the log records).</li>
</ul>
<p>During the recovery, the transaction log must be warned of the actions made by the recovery process so that the data written on disk are synchronized with what’s written in the transaction log. A solution could be to remove the log records of the transactions that are being undone but that’s very difficult. Instead, ARIES writes compensation logs in the transaction log that delete logically the log records of the transactions being removed.</p>
<p>When a transaction is cancelled “manually” or by the lock manager (to stop a deadlock) or just because of a network failure, then the analysis pass is not needed. Indeed, the information about what to REDO and UNDO is available in 2 in-memory tables:</p>
<ul>
<li>a <strong>transaction table</strong> (stores the state of all current transactions)</li>
<li>a <strong>dirty page table</strong> (stores which data need to be written on disk).</li>
</ul>
<p>These tables are updated by the cache manager and the transaction manager for each new transaction event. Since they are in-memory, they are destroyed when the database crashes.</p>
<p>The job of the analysis phase is to recreate both tables after a crash using the information in the transaction log. <em>To speed up the analysis pass, ARIES provides the notion of *</em>checkpoint**. The idea is to write on disk from time to time the content of the transaction table and the dirty page table and the last LSN at the time of this write so that during the analysis pass, only the logs after this LSN are analyzed.</p>
<h1 id="To-conclude"><a href="#To-conclude" class="headerlink" title="To conclude"></a>To conclude</h1><p>Before writing this article, I knew how big the subject was and I knew it would take time to write an in-depth article about it. It turned out that I was very optimistic and I spent twice more time than expected, but I learned a lot.</p>
<p>If you want a good overview about databases, I recommend reading the research paper “<a href="http://db.cs.berkeley.edu/papers/fntdb07-architecture.pdf" target="_blank" rel="noopener">Architecture of a Database System</a> “. This is a good introduction on databases (110 pages) and for once it’s readable by non-CS guys. This paper helped me a lot to find a plan for this article and it’s not focused on data structures and algorithms like my article but more on the architecture concepts.</p>
<p>If you read this article carefully you should now understand how powerful a database is. Since it was a very long article, let me remind you about what we’ve seen:</p>
<ul>
<li>an overview of the B+Tree indexes</li>
<li>a global overview of a database</li>
<li>an overview of the cost based optimization with a strong focus on join operators</li>
<li>an overview of the buffer pool management</li>
<li>an overview of the transaction management</li>
</ul>
<p>But a database contains even more cleverness. For example, I didn’t speak about some touchy problems like:</p>
<ul>
<li>how to manage clustered databases and global transactions</li>
<li>how to take a snapshot when the database is still running</li>
<li>how to efficiently store (and compress) data</li>
<li>how to manage memory</li>
</ul>
<p>So, think twice when you have to choose between a buggy NoSQL database and a rock-solid relational database. Don’t get me wrong, some NoSQL databases are great. But they’re still young and answering specific problems that concern a few applications.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/07/The-Best-Programming-Language/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/07/The-Best-Programming-Language/" class="post-title-link" itemprop="url">The_Best_Programming_Language</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-07 20:26:11 / Modified: 21:29:44" itemprop="dateCreated datePublished" datetime="2020-02-07T20:26:11-05:00">2020-02-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>When I want to take a break at work, I sometimes read technology forums. And there is one kind of posts that I really like: the flame wars between programming languages. I like these posts because you can see passionate and smart people who are arguing as if their lives were at play.</p>
<p>These posts have 2 advantages:</p>
<ul>
<li>they make me laugh</li>
<li>I learn new stuff</li>
</ul>
<p>If I had to sum up this kind of posts, it would be something like:</p>
<p>Post Title “Java is the best language” by NewJavaFanBoy</p>
<blockquote>
<p><strong>NewJavaFanBoy</strong>: Java is the best language because of its community. Moreover, it has really cool features like lambdas. Why so many people hate Java?</p>
<p><strong>FormerJavaFanBoy</strong>: Oracle killed Java.</p>
<p><strong>DotNetFanBoy</strong>: The evolution of Java is too slow, C# had lambdas a while ago. Moreover, some critical features like optional and named parameters are not in Java. Now that dotnet is more open sourced and can be run on Linux with Mono, Java is going to die.</p>
<p><strong>TrollRoxxoR</strong>: BecauseJavaDevelopersDontKnowHowToWriteCode</p>
<p><strong>RealG33k</strong>: Both your languages are for kids, C++ is way better but it’s for real developers only. Do you even know what SOLID means?</p>
<p><strong>HipsterGeek</strong>: So old and lame … you should try Node.js, it’s based on asynchronous calls and it’s very fast.</p>
<p><strong>LinusTorvalds</strong>: Pussies, a real developer uses C or assembly. You can’t have performances with those high level shits.</p>
</blockquote>
<p>I hate PHP. I can’t explain why; it must be because I tried to learn it when I was 14 and it messed with my brain. But guess what, you’re reading this post on a server using PHP/NGINX (which is a kickass server by the way). I’m good with Java. So, I could have used a Java framework running on a fast fat JVM. But, WordPress is a great platform. It’s often looked down by purists but it clearly answers my needs. The aim of my blog is not to be the fastest in the world (though it has surprisingly but painfully survived 2 Hacker News and Reddit front pages involving 500 simultaneous connections). I just want a user-friendly interface where I can share my thoughts.</p>
<p>Which leads to my point: there is no best programming language, it depends on the situation.</p>
<p><strong>1) Do you need performances?</strong></p>
<p>If yes, what kind of performances are we talking about?</p>
<ul>
<li>Seconds? Every language can do it!</li>
<li>Milliseconds? Every language with good programmers can do it.</li>
<li>Microseconds? At this step, you can remove all the interpreted languages (like python, which is a good language). I know that a well-tuned JVM with very good Java programmers can do it. I imagine that it’s the same for C#. Of course, a pure-compiled language can deal with that.</li>
</ul>
<p>But in all these cases the programmer’s skills are more important than the language.</p>
<ul>
<li>Nanoseconds? Only assembly or maybe C can deal with that.</li>
</ul>
<p>So, in most situations developers’ skills are what matters.</p>
<p><strong>2) What’s about the ecosystem?</strong></p>
<p>More than the language itself, the ecosystem is important.</p>
<p>I’ve used Visual Studio during my scholarship and I have been amazed by the coherency of Microsoft’s ecosystem.</p>
<p>Now, I’m more an Eclipse guy. Even in the Java community, Eclipse is looked down by purists who now use IntelliJ IDEA. Eclipse is an open source software developed by different people and it’s clearly visible (in a bad way). Compared to the coherency of Visual Studio, you’ll find different logics in the different plugins of Eclipse.</p>
<p>But, if having tools is great, knowing how to use them is better. For example, when I started in Java, I was very slow. I learned by hearth some Eclipse keywords and it’s changed my developer life. I’ve also looked for useful plugins, and Eclipse has plenty of them, because it’s a rich ecosystem.</p>
<p><strong>3) What’s about the online help?</strong></p>
<p>Ok, you’re using your kickass programming language but don’t tell me you know every side of this language by hearth. Having a well known language is useful when you need help. A simple Google or StackOverflow search and you get your answer by Ninja_Guru_666 and I_AM_THE_EXPERT. If you’re more like an in-depth programmer, you can also check for the official documentation assuming it exists for the problem you’re looking for.</p>
<p><strong>4) What are the skills of the team?</strong></p>
<p>If the developers don’t really know how a computer works, using a compiled language is a suicidal move. And, compared to the purists, I don’t see why knowing (exactly) how a computer works makes you a good developer (though, I must admit, it helps; but there are more important skills).</p>
<p>It’s better not using the best tool but a known one. Moreover, many developers are fan boys. Using their preferred language will help them to stay motivated on the project.</p>
<p><strong>5) The business side</strong></p>
<p>An objective point of view is to see what the most in-demand languages are. It doesn’t mean they’re the best but at least you’ll get a job. In this case, Java, C#, PHP, SQL and JavaScript are clearly above all (at least in France).</p>
<p>Moreover, as a technical leader, it’s always good to check the skills in the market before choosing a technology. If you choose the best but rare technology to deal with your problem, good luck for finding skilled developers on the technology.</p>
<p>But what’s true in 2015 might change in 2018. ActionScript was a must have not so long ago. Likewise, with Swift, all the hours spent on Objective C will become obsolete in a few years.</p>
<p>To conclude, I’ll end up with a lame and (I hope) obvious conclusion: there are no best programming languages or best frameworks; what’s best now might not exist tomorrow. A programming language is just a tool; what matters is the way you overcome your problems.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%90%86%E8%AE%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%90%86%E8%AE%BA/" class="post-title-link" itemprop="url">计算机理论</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-06 16:58:37 / Modified: 18:02:37" itemprop="dateCreated datePublished" datetime="2020-02-06T16:58:37-05:00">2020-02-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据结构和算法"><a href="#数据结构和算法" class="headerlink" title="数据结构和算法"></a>数据结构和算法</h1><p>算法是比较难学习的，而且学习 “ 算法 “ 是需要智商的。数组、链表、哈希表、二叉树、排序算法等一些基础知识，对大多数人来说是没什么问题的。但是一旦进入到路径规划、背包问题、字符串匹配、动态规划、递归遍历等一些比较复杂的问题上，就会让很多人跟不上了，不但跟不上，而且还会非常痛苦。是的，解决算法问题的确是可以区分人类智商的一个比较好的方式，这也是为什么好些公司用算法题当面试题来找到智商比较高的程序员。</p>
<p>然而，在很多时候，我们在工作中却发现根本用不到算法，或是一些基本的算法也没有必要实现，只需要使用一下第三方的库就好了。于是，导致社会上出现很多 “ 算法无用论 “ 的声音。</p>
<p>对此，我想说，算法真的很重要。无论是做业务还是做底层系统，经常需要使用算法处理各种各样的问题。比如，业务上需要用算法比较两个数组中差异的布隆过滤器，或是在做监控系统时实时计算过去一分钟的 P99 统计时的蓄水池算法，或是数据库的 B+ 树索引，还有 Linux 内核中的 epoll 的红黑树，还有在做服务调度里的 “ 背包问题 “ 等都会用算法，真的是会本质上帮助到你，也是会让你非常有成就感的一件事。</p>
<p>虽然算法很难，需要智商，但我还是想鼓励你，这其中是有很多的套路是可以学习的，一旦学会这些套路，你会受益无穷的。</p>
<p>这里有几本书着重推荐一下。</p>
<ul>
<li><strong>基础知识</strong>。《<a href="https://book.douban.com/subject/10432347/" target="_blank" rel="noopener">算法</a>》，是算法领域经典的参考书，不但全面介绍了关于算法和数据结构的必备知识，还给出了每位程序员应知应会的 50 个算法，并提供了实际代码。最不错的是，其深入浅出的算法介绍，让一些比较难的算法也变得容易理解，尤其是书中对红黑树的讲解非常精彩。其中，还有大量的图解，详尽的代码和讲解，也许是最好的数据结构入门图书。不好的是不深，缺乏进一步的算法设计内容，甚至连动态规划都未提及。另外，如果你觉得算法书比较枯燥的话，你可以看看这本有趣的《<a href="https://book.douban.com/subject/26979890/" target="_blank" rel="noopener">算法图解</a>》。</li>
<li><strong>理论加持</strong>。如果说上面这本书偏于实践和工程，而你看完后，对算法和数据结构的兴趣更浓了，那么你可以再看看另一本也是很经典的偏于理论方面的书——《<a href="https://book.douban.com/subject/20432061/" target="_blank" rel="noopener">算法导论</a>》。虽然其中的一些理论知识在《算法》那本书中也有提过，但《算法导论》这本书更为专业一些，是美国计算机科学本科生的教科书。</li>
<li><strong>思维改善</strong>。还有一本叫《<a href="https://book.douban.com/subject/3227098/" target="_blank" rel="noopener">编程珠玑</a>》的书，写这本书的人是世界著名计算机科学家乔恩·本特利（Jon Bentley），被誉为影响算法发展的十位大师之一。你可能不认识这个人，但是你知道他的学生有多厉害吗？一个是 Tcl 语言设计者约翰·奥斯德奥特（John Ousterhout），另一个是 Java 语言设计者詹姆斯·高斯林（James Gosling），还有一个是《算法导论》作者之一查尔斯·雷斯尔森（Charles Leiserson），还有好多好多。这本书也是很经典的算法书，其中都是一些非常实际的问题，并以其独有的洞察力和创造力，来引导读者理解并学会解决这些问题的方法，也是一本可以改善你思维方式的书。</li>
</ul>
<p>然后，你需要去做一些题来训练一下自己的算法能力，这里就要推荐 <a href="https://leetcode.com/" target="_blank" rel="noopener">LeetCode</a> 这个网站了。它是一个很不错的做算法训练的地方。现在也越做越好了。基本上来说，这里会有两类题。</p>
<ul>
<li><strong>基础算法题</strong>。其中有大量的算法题，解这些题都是有套路的，不是用递归（深度优先 DFS，广度优先 BFS），就是要用动态规划（Dynamic Programming），或是折半查找（Binary Search），或是回溯（Back tracing），或是分治法（Divide and Conquer），还有大量的对树、数组、链表、字符串和 hash 表的操作。通过做这些题能让你对这些最基础的算法的思路有非常扎实的了解和训练。</li>
<li><strong>编程题</strong>。比如：atoi，strstr，add two nums，括号匹配，字符串乘法，通配符匹配，文件路径简化，Text Justification，反转单词等，这些题的 Edge Case 和 Corner Case 有很多。这些题需要你想清楚了再干，只要你稍有疏忽，就会有几个 case 让你痛不欲生，而且一不小心就会让你的代码写得又臭又长，无法阅读。通过做这些题，可以非常好地训练你对各种情况的考虑，以及你对程序代码组织的掌控（其实就是其中的状态变量）。</li>
</ul>
<p>我觉得每个程序员都应该花时间和精力做这些题，因为你会从这些题中得到很大的收益。</p>
<p>如果能够把这些算法能力都掌握了，那么你就有很大的概率可以很容易地通过这世界上最优的公司的面试，比如：Google、Amazon、Facebook 之类的公司。对你来说，如果能够进入到这些公司里工作，那么你未来的想像空间也会大得多得多。</p>
<p>最后，我们要知道这个世界上的数据结构和算法很多很多，下面给出了两个网站。</p>
<ul>
<li><strong><a href="https://www.wikiwand.com/en/List_of_algorithms" target="_blank" rel="noopener">List of Algorithms</a></strong> ，这个网站罗列了非常多的算法，完全可以当成一个算法字典，或是用来开阔眼界。</li>
<li>还有一个数据结构动画图的网站 <a href="https://www.cs.usfca.edu/~galles/visualization/Algorithms.html" target="_blank" rel="noopener">Data Structure Visualizations</a>。</li>
</ul>
<h1 id="其它理论基础知识"><a href="#其它理论基础知识" class="headerlink" title="其它理论基础知识"></a>其它理论基础知识</h1><p>下面这些书，基本上是计算机科学系的大学教材。如果你想有科班出生的理论基础，那么这些书是必读的。当然，这些理论基础知识比较枯燥，但我觉得如果你想成为专业的程序员，那么应该要找时间读一下。</p>
<ul>
<li>《<a href="https://book.douban.com/subject/1139426/" target="_blank" rel="noopener">数据结构与算法分析</a>》，这本书曾被评为 20 世纪顶尖的 30 部计算机著作之一，作者 Mark Allen Weiss 在数据结构和算法分析方面卓有建树，他在数据结构和算法分析等方面的著作尤其畅销，并广受好评，已被世界 500 余所大学用作教材。</li>
<li>《<a href="https://book.douban.com/subject/1929984/" target="_blank" rel="noopener">数据库系统概念</a>》，它是数据库系统方面的经典教材之一。国际上许多著名大学包括斯坦福大学、耶鲁大学、德克萨斯大学、康奈尔大学、伊利诺伊大学、印度理工学院等都采用本书作为教科书。这本书全面介绍了数据库系统的各种知识，透彻阐释数据库管理的基本概念。不仅讨论了数据库查询语言、模式设计、数据仓库、数据库应用开发、基于对象的数据库和 XML、数据存储和查询、事务管理、数据挖掘与信息检索以及数据库系统体系结构等方面的内容，而且对性能评测标准、性能调整、标准化以及空间与地理数据、事务处理监控等高级应用主题进行了广泛讨论。</li>
<li>《<a href="https://book.douban.com/subject/3852290/" target="_blank" rel="noopener">现代操作系统</a>》，这本书是操作系统领域的经典之作，书中集中讨论了操作系统的基本原理，包括进程、线程、存储管理、文件系统、输入 / 输出、死锁等，同时还包含了有关计算机安全、多媒体操作系统、掌上计算机操作系统、微内核、多核处理机上的虚拟机以及操作系统设计等方面的内容。</li>
<li>《<a href="https://book.douban.com/subject/1391207/" target="_blank" rel="noopener">计算机网络</a>》，这本书采用了独创的自顶向下方法，即从应用层开始沿协议栈向下讲解计算机网络的基本原理，强调应用层范例和应用编程接口，内容深入浅出，注重教学方法，理论与实践相结合。新版中还增加了无线和移动网络一章，并扩充了对等网络、BGP、MPLS、网络安全、广播选路和因特网编址及转发方面的材料。是一本不可多得的教科书。</li>
<li>《<a href="https://book.douban.com/subject/1148282/" target="_blank" rel="noopener">计算机程序的构造和解释</a>》，这本书也很经典，是 MIT 的计算机科学系的教材。这本书中主要证实了很多程序是怎么构造出来的，以及程序的本质是什么。整本书主要是使用 Scheme/Lisp 语言，从数据抽象、过程抽象、迭代、高阶函数等编程和控制系统复杂性的思想，到数据结构和算法，到编译器 / 解释器、编程语言设计。</li>
<li>《<a href="https://book.douban.com/subject/3296317/" target="_blank" rel="noopener">编译原理</a>》，这本书又叫 “ 龙书 “，其全面、深入地探讨了编译器设计方面的重要主题，包括词法分析、语法分析、语法制导定义和语法制导翻译、运行时刻环境、目标代码生成、代码优化技术、并行性检测以及过程间分析技术，并在相关章节中给出大量的实例。与上一版相比，本书进行了全面的修订，涵盖了编译器开发方面的最新进展。每章中都提供了大量的系统及参考文献。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" class="post-title-link" itemprop="url">计算机编程语言</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-06 16:55:49 / Modified: 17:57:46" itemprop="dateCreated datePublished" datetime="2020-02-06T16:55:49-05:00">2020-02-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>编程语言</strong>。你需要学习 C、C++ 和 Java 这三个工业级的编程语言。为什么说它们是工业级的呢？主要是，C 和 C++ 语言规范都由 ISO 标准化过，而且都有工业界厂商组成的标准化委员会来制定工业标准。次要原因是，它们已经在业界应用于许多重要的生产环境中。</p>
<ul>
<li>C 语言不用多说，现今这个世界上几乎所有重要的软件都跟 C 有直接和间接的关系，操作系统、网络、硬件驱动等等。说得霸气一点儿，这个世界就是在 C 语言之上运行的。</li>
<li>而对于 C++ 来说，现在主流的浏览器、数据库、Microsoft Office、主流的图形界面、著名的游戏引擎等都是用 C++ 编写的。而且，很多公司都用 C++ 开发核心架构，如 Google、腾讯、百度、阿里云等。</li>
<li>而金融电商公司则广泛地使用 Java 语言，因为 Java 的好处太多了，代码稳定性超过 C 和 C++，生产力远超 C 和 C++。有 JVM 在，可以轻松地跨平台，做代码优化，做 AOP 和 IoC 这样的高级技术。以 Spring 为首的由庞大的社区开发的高质量的各种轮子让你只需关注业务，是能够快速搭建企业级应用的不二之选。</li>
</ul>
<p>此外，推荐学习 Go 语言。一方面，Go 语言现在很受关注，它是取代 C 和 C++ 的另一门有潜力的语言。C 语言太原始了，C++ 太复杂了，Java 太高级了，所以 Go 语言就在这个夹缝中出现了。这门语言已经 10 多年了，其已成为云计算领域事实上的标准语言，尤其是在 Docker/Kubernetes 等项目中。Go 语言社区正在不断地从 Java 社区移植各种 Java 的轮子过来，Go 社区现在也很不错。</p>
<p>如果要写一些 PaaS 层的应用，Go 语言会比 C 和 C++ 更好，目前和 Java 有一拼。而且，Go 语言在国内外一些知名公司中有了一定的应用和实践，所以，是可以学习的（参看：《<a href="https://coolshell.cn/articles/18190.html" target="_blank" rel="noopener">Go 语言、Docker 和新技术</a>》一文）。此外，Go 语言语法特别简单，有了 C 和 C++ 的基础，学习 Go 的学习成本基本为零。</p>
<p><strong>理论学科</strong>。你需要学习像算法、数据结构、网络模型、计算机原理等计算机科学专业需要学习的知识。为什么要学好这些理论上的知识呢？</p>
<ul>
<li>其一，这些理论知识可以说是计算机科学这门学科最精华的知识了。说得大一点，这些是人类智慧的精华。你只要想成为高手，这些东西是你必需要掌握和学习的。</li>
<li>其二，当你在解决一些很复杂或是很难的问题时，这些基础理论知识可以帮到你很多。我过去这 20 年从这些基础理论知识中受益匪浅。</li>
<li>其三，这些理论知识的思维方式可以让你有触类旁通，一通百通的感觉。虽然知识比较难啃，但啃过以后，你将获益终生。</li>
</ul>
<p>另外，你千万不要觉得在你的日常工作或是生活当中根本用不上，学了也白学，这样的思维方式千万不要有，因为这是平庸的思维方式。如果你想等我用到了再学也不晚，那么你有必要看一下这篇文章《<a href="https://coolshell.cn/articles/4235.html" target="_blank" rel="noopener">程序员的荒谬之言还是至理名言？</a>》。</p>
<p><strong>系统知识</strong>。系统知识是理论知识的工程实践，这里面有很多很多的细节。比如像 Unix/Linux、TCP/IP、C10K 挑战等这样专业的系统知识。这些知识是你能不能把理论应用到实际项目当中，能不能搞定实际问题的重要知识。</p>
<p>当你在编程的时候，如何和系统进行交互或是获取操作系统的资源，如何进行通讯，当系统出了性能问题，当系统出了故障等，你有大量需要落地的事需要处理和解决。这个时候，这些系统知识就会变得尤为关键和重要了。</p>
<p>这些东西，你可以认为是计算机世界的物理世界，上层无论怎么玩，无论是 Java NIO，还是 Nginx，还是 Node.js，它们都逃脱不掉最下层的限制。所以，你要好好学习这方面的知识。</p>
<h1 id="编程语言"><a href="#编程语言" class="headerlink" title="编程语言"></a>编程语言</h1><h2 id="Java-语言"><a href="#Java-语言" class="headerlink" title="Java 语言"></a>Java 语言</h2><p>学习 Java 语言有以下<strong>入门级的书</strong>（注意：下面一些书在入门篇中有所提及，但为了完整性，还是要在这里提一下，因为可能有朋友是跳着看的）。</p>
<ul>
<li>《<a href="https://book.douban.com/subject/26880667/" target="_blank" rel="noopener">Java 核心技术：卷 1 基础知识</a>》，这本书本来是 Sun 公司的官方用书，是一本 Java 的入门参考书。对于 Java 初学者来说，是一本非常不错的值得时常翻阅的技术手册。书中有较多地方进行 Java 与 C++ 的比较，因为当时 Java 面世的时候，又被叫作 “C++ Killer”。而我在看这本书的时候，发现书中有很多 C++ 的东西，于是又去学习了 C++。学习 C++ 的时候，发现有很多 C 的东西不懂，又顺着去学习了 C。然后，C -&gt; C++ -&gt; Java 整条线融汇贯通，这对我未来的技术成长有非常大的帮助。</li>
<li>有了上述的入门后，Java 的 Spring 框架是你玩 Java 所无法回避的东西，所以接下来是两本 Spring 相关的书，《<a href="https://book.douban.com/subject/26767354/" target="_blank" rel="noopener">Spring 实战</a>》和《<a href="https://book.douban.com/subject/26857423/" target="_blank" rel="noopener">Spring Boot 实战</a>》。前者是传统的 Spring，后者是新式的微服务的 Spring。如果你只想看一本的话，那么就看后者吧。</li>
</ul>
<p>认真学习前面的书可以让你成功入门 Java，但想要进一步成长，就要看下面我推荐的几本<strong>提升级的书</strong>。</p>
<ul>
<li>接下来，你需要了解了一下如何编写高效的代码，于是必需看一下《<a href="https://book.douban.com/subject/27047716/" target="_blank" rel="noopener">Effective Java</a>》（注意，这里我给的引用是第三版的，也是 2017 年末出版的书），这本书是模仿 Scott Meyers 的经典图书《Effective C++》的。Effective 这种书基本上都是各种经验之谈，所以，这是一本非常不错的书，你一定要读。这里需要推荐一下 <a href="https://github.com/google/guava" target="_blank" rel="noopener">Google Guava 库</a> ，这个库不但是 JDK 的升级库，其中有如：集合（collections）、缓存（caching）、原生类型支持（primitives support）、并发库（concurrency libraries）、通用注解（common annotations）、字符串处理（string processing）、I/O 等库，其还是 Effective Java 这本书中的那些经验的实践代表。</li>
<li>《<a href="https://book.douban.com/subject/10484692/" target="_blank" rel="noopener">Java 并发编程实战</a>》，是一本完美的 Java 并发参考手册。书中从并发性和线程安全性的基本概念出发，介绍了如何使用类库提供的基本并发构建块，用于避免并发危险、构造线程安全的类及验证线程安全的规则，如何将小的线程安全类组合成更大的线程安全类，如何利用线程来提高并发应用程序的吞吐量，如何识别可并行执行的任务，如何提高单线程子系统的响应性，如何确保并发程序执行预期任务，如何提高并发代码的性能和可伸缩性等内容。最后介绍了一些高级主题，如显式锁、原子变量、非阻塞算法以及如何开发自定义的同步工具类。</li>
<li>了解如何编写出并发的程序，你还需要了解一下如何优化 Java 的性能。我推荐《<a href="https://book.douban.com/subject/26740520/" target="_blank" rel="noopener">Java 性能权威指南</a>》。通过学习这本书，你可以比较大程度地提升性能测试的效果。其中包括：使用 JDK 中自带的工具收集 Java 应用的性能数据，理解 JIT 编译器的优缺点，调优 JVM 垃圾收集器以减少对程序的影响，学习管理堆内存和 JVM 原生内存的方法，了解如何最大程度地优化 Java 线程及同步的性能，等等。看完这本书后，如果你还有余力，想了解更多的底层细节，那么，你有必要去读一下《<a href="https://book.douban.com/subject/24722612/" target="_blank" rel="noopener">深入理解 Java 虚拟机</a>》。</li>
<li>《<a href="https://book.douban.com/subject/2130190/" target="_blank" rel="noopener">Java 编程思想</a>》，真是一本透着编程思想的书。上面的书让你从微观角度了解 Java，而这本书则可以让你从一个宏观角度了解 Java。这本书和 Java 核心技术的厚度差不多，但这本书的信息密度比较大。所以，读起来是非常耗大脑的，因为它会让你不断地思考。对于想学好 Java 的程序员来说，这是一本必读的书。</li>
<li>《<a href="https://book.douban.com/subject/26952826/" target="_blank" rel="noopener">精通 Spring 4.x</a>》，也是一本很不错的书，就是有点厚，一共有 800 多页，都是干货。我认为其中最不错的是在分析原理，尤其是针对前面提到的 Spring 技术，应用与原理都讲得很透彻，IOC 和 AOP 也分析得很棒，娓娓道来。其对任何一个技术都分析得很细致和全面，不足之处就是内容太多了，所以导致很厚，但这并不影响它是一本不错的工具书。</li>
</ul>
<p>当然，学 Java 你一定要学面向对象的设计模式，这里就只有一本经典的书《<a href="https://book.douban.com/subject/1052241/" target="_blank" rel="noopener">设计模式</a>》。如果你觉得有点儿难度了，那么可以看一下《<a href="https://book.douban.com/subject/2243615/" target="_blank" rel="noopener">Head First 设计模式</a>》。学习面向对象的设计模式时，你不要迷失在那 23 个设计模式中，你一定要明白这两个原则：</p>
<ul>
<li><strong>Program to an ‘interface’, not an ‘implementation’</strong><ul>
<li>使用者不需要知道数据类型、结构、算法的细节。</li>
<li>使用者不需要知道实现细节，只需要知道提供的接口。</li>
<li>利于抽象、封装，动态绑定，多态。符合面向对象的特质和理念。</li>
</ul>
</li>
<li><strong>Favor ‘object composition’ over ‘class inheritance’</strong><ul>
<li>继承需要给子类暴露一些父类的设计和实现细节。</li>
<li>父类实现的改变会造成子类也需要改变。</li>
<li>我们以为继承主要是为了代码重用，但实际上在子类中需要重新实现很多父类的方法。</li>
<li>继承更多的应该是为了多态。</li>
</ul>
</li>
</ul>
<p>至此，如果你把上面的这些知识都融汇贯通的话，那么，你已是一个高级的 Java 程序员了，我保证你已经超过了绝大多数程序员了。基本上来说，你在技术方面是可以进入到一线公司的，而且还不是一般的岗位，至少是高级程序员或是初级架构师的级别了。</p>
<h2 id="C-C-语言"><a href="#C-C-语言" class="headerlink" title="C/C++ 语言"></a>C/C++ 语言</h2><p>不像我出道那个时候，几乎所有的软件都要用 C 语言来写。现在，可能不会有多少人学习 C 语言了，因为一方面有 Java、Python 这样的高级语言为你屏蔽了很多的底层细节，另一方面也有像 Go 语言这样的新兴语言可以让你更容易地写出来也是高性能的软件。但是，我还是想说，C 语言是你必须学习的语言，因为这个世界上绝大多数编程语言都是 C-like 的语言，也是在不同的方面来解决 C 语言的各种问题。<strong>这里，我想放个比较武断话——如果你不学 C 语言，你根本没有资格说你是一个合格的程序员！</strong></p>
<ul>
<li>这里尤其推荐，已故的 C 语言之父 Dennis M. Ritchie 和著名科学家 Brian W. Kernighan 合作的圣经级的教科书《<a href="https://book.douban.com/subject/1139336/" target="_blank" rel="noopener">C 程序设计语言</a>》。注意，这本书是 C 语言原作者写的，其 C 语言的标准不是我们平时常说的 ANSI 标准，而是原作者的标准，又被叫作 K&amp;R C。但是这本书很轻薄，也简洁，不枯燥，是一本你可以拿着躺在床上看还不会看着看着睡着的书。</li>
<li>然后，还有一本非常经典的 C 语言的书《<a href="https://book.douban.com/subject/2280547/" target="_blank" rel="noopener">C 语言程序设计现代方法</a>》。有人说，这本书配合之前的 <a href="https://en.wikipedia.org/wiki/The_C_Programming_Language" target="_blank" rel="noopener">The C Programming Language</a> 那本书简真是无敌。我想说，这本书更实用，也够厚，完整覆盖了 C99 标准，习题的质量和水准也比较高。更好的是，探讨了现代编译器的实现，以及和 C++ 的兼容，还揭穿了各种古老的 C 语言的神话和信条……是相当相当干的一本学习 C 语言的书。</li>
</ul>
<p><strong>对了，千万不要看谭浩强的 C 语言的书。各种误导，我大学时就是用这本书学的 C，后来工作时被坑得不行</strong>。</p>
<p>在学习 C 语言的过程中，你一定会感到，C 语言这么底层，而且代码经常性地崩溃，经过一段时间的挣扎，你才开始觉得你从这个烂泥坑里快要爬出来了。但你还需要看看《<a href="https://book.douban.com/subject/2778632/" target="_blank" rel="noopener">C 陷阱与缺陷</a>》这本书，你会发现，这里面的坑不是一般大。</p>
<p>此时，如果你看过我的《编程范式游记》那个系列文章，你可能会发现 C 语言在泛型编程上的各种问题，这个时候我推荐你学习一下 C++ 语言。可能会有很多人觉得我说的 C++ 是个大坑。是的，这是世界目前来说最复杂也是最难的编程语言了。但是，<strong>C++ 是目前世界上范式最多的语言了，其做得最好的范式就是 “ 泛型编程 “，这在静态语言中，是绝对地划时代的一个事</strong>。</p>
<p>所以，你有必要学习一下 C++，看看 C++ 是如何解决 C 语言中的各种问题的。你可以先看看我的这篇文章 “<a href="https://coolshell.cn/articles/7992.html" target="_blank" rel="noopener">C++ 的坑真的多吗？</a>” ，有个基本认识。下面推荐几本 C++ 的书。</p>
<ul>
<li>《<a href="https://book.douban.com/subject/25708312/" target="_blank" rel="noopener">C++ Primer 中文版</a>》，这本书是久负盛名的 C++ 经典教程。书是有点厚，前面 1/3 讲 C 语言，后面讲 C++。C++ 的知识点实在是太多了，而且又有点晦涩。但是你主要就看几个点，一个是面向对象的多态，一个是模板和重载操作符，以及一些 STL 的东西。看看 C++ 是怎么玩泛型和函数式编程的。</li>
<li>如果你想继续研究，你需要看另外两本更为经典的书《<a href="https://book.douban.com/subject/5387403/" target="_blank" rel="noopener">Effective C++</a>》和《<a href="https://book.douban.com/subject/5908727/" target="_blank" rel="noopener">More Effective C++</a>》。 这两本书不厚，但是我读了 10 多年，每过一段时间再读一下，就会发现有更多的收获。这两本书的内容会随着你经历的丰富而变得丰富，这也是对我影响最大的两本书，其中影响最大的不是书中的那些 C++ 的东西，而是作者的思维方式和不断求真的精神，这真是太赞了。</li>
<li>学习 C/C++ 都是需要好好了解一下编译器到底干了什么事的。就像 Java 需要了解 JVM 一样，所以，这里还有一本非常非常难啃的书你可以挑战一下《<a href="https://book.douban.com/subject/10427315/" target="_blank" rel="noopener">深度探索 C++ 对象模型
</a>》。这本书是非常之经典的，看完后，C++ 对你来说就再也没有什么秘密可言。我以前写过的《<a href="https://coolshell.cn/articles/12165.html" target="_blank" rel="noopener">C++ 虚函数表解析</a>》，还有《<a href="https://coolshell.cn/articles/12176.html" target="_blank" rel="noopener">C++ 对象内存布局</a>》属于这个范畴。</li>
<li>还有 C++ 的作者 Bjarne Stroustrup 写的 <a href="http://www.stroustrup.com/bs_faq.html" target="_blank" rel="noopener">C++ FAQ</a> （<a href="http://www.stroustrup.com/bsfaqcn.html" target="_blank" rel="noopener">中文版</a>），也是非常值得一读的。</li>
</ul>
<h2 id="学习-Go-语言"><a href="#学习-Go-语言" class="headerlink" title="学习 Go 语言"></a>学习 Go 语言</h2><p>C 语言太原始了，C++ 太复杂了，Go 语言是不二之选。有了 C/C++ 的功底，学习 Go 语言非常简单。</p>
<p>首推 <a href="https://gobyexample.com/" target="_blank" rel="noopener">Go by Example</a> 作为你的入门教程。然后，<a href="https://go101.org/article/101.html" target="_blank" rel="noopener">Go 101</a> 也是一个很不错的在线电子书。如果你想看纸书的话，<a href="https://book.douban.com/subject/26337545/" target="_blank" rel="noopener">The Go Programming Language</a> 一书在豆瓣上有 9.2 分，但是国内没有卖的。（当然，我以前也写过两篇入门的供你参考 “<a href="https://coolshell.cn/articles/8460.html" target="_blank" rel="noopener">GO 语言简介（上）- 语法</a>” 和 “<a href="https://coolshell.cn/articles/8489.html" target="_blank" rel="noopener">GO 语言简介（下）- 特性</a>”）。</p>
<p>另外，Go 语言官方的 <a href="https://golang.org/doc/effective_go.html" target="_blank" rel="noopener">Effective Go</a> 是必读的，这篇文章告诉你如何更好地使用 Go 语言，以及 Go 语言中的一些原理。</p>
<p>Go 语言最突出之处是并发编程，Unix 老牌黑客罗勃·派克（Rob Pike）在 Google I/O 上的两个分享，可以让你学习到一些并发编程的模式。</p>
<ul>
<li>Go Concurrency Patterns（ <a href="https://talks.golang.org/2012/concurrency.slide" target="_blank" rel="noopener">幻灯片</a>和<a href="https://www.youtube.com/watch?v=f6kdp27TYZs" target="_blank" rel="noopener">演讲视频</a>）。</li>
<li>Advanced Go Concurrency Patterns（<a href="https://talks.golang.org/2013/advconc.slide" target="_blank" rel="noopener">幻灯片</a>、<a href="https://youtu.be/QDDwwePbDtw" target="_blank" rel="noopener">演讲视频</a>）。</li>
</ul>
<p>然后，Go 在 GitHub 的 wiki 上有好多不错的学习资源，你可以从中学习到多。比如：</p>
<ul>
<li><a href="https://github.com/golang/go/wiki/Articles" target="_blank" rel="noopener">Go 精华文章列表</a>。</li>
<li><a href="https://github.com/golang/go/wiki/Blogs" target="_blank" rel="noopener">Go 相关博客列表</a>。</li>
<li><a href="https://github.com/golang/go/wiki/GoTalks" target="_blank" rel="noopener">Go Talks</a>。</li>
</ul>
<p>此外，还有个内容丰富的 Go 资源列表 <a href="https://github.com/avelino/awesome-go" target="_blank" rel="noopener">Awesome Go</a>，推荐看看。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>在编程语言方面，推荐学习 C、C++、Java 和 Go 四门语言，并分别阐释了推荐的原因。</p>
<ul>
<li>我认为，C 语言是必须学习的语言，因为这个世界上绝大多数编程语言都是 C-like 的语言，也是在不同的方面来解决 C 语言的各种问题。</li>
<li>而 C++ 虽然复杂难学，但它几乎是目前世界上范式最多的语言了，其做得最好的范式就是 “ 泛型编程 “，这在静态语言中，是绝对地划时代的一个事。尤其要看看 C++ 是如何解决 C 语言中的各种问题的。</li>
<li>Java 是综合能力最强的语言。其实我是先学了 Java，然后又去学了 C++，之后去学了 C 语言的。C -&gt; C++ -&gt; Java 整条线融汇贯通，这对我未来的技术成长有非常大的帮助。</li>
<li>在文章最末，我推荐了 Go 语言，并给出了相关的学习资料。</li>
</ul>
<p>一个合格的程序员应该掌握几门语言。一方面，这会让你对不同的语言进行比较，让你有更多的思考。另一方面，这也是一种学习能力的培养，会让你对于未来的新技术学习得更快。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/06/%E7%B3%BB%E7%BB%9F%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/06/%E7%B3%BB%E7%BB%9F%E7%9F%A5%E8%AF%86/" class="post-title-link" itemprop="url">计算机系统知识</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-06 16:50:40 / Modified: 17:55:01" itemprop="dateCreated datePublished" datetime="2020-02-06T16:50:40-05:00">2020-02-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>首先推荐的是翻译版图书《<a href="https://book.douban.com/subject/5333562/" target="_blank" rel="noopener">深入理解计算机系统</a>》，原书名为《Computer Systems A Programmer’s Perspective》。不过，这本书叫做《程序员所需要了解的计算机知识》更为合适。</p>
<p>本书的最大优点是为程序员描述计算机系统的实现细节，帮助其在大脑中构造一个层次型的计算机系统。从最底层的数据在内存中的表示到流水线指令的构成，到虚拟存储器，到编译系统，到动态加载库，到最后的用户态应用。通过掌握程序是如何映射到系统上，以及程序是如何执行的，你能够更好地理解程序的行为为什么是这样的，以及效率低下是如何造成的。</p>
<p><strong>再强调一下，这本书是程序员必读的一本书！</strong></p>
<p>然后就是美国计算机科学家 <a href="https://zh.wikipedia.org/wiki/理查德·史蒂文斯" target="_blank" rel="noopener">理查德·史蒂文斯（Richard Stevens）</a> 的三套巨经典无比的书。（理查德·史蒂文斯于 1999 年 9 月 1 日离世，终年 48 岁。死因不详，有人说是滑雪意外，有人说是攀岩意外，有人说是滑翔机意外。总之，家人没有透露。大师的 <a href="http://www.kohala.com/start/" target="_blank" rel="noopener">个人主页</a> 今天还可以访问。）</p>
<ul>
<li>《<a href="https://book.douban.com/subject/1788421/" target="_blank" rel="noopener">Unix 高级环境编程</a>》。</li>
<li>《Unix 网络编程》 <a href="https://book.douban.com/subject/1500149/" target="_blank" rel="noopener">第 1 卷 套接口 API</a> 、<a href="https://book.douban.com/subject/4118577/" target="_blank" rel="noopener">第 2 卷 进程间通信</a> 。</li>
<li>《<a href="https://book.douban.com/subject/1088054/" target="_blank" rel="noopener">TCP/IP 详解 卷 I 协议</a>》。</li>
</ul>
<p>这几书的地位我就不多说了，你可以自己看相关的书评。但是，这三本书可能都不容易读，一方面是比较厚，另一方面是知识的密度太大了，所以，读起来有点枯燥和乏味。但是，这没办法，你得忍住。</p>
<p>这里要重点说一下《TCP/IP 详解》这本书，是一本很奇怪的书。这本书迄今至少被 <a href="http://portal.acm.org/citation.cfm?id=161724" target="_blank" rel="noopener">近五百篇学术论文引用过</a> 。这本写给工程师看的书居然被各种学院派的论文来引用，也是很神奇的一件事了。而且，虽然理查德·史蒂文斯不是 TCP 的发明人，但是这本书中把这个协议深入浅出地讲出来，还画了几百张时序图，也是令人叹为观止了。</p>
<p>如果你觉得上面这几本经典书比较难啃，你可以试试下面这些通俗易懂的（当然，如果读得懂上面那三本的，下面的这些也就不需要读了）。</p>
<ul>
<li>《<a href="https://book.douban.com/subject/4141733/" target="_blank" rel="noopener">Linux C 编程一站式学习</a>》。</li>
<li>《<a href="https://book.douban.com/subject/25911735/" target="_blank" rel="noopener">TCP/IP 网络编程</a>》。</li>
<li>《<a href="https://book.douban.com/subject/24737674/" target="_blank" rel="noopener">图解 TCP/IP</a>》，这本书其实并不是只讲了 TCP/IP，应该是叫《计算机网络》才对，主要是给想快速入门的人看的。</li>
<li>《<a href="http://www.tcpipguide.com/free/index.htm" target="_blank" rel="noopener">The TCP/IP Guide</a>》，这本书在豆瓣上的评分 9.2，这里给的链接是这本书的 HTML 英文免费版的，里面的图画得很精彩。</li>
</ul>
<p>另外，学习网络协议不单只是看书，最好用个抓包工具看看这些网络包是什么样的。所以，这里推荐一本书《<a href="https://book.douban.com/subject/21691692/" target="_blank" rel="noopener">Wireshark 数据包分析实战</a>》。在这本书中，作者结合一些简单易懂的实际网络案例，图文并茂地演示使用 Wireshark 进行数据包分析的技术方法，可以让我们更好地了解和学习网络协议。当然，也拥有了一定的黑客的技能。</p>
<p>看完《Unix 高级环境编程》后，你可以趁热打铁看看《<a href="https://book.douban.com/subject/25809330/" target="_blank" rel="noopener">Linux/Unix 系统编程手册</a>》或是罗伯特·拉姆（Robert Love）的 <a href="http://igm.univ-mlv.fr/~yahya/progsys/linux.pdf" target="_blank" rel="noopener">Linux System Programming 英文电子版</a> 。其中文翻译版<a href="https://book.douban.com/subject/25828773/" target="_blank" rel="noopener">Linux 系统编程</a> 也值得一读，虽然和《Unix 高级环境编程》很像，不过其主要突出的是 Linux 的一些关键技术和相关的系统调用。</p>
<p>关于 TCP 的东西，你还可以看看下面这一系列的文章。</p>
<ul>
<li><a href="http://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/" target="_blank" rel="noopener">Let’s code a TCP/IP stack, 1: Ethernet &amp; ARP</a></li>
<li><a href="http://www.saminiir.com/lets-code-tcp-ip-stack-2-ipv4-icmpv4/" target="_blank" rel="noopener">Let’s code a TCP/IP stack, 2: IPv4 &amp; ICMPv4</a></li>
<li><a href="http://www.saminiir.com/lets-code-tcp-ip-stack-3-tcp-handshake/" target="_blank" rel="noopener">Let’s code a TCP/IP stack, 3: TCP Basics &amp; Handshake</a></li>
<li><a href="http://www.saminiir.com/lets-code-tcp-ip-stack-4-tcp-data-flow-socket-api/" target="_blank" rel="noopener">Let’s code a TCP/IP stack, 4: TCP Data Flow &amp; Socket API</a></li>
<li><a href="http://www.saminiir.com/lets-code-tcp-ip-stack-5-tcp-retransmission/" target="_blank" rel="noopener">Let’s code a TCP/IP stack, 5: TCP Retransmission</a></li>
</ul>
<p><strong>对于系统知识，主要有以下一些学习要点。</strong></p>
<ul>
<li>用这些系统知识操作一下文件系统，实现一个可以拷贝目录树的小程序。</li>
<li>用 fork / wait / waitpid 写一个多进程的程序，用 pthread 写一个多线程带同步或互斥的程序。比如，多进程购票的程序。</li>
<li>用 signal / kill / raise / alarm / pause / sigprocmask 实现一个多进程间的信号量通信的程序。</li>
<li>学会使用 gcc 和 gdb 来编程和调试程序（参看《<strong>用 gdb 调试程序</strong>》<a href="https://blog.csdn.net/haoel/article/details/2879" target="_blank" rel="noopener">一</a>、<a href="https://blog.csdn.net/haoel/article/details/2880" target="_blank" rel="noopener">二</a>、<a href="https://blog.csdn.net/haoel/article/details/2881" target="_blank" rel="noopener">三</a>、<a href="https://blog.csdn.net/haoel/article/details/2882" target="_blank" rel="noopener">四</a>、<a href="https://blog.csdn.net/haoel/article/details/2883" target="_blank" rel="noopener">五</a>、<a href="https://blog.csdn.net/haoel/article/details/2884" target="_blank" rel="noopener">六</a>、<a href="https://blog.csdn.net/haoel/article/details/2885" target="_blank" rel="noopener">七</a>）。</li>
<li>学会使用 makefile 来编译程序（参看《<strong>跟我一起写 makefile</strong>》<a href="https://blog.csdn.net/haoel/article/details/2886" target="_blank" rel="noopener">一</a>、<a href="https://blog.csdn.net/haoel/article/details/2887" target="_blank" rel="noopener">二</a>、<a href="https://blog.csdn.net/haoel/article/details/2888" target="_blank" rel="noopener">三</a>、<a href="https://blog.csdn.net/haoel/article/details/2889" target="_blank" rel="noopener">四</a>、<a href="https://blog.csdn.net/haoel/article/details/2890" target="_blank" rel="noopener">五</a>、<a href="https://blog.csdn.net/haoel/article/details/2891" target="_blank" rel="noopener">六</a>、<a href="https://blog.csdn.net/haoel/article/details/2892" target="_blank" rel="noopener">七</a>、<a href="https://blog.csdn.net/haoel/article/details/2893" target="_blank" rel="noopener">八</a>、<a href="https://blog.csdn.net/haoel/article/details/2894" target="_blank" rel="noopener">九</a>、<a href="https://blog.csdn.net/haoel/article/details/2895" target="_blank" rel="noopener">十</a>、<a href="https://blog.csdn.net/haoel/article/details/2896" target="_blank" rel="noopener">十一</a>、<a href="https://blog.csdn.net/haoel/article/details/2897" target="_blank" rel="noopener">十二</a>、<a href="https://blog.csdn.net/haoel/article/details/2898" target="_blank" rel="noopener">十三</a>、<a href="https://blog.csdn.net/haoel/article/details/2899" target="_blank" rel="noopener">十四</a>）。</li>
<li>Socket 的进程间通信。用 C 语言写一个 1 对 1 的聊天小程序，或是一个简单的 HTTP 服务器。</li>
</ul>
<h1 id="C10K-问题"><a href="#C10K-问题" class="headerlink" title="C10K 问题"></a>C10K 问题</h1><p>然后，当你读完《Unix 网络编程》后，千万要去读一下 “<a href="http://www.kegel.com/c10k.html" target="_blank" rel="noopener">C10K Problem</a> （<a href="https://www.oschina.net/translate/c10k" target="_blank" rel="noopener">中文翻译版</a>）”。提出这个问题的人叫丹·凯格尔（Dan Kegel），目前工作在美国 Google 公司。</p>
<p>他从 1978 年起开始接触计算机编程，是 Winetricks 的作者，也是 Wine 1.0 的管理员，同时也是 Crosstool（ 一个让 gcc/glibc 编译器更易用的工具套件）的作者。还是 Java JSR 51 规范的提交者并参与编写了 Java 平台的 NIO 和文件锁，同时参与了 RFC 5128 标准中有关 NAT 穿越（P2P 打洞）技术的描述和定义。</p>
<p>C10K 问题本质上是操作系统处理大并发请求的问题。对于 Web 时代的操作系统而言，对于客户端过来的大量的并发请求，需要创建相应的服务进程或线程。这些进程或线程多了，导致数据拷贝频繁（缓存 I/O、内核将数据拷贝到用户进程空间、阻塞）， 进程 / 线程上下文切换消耗大，从而导致资源被耗尽而崩溃。这就是 C10K 问题的本质。</p>
<p>了解这个问题，并了解操作系统是如何通过多路复用的技术来解决这个问题的，有助于你了解各种 I/O 和异步模型，这对于你未来的编程和架构能力是相当重要的。</p>
<p>另外，现在，整个世界都在解决 C10M 问题，推荐看看 <a href="http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html" target="_blank" rel="noopener">The Secret To 10 Million Concurrent Connections -The Kernel Is The Problem, Not The Solution</a> 一文。</p>
<h1 id="实践项目"><a href="#实践项目" class="headerlink" title="实践项目"></a>实践项目</h1><p>学习完了编程语言、理论学科和系统知识三部分内容，下面就来做几个实践项目，小试牛刀一下。实现语言可以用 C、C++ 或 Java。</p>
<p>实现一个 telnet 版本的聊天服务器，主要有以下需求。</p>
<ul>
<li>每个客户端可以用使用<code>telnet ip:port</code>的方式连接到服务器上。</li>
<li>新连接需要用用户名和密码登录，如果没有，则需要注册一个。</li>
<li>然后可以选择一个聊天室加入聊天。</li>
<li>管理员有权创建或删除聊天室，普通人员只有加入、退出、查询聊天室的权力。</li>
<li>聊天室需要有人数限制，每个人发出来的话，其它所有的人都要能看得到。</li>
</ul>
<p>实现一个简单的 HTTP 服务器，主要有以下需求。</p>
<ul>
<li>解释浏览器传来的 HTTP 协议，只需要处理 URL path。</li>
<li>然后把所代理的目录列出来。</li>
<li>在浏览器上可以浏览目录里的文件和下级目录。</li>
<li>如果点击文件，则把文件打开传给浏览器（浏览器能够自动显示图片、PDF，或 HTML、CSS、JavaScript 以及文本文件）。</li>
<li>如果点击子目录，则进入到子目录中，并把子目录中的文件列出来。</li>
</ul>
<p>实现一个生产者 / 消费者消息队列服务，主要有以下需求。</p>
<ul>
<li>消息队列采用一个 Ring-buffer 的数据结构。</li>
<li>可以有多个 topic 供生产者写入消息及消费者取出消息。</li>
<li>需要支持多个生产者并发写。</li>
<li>需要支持多个消费者消费消息（只要有一个消费者成功处理消息就可以删除消息）。</li>
<li>消息队列要做到不丢数据（要把消息持久化下来）。</li>
<li>能做到性能很高。</li>
</ul>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>学习完了专业编程方面最为重要的三部分内容：编程语言、理论学科和系统知识，针对这些内容做个小结。如果想看完推荐的那些书和知识，并能理解和掌握，估计怎么也得需要 4-5 年的时间。嗯，是的，就是一个计算机科学系科班出身的程序员需要学习的一些东西。这其中，最重要的是下面这几点。</p>
<p><strong>编程语言</strong>。以工业级的 C、C++、Java 这三门语言为主，这三门语言才是真正算得上工业级的编程语言，因为有工业级的标准化组织在控制着这几门语言，而且也有工业级的企业应用。尤其是 Java，还衍生出了大量的企业级架构上的开源生态。你至少需要掌握 C 语言和 Java 语言，这对你以后面对各式各样的编程语言是非常重要的。</p>
<p>此外，还推荐学习 Go 语言，它已成为云计算领域事实上的标准语言，尤其是在 Docker、Kubernetes 等项目中。而且，Go 语言在国内外一些知名公司中有了一定的应用和实践，并且其生态圈也越来越好。</p>
<p><strong>算法和数据结构</strong>。这个太重要了，尤其是最基础的算法和数据结构，这是任何一个称职的程序员都需要学习和掌握的。你必需要掌握。</p>
<p><strong>计算机的相关系统</strong>。你至少要掌握三个系统的基础知识，一个是操作系统，一个是网络系统，还有一个是数据库系统。它们分别代表着计算机基础构架的三大件——计算、存储、网络。</p>
<p>如果你能够走到这里，把前面的那些知识都了解了（不用精通，因为精通是需要时间和实践来慢慢锤炼出来的，所以，你也不用着急），那么你已经是一个非常非常合格的程序员了，而且你的潜力和可能性是非常非常高的。</p>
<p>如果经历过这些比较枯燥的理论知识，而且你还能有热情和成就感，那么我要恭喜你了。因为你已经超过了绝大多数人，而且还是排在上游的比较抢手的程序员了。我相信你至少可以找到年薪 50 万以上的工作了。</p>
<p>但是，你还需要很多的经验或是一些实践，以及一些大系统大项目的实际动手的经验。没关系，我们后面会有教你怎么实操的方法和攻略。</p>
<p>但是，往后面走，你需要开始需要术业有专攻了。下面给一些建议的方向。</p>
<ul>
<li><strong>底层方向</strong>：操作系统、文件系统、数据库、网络……</li>
<li><strong>架构方向</strong>：分布式系统架构、微服务、DevOps、Cloud Native……</li>
<li><strong>数据方向</strong>：大数据、机器学习、人工智能……</li>
<li><strong>前端方向</strong>：你对用户体验或是交互更感兴趣，那么你走前端的路吧。</li>
<li><strong>其它方向</strong>：比如，安全开发、运维开发、嵌入式开发……</li>
</ul>
<p>这些方向你要仔细选择，因为一旦选好，就要勇往直前地走下去，当然，你要回头转别的方向也没什么问题，因为你有前面的这些基础知识在身，所以，不用害怕。只是不同的方向上会有不同的经验积累，经验积累是看书看不来的，这个是转方向的成本。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/05/%E6%95%B0%E6%8D%AE%E5%BA%93/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/05/%E6%95%B0%E6%8D%AE%E5%BA%93/" class="post-title-link" itemprop="url">数据库</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-05 16:26:32 / Modified: 17:29:50" itemprop="dateCreated datePublished" datetime="2020-02-05T16:26:32-05:00">2020-02-05</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>对于数据库方向，重点就是两种数据库，一种是以 SQL 为代表的关系型数据库，另一种是以非 SQL 为代表的 NoSQL 数据库。关系型数据库主要有三个：Oracle、MySQL 和 Postgres。</p>
<p>在这里，只讨论越来越主流的 MySQL 数据库。首先，我们要了解数据库的一些实现原理和内存的一些细节，然后我们要知道数据的高可用和数据复制这些比较重要的话题，了解一下关系型数据库的一些实践和难点。然后，我们会进入到 NoSQL 数据库的学习。</p>
<p>NoSQL 数据库千奇百怪，其主要是解决了关系型数据库中的各种问题。第一个大问题就是数据的 Schema 非常多，用关系型数据库来表示不同的 Data Schema 是非常笨拙的，所以要有不同的数据库（如时序型、键值对型、搜索型、文档型、图结构型等）。另一个大问题是，关系型数据库的 ACID 是一件很讨厌的事，这极大地影响了数据库的性能和扩展性，所以 NoSQL 在这上面做了相应的妥协以解决大规模伸缩的问题。</p>
<p>对于一个程序员，你可能觉得数据库的事都是 DBA 的事，然而，这些事才真正是程序员的事。因为程序是需要和数据打交道的，所以程序员或架构师不仅需要设计数据模型，还要保证整体系统的稳定性和可用性，数据是整个系统中关键中的关键。所以，作为一个架构师或程序员，你必须了解最重要的数据存储——数据库。</p>
<h1 id="关系型数据库"><a href="#关系型数据库" class="headerlink" title="关系型数据库"></a>关系型数据库</h1><p>今天，关系型数据库最主要的两个代表是闭源的 Oracle 和开源的 MySQL。当然，还有很多了，比如微软的 SQL Server，IBM 的 DB2 等，还有开源的 PostgreSQL。关系型数据库的世界中有好多好多产品。当然，还是 Oracle 和 MySQL 是比较主流的。所以，这里主要介绍更为开放和主流的 MySQL。</p>
<p>如果你要玩 Oracle，这里只推荐一本书《<a href="https://book.douban.com/subject/5402711/" target="_blank" rel="noopener">Oracle Database  编程艺术</a>》，无论是开发人员还是 DBA，它都是必读的书。这本书的作者是 Oracle 公司的技术副总裁托马斯·凯特（Thomas Kyte），他也是世界顶级的 Oracle 专家。</p>
<p>这本书中深入分析了 Oracle 数据库体系结构，包括文件、内存结构以及构成 Oracle 数据库和实例的底层进程，利用具体示例讨论了一些重要的数据库主题，如锁定、并发控制、事务等。同时分析了数据库中的物理结构，如表、索引和数据类型，并介绍采用哪些技术能最优地使用这些物理结构。</p>
<ul>
<li><p>学习 MySQL，首先一定是要看<a href="https://dev.mysql.com/doc/" target="_blank" rel="noopener">MySQL 官方手册</a>。</p>
</li>
<li><p>然后，官方还有几个 PPT 也要学习一下。</p>
<ul>
<li><a href="https://www.mysql.com/cn/why-mysql/presentations/tune-mysql-queries-performance/" target="_blank" rel="noopener">How to Analyze and Tune MySQL Queries for Better Performance</a></li>
<li><a href="https://www.mysql.com/cn/why-mysql/presentations/mysql-performance-tuning101/" target="_blank" rel="noopener">MySQL Performance Tuning 101</a></li>
<li><a href="https://www.mysql.com/cn/why-mysql/presentations/mysql-performance-sys-schema/" target="_blank" rel="noopener">MySQL Performance Schema &amp; Sys Schema</a></li>
<li><a href="https://www.mysql.com/cn/why-mysql/presentations/mysql-performance-tuning-best-practices/" target="_blank" rel="noopener">MySQL Performance: Demystified Tuning &amp; Best Practices</a></li>
<li><a href="https://www.mysql.com/cn/why-mysql/presentations/mysql-security-best-practices/" target="_blank" rel="noopener">MySQL Security Best Practices</a></li>
<li><a href="https://www.mysql.com/cn/why-mysql/presentations/mysql-cluster-deployment-best-practices/" target="_blank" rel="noopener">MySQL Cluster Deployment Best Practices</a></li>
<li><a href="https://www.mysql.com/cn/why-mysql/presentations/mysql-high-availability-innodb-cluster/" target="_blank" rel="noopener">MySQL High Availability with InnoDB Cluster</a></li>
</ul>
</li>
<li><p>然后推荐《<a href="https://book.douban.com/subject/23008813/" target="_blank" rel="noopener">高性能 MySQL</a>》，这本书是 MySQL 领域的经典之作，拥有广泛的影响力。不但适合数据库管理员（DBA）阅读，也适合开发人员参考学习。不管是数据库新手还是专家，都能从本书中有所收获。</p>
</li>
<li><p>如果你对 MySQL 的内部原理有兴趣的话，可以看一下这本书《<a href="https://book.douban.com/subject/24708143/" target="_blank" rel="noopener">MySQL 技术内幕：InnoDB 存储引擎</a>》。当然，还有官网的<a href="https://dev.mysql.com/doc/internals/en/" target="_blank" rel="noopener">MySQL Internals Manual</a> 。</p>
</li>
<li><p>数据库的索引设计和优化也是非常关键的，这里还有一本书《<a href="https://book.douban.com/subject/26419771/" target="_blank" rel="noopener">数据库的索引设计与优化</a>》也是很不错的。虽然不是讲 MySQL 的，但是原理都是相通的。这也是上面推荐过的《高性能 MySQL》在其索引部分推荐的一本好书。</p>
<p>你千万不要觉得只有做数据库你才需要学习这种索引技术。不是的！在系统架构上，在分布式架构中，索引技术也是非常重要的。这本书对于索引性能进行了非常清楚的估算，不像其它书中只是模糊的描述，你一定会收获很多。</p>
</li>
</ul>
<p>下面还有一些不错的和 MySQL 相关的文章。</p>
<ul>
<li><a href="http://blog.codinglabs.org/articles/theory-of-mysql-index.html" target="_blank" rel="noopener">MySQL 索引背后的数据结构及算法原理</a></li>
<li><a href="https://medium.com/@kousiknath/data-structures-database-storage-internals-1f5ed3619d43" target="_blank" rel="noopener">Some study on database storage internals</a></li>
<li><a href="https://medium.com/@Pinterest_Engineering/sharding-pinterest-how-we-scaled-our-mysql-fleet-3f341e96ca6f" target="_blank" rel="noopener">Sharding Pinterest: How we scaled our MySQL fleet</a></li>
<li><a href="https://www.mysql.com/cn/why-mysql/white-papers/mysql-guide-to-high-availability-solutions/" target="_blank" rel="noopener">Guide to MySQL High Availability</a></li>
<li><a href="https://dzone.com/articles/choosing-mysql-high-availability-solutions" target="_blank" rel="noopener">Choosing MySQL High Availability Solutions</a></li>
<li><a href="https://mariadb.com/sites/default/files/content/Whitepaper_High_availability_with_MariaDB-TX.pdf" target="_blank" rel="noopener">High availability with MariaDB TX: The definitive guide</a></li>
</ul>
<p>最后，还有一个 MySQL 的资源列表 <a href="https://shlomi-noach.github.io/awesome-mysql/" target="_blank" rel="noopener">Awesome MySQL</a>，这个列表中有很多的工具和开发资源，可以帮助你做很多事。</p>
<p>MySQL 有两个比较有名的分支，一个是 Percona，另一个是 MariaDB，其官网上的 Resources 页面中有很多不错的资源和文档，可以经常看看。 <a href="https://www.percona.com/resources" target="_blank" rel="noopener">Percona Resources</a>、<a href="https://mariadb.com/resources" target="_blank" rel="noopener">MariaDB Resources</a> ，以及它们的开发博客中也有很多不错的文章，分别为 <a href="https://www.percona.com/blog/" target="_blank" rel="noopener">Percona Blog</a> 和 <a href="https://mariadb.com/resources/blog" target="_blank" rel="noopener">MariaDB Blog</a>。</p>
<p>然后是关于 MySQL 的一些相关经验型的文章。</p>
<ul>
<li><a href="https://www.percona.com/live/mysql-conference-2015/sessions/bookingcom-evolution-mysql-system-design" target="_blank" rel="noopener">Booking.com: Evolution of MySQL System Design</a> ，Booking.com 的 MySQL 数据库使用的演化，其中有很多不错的经验分享，也是很多公司会遇到的的问题。</li>
<li><a href="https://medium.com/airbnb-engineering/tracking-the-money-scaling-financial-reporting-at-airbnb-6d742b80f040" target="_blank" rel="noopener">Tracking the Money - Scaling Financial Reporting at Airbnb</a> ，Airbnb 的数据库扩展的经验分享。</li>
<li><a href="https://eng.uber.com/mysql-migration/" target="_blank" rel="noopener">Why Uber Engineering Switched from Postgres to MySQL</a> ，无意比较两个数据库谁好谁不好，推荐这篇 Uber 的长文，主要是想让你从中学习到一些经验和技术细节，这是一篇很不错的文章。</li>
</ul>
<p>关于 MySQL 的集群复制，下面有这些文章供你学习一下，都是很不错的实践性比较强的文章。</p>
<ul>
<li><a href="https://engineering.imvu.com/2013/01/09/monitoring-delayed-replication-with-a-focus-on-mysql/" target="_blank" rel="noopener">Monitoring Delayed Replication, With A Focus On MySQL</a></li>
<li><a href="https://githubengineering.com/mitigating-replication-lag-and-reducing-read-load-with-freno/" target="_blank" rel="noopener">Mitigating replication lag and reducing read load with freno</a></li>
<li>另外，Booking.com 给了一系列的文章可以看看：<ul>
<li><a href="https://medium.com/booking-com-infrastructure/better-parallel-replication-for-mysql-14e2d7857813" target="_blank" rel="noopener">Better Parallel Replication for MySQL</a></li>
<li><a href="https://medium.com/booking-com-infrastructure/evaluating-mysql-parallel-replication-part-2-slave-group-commit-459026a141d2" target="_blank" rel="noopener">Evaluating MySQL Parallel Replication Part 2: Slave Group Commit</a></li>
<li><a href="https://medium.com/booking-com-infrastructure/evaluating-mysql-parallel-replication-part-3-benchmarks-in-production-db5811058d74" target="_blank" rel="noopener">Evaluating MySQL Parallel Replication Part 3: Benchmarks in Production</a></li>
<li><a href="https://medium.com/booking-com-infrastructure/evaluating-mysql-parallel-replication-part-4-more-benchmarks-in-production-49ee255043ab" target="_blank" rel="noopener">Evaluating MySQL Parallel Replication Part 4: More Benchmarks in Production
</a></li>
<li><a href="https://medium.com/booking-com-infrastructure/evaluating-mysql-parallel-replication-part-4-annex-under-the-hood-eb456cf8b2fb" target="_blank" rel="noopener">Evaluating MySQL Parallel Replication Part 4, Annex: Under the Hood</a></li>
</ul>
</li>
</ul>
<p>对于 MySQL 的数据分区来说，还有下面几篇文章你可以看看。</p>
<ul>
<li><a href="https://stackoverflow.com/questions/5541421/mysql-sharding-approaches" target="_blank" rel="noopener">StackOverflow: MySQL sharding approaches?</a></li>
<li><a href="https://www.percona.com/blog/2009/08/06/why-you-dont-want-to-shard/" target="_blank" rel="noopener">Why you don’t want to shard</a></li>
<li>[How to Scale Big Data Applications](<a href="https://www.percona.com/sites/default/files/presentations/How" target="_blank" rel="noopener">https://www.percona.com/sites/default/files/presentations/How</a> to Scale Big Data Applications.pdf)</li>
<li><a href="https://www.percona.com/blog/2016/08/30/mysql-sharding-with-proxysql/" target="_blank" rel="noopener">MySQL Sharding with ProxySQL</a></li>
</ul>
<p>然后，再看看各个公司做 MySQL Sharding 的一些经验分享。</p>
<ul>
<li><a href="https://devs.mailchimp.com/blog/using-shards-to-accommodate-millions-of-users/" target="_blank" rel="noopener">MailChimp: Using Shards to Accommodate Millions of Users
</a></li>
<li><a href="https://eng.uber.com/schemaless-rewrite/" target="_blank" rel="noopener">Uber: Code Migration in Production: Rewriting the Sharding Layer of Uber’s Schemaless Datastore</a></li>
<li><a href="https://instagram-engineering.com/sharding-ids-at-instagram-1cf5a71e5a5c" target="_blank" rel="noopener">Sharding &amp; IDs at Instagram</a></li>
<li><a href="https://medium.com/airbnb-engineering/how-we-partitioned-airbnb-s-main-database-in-two-weeks-55f7e006ff21" target="_blank" rel="noopener">Airbnb: How We Partitioned Airbnb’s Main Database in Two Weeks</a></li>
</ul>
<h1 id="NoSQL-数据库"><a href="#NoSQL-数据库" class="headerlink" title="NoSQL 数据库"></a>NoSQL 数据库</h1><p>关于 NoSQL 数据库，其最初目的就是解决大数据的问题。然而，也有人把其直接用来替换掉关系型数据库。所以在学习这个技术之前，我们需要对这个技术的一些概念和初衷有一定的了解。下面是一些推荐资料。</p>
<ul>
<li>Martin Fowler 在 YouTube 上分享的 NoSQL 介绍 <a href="https://youtu.be/qI_g07C_Q5I" target="_blank" rel="noopener">Introduction To NoSQL</a>， 以及他参与编写的 <a href="https://book.douban.com/subject/25662138/" target="_blank" rel="noopener">NoSQL Distilled - NoSQL 精粹</a>，这本书才 100 多页，是本难得的关于 NoSQL 的书，很不错，非常易读。</li>
<li><a href="https://medium.com/baqend-blog/nosql-databases-a-survey-and-decision-guidance-ea7823a822d#.nhzop4d23" target="_blank" rel="noopener">NoSQL Databases: a Survey and Decision Guidance</a>，这篇文章可以带你自上而下地从 CAP 原理到开始了解 NoSQL 的种种技术，是一篇非常不错的文章。</li>
<li><a href="https://resources.sei.cmu.edu/asset_files/WhitePaper/2014_019_001_90915.pdf" target="_blank" rel="noopener">Distribution, Data, Deployment: Software Architecture Convergence in Big Data Systems</a>，这是卡内基·梅隆大学的一篇讲分布式大数据系统的论文。其中主要讨论了在大数据时代下的软件工程中的一些关键点，也说到了 NoSQL 数据库。</li>
<li><a href="http://ianvarley.com/UT/MR/Varley_MastersReport_Full_2009-08-07.pdf" target="_blank" rel="noopener">No Relation: The Mixed Blessings of Non-Relational Databases</a>，这篇论文虽然有点年代久远。但这篇论文是 HBase 的基础，你花上一点时间来读读，就可以了解到，对各种非关系型数据存储优缺点的一个很好的比较。</li>
<li><a href="https://highlyscalable.wordpress.com/2012/03/01/nosql-data-modeling-techniques/" target="_blank" rel="noopener">NoSQL Data Modeling Techniques</a> ，NoSQL 建模技术。这篇文章曾经翻译在了 CoolShell 上，标题为 <a href="https://coolshell.cn/articles/7270.htm" target="_blank" rel="noopener">NoSQL 数据建模技术</a>。<ul>
<li><a href="https://docs.mongodb.com/manual/core/data-modeling-introduction/" target="_blank" rel="noopener">MongoDB - Data Modeling Introduction</a> ，虽然这是 MongoDB 的数据建模介绍，但是其很多观点可以用于其它的 NoSQL 数据库。</li>
<li><a href="https://firebase.google.com/docs/database/android/structure-data" target="_blank" rel="noopener">Firebase - Structure Your Database</a> ，Google 的 Firebase 数据库使用 JSON 建模的一些最佳实践。</li>
</ul>
</li>
<li>因为 CAP 原理，所以当你需要选择一个 NoSQL 数据库的时候，你应该看看这篇文档 <a href="http://blog.nahurst.com/visual-guide-to-nosql-systems" target="_blank" rel="noopener">Visual Guide to NoSQL Systems</a>。</li>
</ul>
<p>选 SQL 还是 NoSQL，这里有两篇文章，值得你看看。</p>
<ul>
<li><a href="https://www.upwork.com/hiring/data/sql-vs-nosql-databases-whats-the-difference/" target="_blank" rel="noopener">SQL vs. NoSQL Databases: What’s the Difference?</a></li>
<li><a href="https://engineering.salesforce.com/sql-or-nosql-9eaf1d92545b" target="_blank" rel="noopener">Salesforce: SQL or NoSQL</a></li>
</ul>
<h1 id="各种-NoSQL-数据库"><a href="#各种-NoSQL-数据库" class="headerlink" title="各种 NoSQL 数据库"></a>各种 NoSQL 数据库</h1><p>学习使用 NoSQL 数据库其实并不是一件很难的事，只要你把官方的文档仔细地读一下，是很容易上手的，而且大多数 NoSQL 数据库都是开源的，所以，也可以通过代码自己解决问题。下面我主要给出一些典型的 NoSQL 数据库的一些经验型的文章，供你参考。</p>
<p><strong>列数据库 Column Database</strong></p>
<ul>
<li>Cassandra 相关<ul>
<li>沃尔玛实验室有两篇文章值得一读。<ul>
<li><a href="https://medium.com/walmartlabs/avoid-pitfalls-in-scaling-your-cassandra-cluster-lessons-and-remedies-a71ca01f8c04" target="_blank" rel="noopener">Avoid Pitfalls in Scaling Cassandra Cluster at Walmart</a></li>
<li><a href="https://medium.com/walmartlabs/building-object-store-storing-images-in-cassandra-walmart-scale-a6b9c02af593" target="_blank" rel="noopener">Storing Images in Cassandra at Walmart</a></li>
</ul>
</li>
<li><a href="https://engineeringblog.yelp.com/2016/08/how-we-scaled-our-ad-analytics-with-cassandra.html" target="_blank" rel="noopener">Yelp: How We Scaled Our Ad Analytics with Apache Cassandra</a> ，Yelp 的这篇博客也有一些相关的经验和教训。</li>
<li><a href="https://blog.discordapp.com/how-discord-stores-billions-of-messages-7fa6ec7ee4c7" target="_blank" rel="noopener">Discord: How Discord Stores Billions of Messages</a> ，Discord 公司分享的一个如何存储十亿级消息的技术文章。</li>
<li><a href="https://www.slideshare.net/DataStax/cassandra-at-instagram-2016" target="_blank" rel="noopener">Cassandra at Instagram</a> ，Instagram 的一个 PPT，其中介绍了 Instagram 中是怎么使用 Cassandra 的。</li>
<li><a href="https://medium.com/netflix-techblog/benchmarking-cassandra-scalability-on-aws-over-a-million-writes-per-second-39f45f066c9e" target="_blank" rel="noopener">Netflix: Benchmarking Cassandra Scalability on AWS - Over a million writes per second</a> ，Netflix 公司在 AWS 上给 Cassandra 做的一个 Benchmark。</li>
</ul>
</li>
<li>HBase 相关<ul>
<li><a href="https://medium.com/imgur-engineering/imgur-notifications-from-mysql-to-hbase-9dba6fc44183" target="_blank" rel="noopener">Imgur Notification: From MySQL to HBASE</a></li>
<li><a href="https://medium.com/@Pinterest_Engineering/improving-hbase-backup-efficiency-at-pinterest-86159da4b954" target="_blank" rel="noopener">Pinterest: Improving HBase Backup Efficiency</a></li>
<li><a href="https://www.ibm.com/support/knowledgecenter/en/SSPT3X_2.1.2/com.ibm.swg.im.infosphere.biginsights.analyze.doc/doc/bigsql_TuneHbase.html" target="_blank" rel="noopener">IBM : Tuning HBase performance</a></li>
<li><a href="http://www.larsgeorge.com/2010/05/hbase-file-locality-in-hdfs.html" target="_blank" rel="noopener">HBase File Locality in HDFS</a></li>
<li><a href="http://borthakur.com/ftp/RealtimeHadoopSigmod2011.pdf" target="_blank" rel="noopener">Apache Hadoop Goes Realtime at Facebook</a></li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.294.8459&rep=rep1&type=pdf" target="_blank" rel="noopener">Storage Infrastructure Behind Facebook Messages: Using HBase at Scale</a></li>
<li><a href="https://github.com/rayokota/awesome-hbase" target="_blank" rel="noopener">GitHub: Awesome HBase</a></li>
</ul>
</li>
</ul>
<p>针对于 HBase 有两本书你可以考虑一下。</p>
<ul>
<li>首先，先推荐两本书，一本是偏实践的《<a href="https://book.douban.com/subject/25706541/" target="_blank" rel="noopener">HBase 实战</a>》，另一本是偏大而全的手册型的《<a href="https://book.douban.com/subject/10748460/" target="_blank" rel="noopener">HBase 权威指南</a>》。</li>
<li>当然，你也可以看看官方的 <a href="http://hbase.apache.org/0.94/book/book.html" target="_blank" rel="noopener">The Apache HBase™ Reference Guide</a></li>
<li>另外两个列数据库：<ul>
<li><a href="https://clickhouse.yandex/" target="_blank" rel="noopener">ClickHouse - Open Source Distributed Column Database at Yandex</a></li>
<li><a href="https://engineering.giphy.com/scaling-redshift-without-scaling-costs/" target="_blank" rel="noopener">Scaling Redshift without Scaling Costs at GIPHY</a></li>
</ul>
</li>
</ul>
<p><strong>文档数据库 Document Database - MongoDB, SimpleDB, CouchDB</strong></p>
<ul>
<li><a href="https://msdn.microsoft.com/en-us/magazine/hh547103.aspx" target="_blank" rel="noopener">Data Points - What the Heck Are Document Databases?</a></li>
<li><a href="https://www.mongodb.com/blog/post/ebay-building-mission-critical-multi-data-center-applications-with-mongodb" target="_blank" rel="noopener">eBay: Building Mission-Critical Multi-Data Center Applications with MongoDB</a></li>
<li><a href="https://medium.baqend.com/parse-is-gone-a-few-secrets-about-their-infrastructure-91b3ab2fcf71" target="_blank" rel="noopener">The AWS and MongoDB Infrastructure of Parse: Lessons Learned</a></li>
<li><a href="https://medium.com/build-addepar/migrating-mountains-of-mongo-data-63e530539952" target="_blank" rel="noopener">Migrating Mountains of Mongo Data</a></li>
<li><a href="https://engineering.linkedin.com/blog/2017/12/couchbase-ecosystem-at-linkedin" target="_blank" rel="noopener">Couchbase Ecosystem at LinkedIn</a></li>
<li><a href="https://medium.com/zendesk-engineering/resurrecting-amazon-simpledb-9404034ec506" target="_blank" rel="noopener">SimpleDB at Zendesk</a></li>
<li><a href="https://github.com/ramnes/awesome-mongodb" target="_blank" rel="noopener">Github: Awesome MongoDB</a></li>
</ul>
<p><strong>数据结构数据库 Data structure Database - Redis</strong></p>
<ul>
<li><a href="http://tech.trivago.com/2017/01/25/learn-redis-the-hard-way-in-production/" target="_blank" rel="noopener">Learn Redis the hard way (in production) at Trivago</a></li>
<li><a href="http://highscalability.com/blog/2014/9/8/how-twitter-uses-redis-to-scale-105tb-ram-39mm-qps-10000-ins.html" target="_blank" rel="noopener">Twitter: How Twitter Uses Redis To Scale - 105TB RAM, 39MM QPS, 10,000+ Instances</a></li>
<li><a href="https://slack.engineering/scaling-slacks-job-queue-687222e9d100" target="_blank" rel="noopener">Slack: Scaling Slack’s Job Queue - Robustly Handling Billions of Tasks in Milliseconds Using Kafka and Redis</a></li>
<li><a href="https://githubengineering.com/moving-persistent-data-out-of-redis/" target="_blank" rel="noopener">GitHub: Moving persistent data out of Redis at GitHub</a></li>
<li><a href="https://engineering.instagram.com/storing-hundreds-of-millions-of-simple-key-value-pairs-in-redis-1091ae80f74c" target="_blank" rel="noopener">Instagram: Storing Hundreds of Millions of Simple Key-Value Pairs in Redis</a></li>
<li><a href="https://www.infoq.com/presentations/twitch-pokemon" target="_blank" rel="noopener">Redis in Chat Architecture of Twitch (from 27:22)</a></li>
<li><a href="https://deliveroo.engineering/2016/10/07/optimising-session-key-storage.html" target="_blank" rel="noopener">Deliveroo: Optimizing Session Key Storage in Redis</a></li>
<li><a href="https://deliveroo.engineering/2017/01/19/optimising-membership-queries.html" target="_blank" rel="noopener">Deliveroo: Optimizing Redis Storage</a></li>
<li><a href="https://github.com/JamzyWang/awesome-redis" target="_blank" rel="noopener">GitHub: Awesome Redis</a></li>
</ul>
<p><strong>时序数据库 Time-Series Database</strong></p>
<ul>
<li><a href="https://blog.timescale.com/what-the-heck-is-time-series-data-and-why-do-i-need-a-time-series-database-dcf3b1b18563" target="_blank" rel="noopener">What is Time-Series Data &amp; Why We Need a Time-Series Database</a></li>
<li><a href="https://blog.timescale.com/time-series-data-why-and-how-to-use-a-relational-database-instead-of-nosql-d0cd6975e87c" target="_blank" rel="noopener">Time Series Data: Why and How to Use a Relational Database instead of NoSQL</a></li>
<li><a href="https://code.facebook.com/posts/952820474848503/beringei-a-high-performance-time-series-storage-engine/" target="_blank" rel="noopener">Beringei: High-performance Time Series Storage Engine @Facebook</a></li>
<li><a href="https://medium.com/netflix-techblog/introducing-atlas-netflixs-primary-telemetry-platform-bd31f4d8ed9a" target="_blank" rel="noopener">Introducing Atlas: Netflix’s Primary Telemetry Platform @Netflix</a></li>
<li><a href="https://blog.timescale.com/when-boring-is-awesome-building-a-scalable-time-series-database-on-postgresql-2900ea453ee2" target="_blank" rel="noopener">Building a Scalable Time Series Database on PostgreSQL</a></li>
<li><a href="https://medium.com/netflix-techblog/scaling-time-series-data-storage-part-i-ec2b6d44ba39" target="_blank" rel="noopener">Scaling Time Series Data Storage - Part I @Netflix</a></li>
<li><a href="https://medium.com/@leventov/design-of-a-cost-efficient-time-series-store-for-big-data-88c5dc41af8e" target="_blank" rel="noopener">Design of a Cost Efficient Time Series Store for Big Data</a></li>
<li><a href="https://github.com/xephonhq/awesome-time-series-database" target="_blank" rel="noopener">GitHub: Awesome Time-Series Database</a></li>
</ul>
<p><strong>图数据库 - Graph Platform</strong></p>
<ul>
<li>首先是 IBM Devloperworks 上的两个简介性的 PPT。<ul>
<li><a href="https://www.ibm.com/developerworks/library/cl-graph-database-1/cl-graph-database-1-pdf.pdf" target="_blank" rel="noopener">Intro to graph databases, Part 1, Graph databases and the CRUD operations</a></li>
<li><a href="https://www.ibm.com/developerworks/library/cl-graph-database-2/cl-graph-database-2-pdf.pdf" target="_blank" rel="noopener">Intro to graph databases, Part 2, Building a recommendation engine with a graph database</a></li>
</ul>
</li>
<li>然后是一本免费的电子书《<a href="http://graphdatabases.com" target="_blank" rel="noopener">Graph Database</a>》。</li>
<li>接下来是一些图数据库的介绍文章。<ul>
<li><a href="https://www.infoq.com/presentations/graph-database-scalability" target="_blank" rel="noopener">Handling Billions of Edges in a Graph Database</a></li>
<li><a href="https://neo4j.com/customers/" target="_blank" rel="noopener">Neo4j case studies with Walmart, eBay, AirBnB, NASA, etc</a></li>
<li><a href="https://blog.twitter.com/engineering/en_us/a/2010/introducing-flockdb.html" target="_blank" rel="noopener">FlockDB: Distributed Graph Database for Storing Adjacency Lists at Twitter</a></li>
<li><a href="https://architecht.io/google-ibm-back-new-open-source-graph-database-project-janusgraph-1d74fb78db6b" target="_blank" rel="noopener">JanusGraph: Scalable Graph Database backed by Google, IBM and Hortonworks</a></li>
<li><a href="https://aws.amazon.com/neptune/" target="_blank" rel="noopener">Amazon Neptune</a></li>
</ul>
</li>
</ul>
<p><strong>搜索数据库 - ElasticSearch</strong></p>
<ul>
<li><a href="https://www.elastic.co/guide/en/elasticsearch/guide/master/index.html" target="_blank" rel="noopener">Elasticsearch: The Definitive Guide</a> 这是官网方的 ElasticSearch 的学习资料，基本上来说，看这个就够了。</li>
<li>接下来是 4 篇和性能调优相关的工程实践。<ul>
<li><a href="https://www.ebayinc.com/stories/blogs/tech/elasticsearch-performance-tuning-practice-at-ebay/" target="_blank" rel="noopener">Elasticsearch Performance Tuning Practice at eBay</a></li>
<li><a href="https://kickstarter.engineering/elasticsearch-at-kickstarter-db3c487887fc" target="_blank" rel="noopener">Elasticsearch at Kickstarter</a></li>
<li><a href="https://www.loggly.com/blog/nine-tips-configuring-elasticsearch-for-high-performance/" target="_blank" rel="noopener">9 tips on ElasticSearch configuration for high performance</a></li>
<li><a href="https://medium.com/@abhidrona/elasticsearch-deployment-best-practices-d6c1323b25d7" target="_blank" rel="noopener">Elasticsearch In Production - Deployment Best Practices</a></li>
</ul>
</li>
<li>最后是 GitHub 上的资源列表 <a href="https://github.com/dzharii/awesome-elasticsearch" target="_blank" rel="noopener">GitHub: Awesome ElasticSearch</a> 。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/04/%E5%89%8D%E7%AB%AF%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/04/%E5%89%8D%E7%AB%AF%E4%BC%98%E5%8C%96/" class="post-title-link" itemprop="url">前端优化</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-04 17:55:14 / Modified: 18:58:48" itemprop="dateCreated datePublished" datetime="2020-02-04T17:55:14-05:00">2020-02-04</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="前端性能优化"><a href="#前端性能优化" class="headerlink" title="前端性能优化"></a>前端性能优化</h1><p>首先是推荐几本前端性能优化方面的图书。</p>
<ul>
<li><p><a href="http://www.allitebooks.in/web-performance-action/" target="_blank" rel="noopener">Web Performance in Action</a> ，这本书目前国内没有卖的。可以看电子版本，是一本很不错的书，其中有 CSS、图片、字体、JavaScript 性能调优等。</p>
</li>
<li><p><a href="http://designingforperformance.com/" target="_blank" rel="noopener">Designing for Performance</a> ，这本在线的电子书很不错，其中讲了很多网页优化的技术和相关的工具，可以让你对整体网页性能优化有所了解。</p>
</li>
<li><p><a href="https://book.douban.com/subject/5362856/" target="_blank" rel="noopener">High Performance JavaScript</a> ，这本书在国内可以买到，能让你了解如何提升各方面的性能，包括代码的加载、运行、DOM 交互、页面生存周期等。雅虎的前端工程师尼古拉斯·扎卡斯（Nicholas C. Zakas）和其他五位 JavaScript 专家介绍了页面代码加载的最佳方法和编程技巧，来帮助你编写更为高效和快速的代码。你还会了解到构建和部署文件到生产环境的最佳实践，以及有助于定位线上问题的工具。</p>
</li>
<li><p><a href="https://book.douban.com/subject/26411563/" target="_blank" rel="noopener">High Performance Web Sites: Essential Knowledge for Front-End Engineers</a> ，这本书国内也有卖，翻译版为《高性能网站建设指南：前端工程师技能精髓》。作者给出了 14 条具体的优化原则，每一条原则都配以范例佐证，并提供了在线支持。</p>
<p>全书内容丰富，主要包括减少 HTTP 请求、Edge Computing 技术、Expires Header 技术、gzip 组件、CSS 和 JavaScript 最佳实践、主页内联、Domain 最小化、JavaScript 优化、避免重定向的技巧、删除重复 JavaScript 的技巧、关闭 ETags 的技巧、Ajax 缓存技术和最小化技术等。</p>
</li>
<li><p>除了上面这几本书之外，Google 的 <a href="https://developers.google.com/web/fundamentals/" target="_blank" rel="noopener">Web Fundamentals</a> 里的 <a href="https://developers.google.com/web/fundamentals/performance/why-performance-matters/" target="_blank" rel="noopener">Performance</a> 这一章节也有很多非常不错的知识和经验。</p>
</li>
</ul>
<p>接下来是一些最佳实践性的文档。</p>
<ul>
<li><a href="http://browserdiet.com/zh/" target="_blank" rel="noopener">Browser Diet</a> ，前端权威性能指南（中文版）。这是一群为大型站点工作的专家们建立的一份前端性能的工作指南。</li>
<li><a href="https://developers.google.com/speed/docs/insights/rules" target="_blank" rel="noopener">PageSpeed Insights Rules</a> ，谷歌给的一份性能指南和最佳实践。</li>
<li><a href="https://developer.yahoo.com/performance/rules.html" target="_blank" rel="noopener">Best Practices for Speeding Up Your Web Site</a> ，雅虎公司给的一份 7 个分类共 35 个最佳实践的文档。</li>
</ul>
<p>接下来，重点推荐一个性能优化的案例学习网站 <a href="https://wpostats.com/" target="_blank" rel="noopener">WPO Stats</a> 。WPO 是 Web Performance Optimization 的缩写，这个网站上有很多很不错的性能优化的案例分享，一定可以帮助你很多。</p>
<p>然后是一些文章和案例。</p>
<ul>
<li><a href="http://blog.httpwatch.com/2015/01/16/a-simple-performance-comparison-of-https-spdy-and-http2/" target="_blank" rel="noopener">A Simple Performance Comparison of HTTPS, SPDY and HTTP/2</a> ，这是一篇比较浏览器的 HTTPS、SPDY 和 HTTP/2 性能的文章，除了比较之外，还可以让你了解一些技术细节。</li>
<li><a href="https://www.nginx.com/blog/7-tips-for-faster-http2-performance/" target="_blank" rel="noopener">7 Tips for Faster HTTP/2 Performance</a> ，对于 HTTP/2 来说，Nginx 公司给出的 7 个增加其性能的小提示。</li>
<li><a href="https://slack.engineering/reducing-slacks-memory-footprint-4480fec7e8eb" target="_blank" rel="noopener">Reducing Slack’s memory footprint</a> ，Slack 团队减少内存使用量的实践。</li>
<li><a href="https://medium.com/@Pinterest_Engineering/driving-user-growth-with-performance-improvements-cfc50dafadd7" target="_blank" rel="noopener">Pinterest: Driving user growth with performance improvements</a> ，Pinterest 关于性能调优的一些分享，其中包括了前后端的一些性能调优实践。其实也是一些比较通用的玩法，这篇文章主要是想让前端的同学了解一下如何做整体的性能调优。</li>
<li><a href="http://jonraasch.com/blog/10-javascript-performance-boosting-tips-from-nicholas-zakas" target="_blank" rel="noopener">10 JavaScript Performance Boosting Tips</a> ，10 个提高 JavaScript 运行效率的小提示，挺有用的。</li>
<li><a href="http://www.guypo.com/17-statistics-to-sell-web-performance-optimization/" target="_blank" rel="noopener">17 Statistics to Sell Web Performance Optimization</a> ，这个网页上收集了好些公司的 Web 性能优化的工程分享，都是非常有价值的。</li>
<li><a href="http://deanhume.com/Home/BlogPost/getting-started-with-the-picture-element/8109" target="_blank" rel="noopener">Getting started with the Picture Element</a> ，这篇文章讲述了 Responsive 布局所带来的一些负面的问题。主要是图像适配的问题，其中引出了一篇文章 “<a href="https://dev.opera.com/articles/native-responsive-images/" target="_blank" rel="noopener">Native Responsive Images</a>“ ，值得一读。</li>
<li><a href="http://www.deanhume.com/Home/BlogPost/improve-page-load-times-with-dns-prefetching/80" target="_blank" rel="noopener">Improve Page Load Times With DNS Prefetching</a> ，这篇文章教了你一个如何降低 DNS 解析时间的小技术——DNS prefetching。</li>
<li><a href="http://www.html5rocks.com/en/tutorials/speed/rendering/" target="_blank" rel="noopener">Jank Busting for Better Rendering Performance</a> ，这是一篇 Google I/O 上的分享，关于前端动画渲染性能提升。</li>
<li><a href="https://developer.chrome.com/devtools/docs/javascript-memory-profiling" target="_blank" rel="noopener">JavaScript Memory Profiling</a> ，这是一篇谷歌官方教你如何使用 Chrome 的开发工具来分析 JavaScript 内存问题的文章。</li>
</ul>
<p>接下来是一些性能工具。在线性能测试分析工具太多，这里只推荐比较权威的。</p>
<ul>
<li><a href="https://developers.google.com/speed/pagespeed/" target="_blank" rel="noopener">PageSpeed</a> ，谷歌有一组 PageSpeed 工具来帮助你分析和优化网站的性能。Google 出品的，质量相当有保证。</li>
<li><a href="https://github.com/marcelduran/yslow" target="_blank" rel="noopener">YSlow</a> ，雅虎的一个网页分析工具。</li>
<li><a href="https://gtmetrix.com/" target="_blank" rel="noopener">GTmetrix</a> ，是一个将 PageSpeed 和 YSlow 合并起来的一个网页分析工具，并且加上一些 Page load 或是其它的一些分析。也是一个很不错的分析工具。</li>
<li><a href="https://github.com/davidsonfellipe/awesome-wpo" target="_blank" rel="noopener">Awesome WPO</a> ，在 GitHub 上的这个 Awesome 中，你可以找到更多的性能优化工具和资源。</li>
</ul>
<p>另外，中国的网络有各种问题（你懂的），所以，你不能使用 Google 共享的 JavaScript 链接来提速，你得用中国自己的。你可以到这里看看中国的共享库资源，<a href="http://chineseseoshifu.com/blog/china-hosted-javascript-libraries-jquery-dojo-boostrap.html" target="_blank" rel="noopener">Forget Google and Use These Hosted JavaScript Libraries in China</a> 。</p>
<h1 id="前端框架"><a href="#前端框架" class="headerlink" title="前端框架"></a>前端框架</h1><p>接下来，要学习的是 Web 前端的几大框架。目前而言，前端社区有三大框架 Angular.js、React.js 和 Vue.js。React 和 Vue 更为强劲一些，所以，这里只写和 React 和 Vue 相关的攻略。关于两者的比较，网上有好多文章。这里推荐几篇供你参考。</p>
<ul>
<li><a href="https://medium.com/unicorn-supplies/angular-vs-react-vs-vue-a-2017-comparison-c5c52d620176" target="_blank" rel="noopener">Angular vs. React vs. Vue: A 2017 comparison</a></li>
<li><a href="https://medium.com/js-dojo/react-or-vue-which-javascript-ui-library-should-you-be-using-543a383608d" target="_blank" rel="noopener">React or Vue: Which JavaScript UI Library Should You Be Using?</a></li>
<li><a href="https://medium.com/@TechMagic/reactjs-vs-angular5-vs-vue-js-what-to-choose-in-2018-b91e028fa91d" target="_blank" rel="noopener">ReactJS vs Angular5 vs Vue.js - What to choose in 2018?</a></li>
</ul>
<p>其实，比较这些框架的优缺点还有利弊并不是要比出个输赢，而是让你了解一下不同框架的优缺点。这些框架都是可以学习的。而在我们生活工作中具体要用哪个框架，最好还是要有一些出发点，比如，你是为了找份好的工作，为了快速地搭一个网站，为了改造一个大规模的前端系统，还是纯粹地为了学习……</p>
<p>不同的目的会导致不同的决定。并不希望上述的这些比较会让你进入 “ 二选一 “ 或是 “ 三选一 “ 的境地。只是想通过这些文章让你知道这些框架的设计思路和实现原理，这些才是让你受益一辈子的事。</p>
<h2 id="React-js-框架"><a href="#React-js-框架" class="headerlink" title="React.js 框架"></a>React.js 框架</h2><p>下面先来学习一下 React.js 框架。</p>
<p><strong>入门</strong></p>
<p>React 学起来并不复杂，就看 <a href="https://reactjs.org/tutorial/tutorial.html" target="_blank" rel="noopener">React 官方教程</a> 和其文档就好了（ <a href="https://doc.react-china.org/" target="_blank" rel="noopener">React 的中文教程</a> ）。</p>
<p>然后，下面的文章会带你了解一下 React.js 的基本原理。</p>
<ul>
<li><a href="https://medium.freecodecamp.org/all-the-fundamental-react-js-concepts-jammed-into-this-single-medium-article-c83f9b53eac2" target="_blank" rel="noopener">All the fundamental React.js concepts</a> ，这篇文章讲了所有的 React.js 的基本原理。</li>
<li><a href="https://blog.kentcdodds.com/learn-react-fundamentals-and-advanced-patterns-eac90341c9db" target="_blank" rel="noopener">Learn React Fundamentals and Advanced Patterns</a> ，这篇文章中有几个短视频，每个视频不超过 5 分钟，是学习 React 的一个很不错的地方。</li>
<li><a href="https://reactjs.org/docs/thinking-in-react.html" target="_blank" rel="noopener">Thinking in React</a>，这篇文章将引导你完成使用 React 构建可搜索产品数据表的思考过程。</li>
</ul>
<p><strong>提高</strong></p>
<p>学习一个技术最重要的是要学到其中的思想和方法。下面是一些学习 React 中最重要的东西。</p>
<ul>
<li><p><strong>状态</strong>，对于富客户端来说是非常麻烦也是坑最多的地方，这里有几篇文章你可以一读。</p>
<ul>
<li><a href="http://reactkungfu.com/2015/09/common-react-dot-js-mistakes-unneeded-state/" target="_blank" rel="noopener">Common React.js mistakes: Unneeded state</a> ，React.js 编程的常见错误——不必要的状态。</li>
<li><a href="https://www.reddit.com/r/reactjs/comments/3bjdoe/state_is_an_antipattern/" target="_blank" rel="noopener">State is an Anti-Pattern</a> ，关于如何做一个不错的组件的思考，很有帮助。</li>
<li><a href="https://www.safaribooksonline.com/blog/2015/10/29/react-local-component-state/" target="_blank" rel="noopener">Why Local Component State is a Trap</a> ，一些关于 “Single state tree” 的想法。</li>
<li><a href="https://daveceddia.com/thinking-statefully/" target="_blank" rel="noopener">Thinking Statefully</a> ，几个很不错的例子让你对声明式的有状态的技术有更好的理解。</li>
<li>传统上，解决 React 的状态问题一般用 Redux。在这里推荐 <a href="https://www.robinwieruch.de/tips-to-learn-react-redux/" target="_blank" rel="noopener">Tips to learn React + Redux in 2018</a> 。Redux 是一个状态粘合组件，一般来说，我们会用 Redux 来做一些数据状态和其上层 Component 上的同步。这篇教程很不错。</li>
<li>最后是 “State Architecture Patterns in React “ 系列文章，非常值得一读。<ul>
<li><a href="https://medium.com/@skylernelson_64801/state-architecture-patterns-in-react-a-review-df02c1e193c6" target="_blank" rel="noopener">Part 1: A Review</a></li>
<li><a href="https://medium.com/@skylernelson_64801/state-architecture-patterns-in-react-part-2-the-top-heavy-architecture-flux-and-performance-a388b928ce89" target="_blank" rel="noopener">Part 2: The Top-Heavy Architecture, Flux and Performance</a></li>
<li><a href="https://medium.com/@skylernelson_64801/state-architecture-patterns-in-react-part-3-articulation-points-zine-and-an-overall-strategy-cf076f906391" target="_blank" rel="noopener">Part 3: Articulation Points, zine and An Overall Strategy</a></li>
<li><a href="https://medium.com/@skylernelson_64801/state-architecture-patterns-in-react-part-4-purity-flux-duality-and-dataflow-d06016b3379a" target="_blank" rel="noopener">Part 4: Purity, Flux-duality and Dataflow</a></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>函数式编程</strong>。从 jQuery 过来的同学一定非常不习惯 React，而从 Java 等后端过来的程序员就会很习惯了。所以，React 就是后端人员开发的，或者说是做函数式编程的人开发的。对此，你需要学习一下 JavaScript 函数式编程的东西。</p>
<p>这里推荐一本免费的电子书 《<a href="https://github.com/MostlyAdequate/mostly-adequate-guide" target="_blank" rel="noopener">Professor Frisby’s Mostly Adequate Guide to Functional Programming</a>》，其中译版为《<a href="https://jigsawye.gitbooks.io/mostly-adequate-guide/content/" target="_blank" rel="noopener">JS 函数式编程指南中文版</a>》。</p>
<p>下面有几篇文章非常不错。前两篇和函数式编程有关的文章非常值得一读。后三篇是一些比较实用的函数式编程和 React 结合的文章。</p>
<ul>
<li><a href="https://medium.com/javascript-scene/master-the-javascript-interview-what-is-functional-programming-7f218c68b3a0?utm_source=mybridge&utm_medium=email&utm_campaign=read_more" target="_blank" rel="noopener">Master the JavaScript Interview: What is Functional Programming?</a></li>
<li><a href="https://medium.com/javascript-scene/the-rise-and-fall-and-rise-of-functional-programming-composable-software-c2d91b424c8c" target="_blank" rel="noopener">The Rise and Fall and Rise of Functional Programming (Composing Software)</a></li>
<li><a href="https://blog.risingstack.com/functional-ui-and-components-as-higher-order-functions/" target="_blank" rel="noopener">Functional UI and Components as Higher Order Functions</a></li>
<li><a href="http://banderson.github.io/functional-js-reverse-engineering-the-hype/" target="_blank" rel="noopener">Functional JavaScript: Reverse-Engineering the Hype</a></li>
<li><a href="https://medium.com/javascript-inside/some-thoughts-on-function-components-in-react-cb2938686bc7" target="_blank" rel="noopener">Some Thoughts on Function Components in React</a></li>
</ul>
</li>
<li><p><strong>设计相关</strong>。接下来是学习一些 React 的设计模式。<a href="https://reactpatterns.com/" target="_blank" rel="noopener">React Pattern</a> 是一个不错的学习 React 模式的地方。除此之外，还有如下的一些不错的文章也会对你很有帮助的。</p>
<ul>
<li><a href="https://medium.com/@franleplant/react-higher-order-components-in-depth-cf9032ee6c3e" target="_blank" rel="noopener">React Higher Order Components in depth</a></li>
<li><a href="https://medium.com/@dan_abramov/smart-and-dumb-components-7ca2f9a7c7d0" target="_blank" rel="noopener">Presentational and Container Components</a></li>
<li><a href="https://goshakkk.name/controlled-vs-uncontrolled-inputs-react/" target="_blank" rel="noopener">Controlled and uncontrolled form inputs in React don’t have to be complicated</a></li>
<li><a href="https://medium.com/merrickchristensen/function-as-child-components-5f3920a9ace9" target="_blank" rel="noopener">Function as Child Components</a></li>
<li><a href="https://medium.com/styled-components/component-folder-pattern-ee42df37ec68" target="_blank" rel="noopener">Writing Scalable React Apps with the Component Folder Pattern</a></li>
<li><a href="https://medium.freecodecamp.org/reusable-web-application-strategies-d51517ea68c8" target="_blank" rel="noopener">Reusable Web Application Strategies</a></li>
<li><a href="https://medium.com/@robftw/characteristics-of-an-ideal-react-architecture-883b9b92be0b" target="_blank" rel="noopener">Characteristics of an Ideal React Architecture</a></li>
</ul>
</li>
<li><p><strong>实践和经验</strong></p>
</li>
</ul>
<p>还有一些不错的实践和经验。</p>
<ul>
<li><a href="https://camjackson.net/post/9-things-every-reactjs-beginner-should-know" target="_blank" rel="noopener">9 things every React.js beginner should know</a></li>
<li><a href="https://engineering.siftscience.com/best-practices-for-building-large-react-applications/" target="_blank" rel="noopener">Best practices for building large React applications</a></li>
<li><a href="https://americanexpress.io/clean-code-dirty-code/" target="_blank" rel="noopener">Clean Code vs. Dirty Code: React Best Practices</a></li>
<li><a href="https://dev.to/jakoblind/how-to-become-a-more-productive-react-developer" target="_blank" rel="noopener">How to become a more productive React Developer</a></li>
<li><a href="https://medium.freecodecamp.org/8-key-react-component-decisions-cc965db11594" target="_blank" rel="noopener">8 Key React Component Decisions</a></li>
</ul>
<p><strong>资源列表</strong></p>
<p>最后就是 React 的资源列表。</p>
<ul>
<li><a href="https://github.com/enaqx/awesome-react" target="_blank" rel="noopener">Awesome React</a> ，这是一些 React 相关资源的列表，很大很全。</li>
<li><a href="https://github.com/markerikson/react-redux-links" target="_blank" rel="noopener">React/Redux Links</a> ，这也是 React 相关的资源列表，与上面不一样的是，这个列表主要收集了大量的文章，其中讲述了很多 React 知识和技术，比上面的列表好很多。</li>
<li><a href="https://react.rocks/" target="_blank" rel="noopener">React Rocks</a> ，这个网站主要收集各种 React 的组件示例，可以让你大开眼界。</li>
</ul>
<h2 id="Vue-js-框架"><a href="#Vue-js-框架" class="headerlink" title="Vue.js 框架"></a>Vue.js 框架</h2><p>Vue 可能是一个更符合前端工程师习惯的框架。不像 React.js 那样使用函数式编程方式，是后端程序员的思路。</p>
<ul>
<li>通过文章 “<a href="https://medium.com/vue-mastery/why-43-of-front-end-developers-want-to-learn-vue-js-7f23348bc5be" target="_blank" rel="noopener">Why 43% of Front-End Developers want to learn Vue.js</a>” ，你可以看出其编程方式和 React 是大相径庭的，符合传统的前端开发的思维方式。</li>
<li>通过文章 <a href="https://www.smashingmagazine.com/2018/02/jquery-vue-javascript/" target="_blank" rel="noopener">Replacing jQuery With Vue.js: No Build Step Necessary</a> ，我们可以看到，从 jQuery 是可以平滑过度到 Vue 的。</li>
<li>另外，我们可以通过 “<a href="https://medium.com/@dalaidunc/10-things-i-love-about-vue-505886ddaff2" target="_blank" rel="noopener">10 things I love about Vue</a>” ，了解 Vue 的一些比较优秀的特性。</li>
</ul>
<p>最令人高兴的是，Vue 的作者是尤雨溪（Evan You），最近一次对他的采访 “<a href="https://blog.hackages.io/https-blog-hackages-io-evanyoubhack2017-cc5559806157" target="_blank" rel="noopener">Vue on 2018 - Interview with Evan You</a>” 当中有很多故事以及对 Vue 的展望。（<strong>注意：Vue 是完全由其支持者和用户资助的，这意味着它更接近社区而不受大公司的控制。</strong>）</p>
<p>要学习 Vue 并不难，上官网看文档（ <a href="http://vuejs.org/guide/" target="_blank" rel="noopener">Vue 官方文档</a>（<a href="https://cn.vuejs.org/v2/guide/" target="_blank" rel="noopener">中文版</a>）），照着搞一搞就可以很快上手了。<a href="https://laracasts.com/series/learn-vue-2-step-by-step" target="_blank" rel="noopener">Vue.js screencasts</a> 是一个很不错的英文视频教程。</p>
<p>另外，推荐 <a href="https://zhuanlan.zhihu.com/p/23134551" target="_blank" rel="noopener">新手向：Vue 2.0 的建议学习顺序</a> ，这是 Vue 作者写的，所以有特殊意义。</p>
<p>Vue 的确比较简单，有 Web 开发经验的人上手也比较快，所以这里也不会像 React 那样给出很多的资料。下面是一些还不错的内容，推荐给你。</p>
<ul>
<li><a href="https://itnext.io/how-not-to-vue-18f16fe620b5" target="_blank" rel="noopener">How not to Vue</a> ，任何技术都有坑，了解 Vue 的短板，你就能扬长避短，就能用得更好。</li>
<li><a href="https://alligator.io/vuejs/component-communication/" target="_blank" rel="noopener">Vue.js Component Communication Patterns</a></li>
<li><a href="https://medium.com/js-dojo/4-ajax-patterns-for-vue-js-apps-add915fc9168" target="_blank" rel="noopener">4 AJAX Patterns For Vue.js Apps</a></li>
<li><a href="https://vuejsdevelopers.com/2017/05/20/vue-js-safely-jquery-plugin/" target="_blank" rel="noopener">How To (Safely) Use A jQuery Plugin With Vue.js</a></li>
<li><a href="https://vuejsdevelopers.com/2017/03/24/vue-js-component-templates/" target="_blank" rel="noopener">7 Ways To Define A Component Template in Vue.js</a></li>
<li><a href="https://vuejsdevelopers.com/2017/04/22/vue-js-libraries-plugins/" target="_blank" rel="noopener">Use Any Javascript Library With Vue.js</a></li>
<li><a href="https://lobotuerto.com/blog/dynamic-and-async-components-made-easy-with-vuejs/" target="_blank" rel="noopener">Dynamic and async components made easy with Vue.js</a></li>
</ul>
<p>当然，最后一定还有 <a href="https://github.com/vuejs/awesome-vue" target="_blank" rel="noopener">Awesome Vue</a> ，Vue.js 里最为巨大最为优秀的资源列表。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=26924746&auto=1&height=66"></iframe>
      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yuanchen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">121</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">53</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yuanchen</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.0
  </div>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
