<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    hostname: new URL('http://yoursite.com').hostname,
    root: '/',
    scheme: 'Pisces',
    version: '7.7.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":false,"scrollpercent":false},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    comments: {"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: '',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}
  };
</script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Yuanchen&#39;s Blog">
<meta property="og:url" content="http://yoursite.com/page/8/index.html">
<meta property="og:site_name" content="Yuanchen&#39;s Blog">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Yuanchen">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/page/8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>Yuanchen's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Yuanchen's Blog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>

</nav>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/11/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E9%9A%BE%E7%82%B9/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/11/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E9%9A%BE%E7%82%B9/" class="post-title-link" itemprop="url">分布式架构难点</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-11 21:07:13 / Modified: 22:16:09" itemprop="dateCreated datePublished" datetime="2020-02-11T21:07:13-05:00">2020-02-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>对分布式服务化架构实践最早的应该是亚马逊。因为早在 2002 年的时候，亚马逊 CEO 杰夫·贝索斯（Jeff Bezos）就向全公司颁布了下面的这几条架构规定.</p>
<ol>
<li>所有团队的程序模块都要通过 Service Interface 方式将其数据与功能开放出来。</li>
<li>团队间程序模块的信息通信，都要通过这些接口。</li>
<li>除此之外没有其它的通信方式。其他形式一概不允许：不能直接链结别的程序（把其他团队的程序当做动态链接库来链接），不能直接读取其他团队的数据库，不能使用共享内存模式，不能使用别人模块的后门，等等。唯一允许的通信方式是调用 Service Interface。</li>
<li>任何技术都可以使用。比如：HTTP、CORBA、Pub/Sub、自定义的网络协议等。</li>
<li>所有的 Service Interface，毫无例外，都必须从骨子里到表面上设计成能对外界开放的。也就是说，团队必须做好规划与设计，以便未来把接口开放给全世界的程序员，没有任何例外。</li>
<li>不这样做的人会被炒鱿鱼。</li>
</ol>
<p>这应该就是 AWS（Amazon Web Service）出现的基因吧。当然，前面说过，采用分布式系统架构后会出现很多的问题。比如：</p>
<ul>
<li>一个线上故障的工单会在不同的服务和不同的团队中转过来转过去的。</li>
<li>每个团队都可能成为一个潜在的 DDoS 攻击者，除非每个服务都要做好配额和限流。</li>
<li>监控和查错变得更为复杂。除非有非常强大的监控手段。</li>
<li>服务发现和服务治理也变得非常复杂。</li>
</ul>
<p>为了克服这些问题，亚马逊这么多年的实践让其可以运维和管理极其复杂的分布式服务架构。主要有以下几点。</p>
<ol>
<li><strong>分布式服务的架构需要分布式的团队架构</strong>。在亚马逊，一个服务由一个小团队（Two Pizza Team 不超过 16 个人，两张 Pizza 可以喂饱的团队）负责，从前端负责到数据，从需求分析负责到上线运维。这是良性的分工策略——按职责分工，而不是按技能分工。</li>
<li><strong>分布式服务查错不容易</strong>。一旦出现比较严重的故障，需要整体查错。出现一个 S2 的故障，就可以看到每个团队的人都会上线。在工单系统里能看到，在故障发生的一开始，大家都在签到并自查自己的系统。如果没问题，也要在线待命（standby），等问题解决。</li>
<li><strong>没有专职的测试人员，也没有专职的运维人员，开发人员做所有的事情</strong>。开发人员做所有事情的好处是——吃自己的狗粮（Eat Your Own Dog Food） 最微观的实践。自己写的代码自己维护自己养，会让开发人员明白，写代码容易维护代码复杂。这样，开发人员在接需求、做设计、写代码、做工具时都会考虑到软件的长期维护性。</li>
<li><strong>运维优先，崇尚简化和自动化</strong>。为了能够运维如此复杂的系统，亚马逊内部在运维上下了非常大的功夫。现在人们所说的 DevOps 这个事，亚马逊在 10 多年前就做到了。亚马逊最为强大的就是运维，拼命地对系统进行简化和自动化，让亚马逊做到了可以轻松运维拥有上千万台虚机的 AWS 云平台。</li>
<li><strong>内部服务和外部服务一致</strong>。无论是从安全方面，还是接口设计方面，无论是从运维方面，还是故障处理的流程方面，亚马逊的内部系统都和外部系统一样对待。这样做的好处是，内部系统的服务随时都可以开放出来。而且，从第一天开始，服务提供方就有对外服务的能力。可以想像，以这样的标准运作的团队其能力会是什么样的。</li>
</ol>
<p>我们再来看一下分布式系统在技术上需要注意的问题。</p>
<h1 id="问题一：异构系统的不标准问题"><a href="#问题一：异构系统的不标准问题" class="headerlink" title="问题一：异构系统的不标准问题"></a>问题一：异构系统的不标准问题</h1><p>这主要表现在：</p>
<ul>
<li>软件和应用不标准。</li>
<li>通讯协议不标准。</li>
<li>数据格式不标准。</li>
<li>开发和运维的过程和方法不标准。</li>
</ul>
<p>不同的软件，不同的语言会出现不同的兼容性和不同的开发、测试、运维标准。不同的标准会让我们用不同的方式来开发和运维，引起架构复杂度的提升。比如：有的软件修改配置要改它的.conf 文件，而有的则是调用管理 API 接口。</p>
<p>在通讯方面，不同的软件用不同的协议，就算是相同的网络协议里也会出现不同的数据格式。还有，不同的团队因为用不同的技术，会有不同的开发和运维方式。这些不同的东西，会让我们的整个分布式系统架构变得异常复杂。所以，分布式系统架构需要有相应的规范。</p>
<p>比如，我看到，很多服务的 API 出错不返回 HTTP 的错误状态码，而是返回个正常的状态码 200，然后在 HTTP Body 里的 JSON 字符串中写着个：error，bla bla error message。这简直就是一种反人类的做法。我实在不明白为什么会有众多这样的设计。这让监控怎么做啊？现在，你应该使用 Swagger 的规范了。</p>
<p>再比如，我看到很多公司的软件配置管理里就是一个 key-value 的东西，这样的东西灵活到可以很容易地被滥用。不规范的配置命名，不规范的值，甚至在配置中直接嵌入前端展示内容……</p>
<p>一个好的配置管理，应该分成三层：底层和操作系统相关，中间层和中间件相关，最上面和业务应用相关。于是底层和中间层是不能让用户灵活修改的，而是只让用户选择。比如：操作系统的相关配置应该形成模板来让人选择，而不是让人乱配置的。只有配置系统形成了规范，我们才 hold 得住众多的系统。</p>
<p>再比如：数据通讯协议。通常来说，作为一个协议，一定要有协议头和协议体。协议头定义了最基本的协议数据，而协议体才是真正的业务数据。对于协议头，我们需要非常规范地让每一个使用这个协议的团队都使用一套标准的方式来定义，这样我们才容易对请求进行监控、调度和管理。</p>
<h1 id="问题二：系统架构中的服务依赖性问题"><a href="#问题二：系统架构中的服务依赖性问题" class="headerlink" title="问题二：系统架构中的服务依赖性问题"></a>问题二：系统架构中的服务依赖性问题</h1><p>对于传统的单体应用，一台机器挂了，整个软件就挂掉了。但是你千万不要以为在分布式的架构下不会发生这样的事。分布式架构下，服务是会有依赖的，于是一个服务依赖链上，某个服务挂掉了，会导致出现“多米诺骨牌”效应，会倒一片。</p>
<p>所以，在分布式系统中，服务的依赖也会带来一些问题。</p>
<ul>
<li>如果非关键业务被关键业务所依赖，会导致非关键业务变成一个关键业务。</li>
<li>服务依赖链中，出现“木桶短板效应”——整个 SLA 由最差的那个服务所决定。</li>
</ul>
<p>这是服务治理的内容了。服务治理不但需要我们定义出服务的关键程度，还需要我们定义或是描述出关键业务或服务调用的主要路径。没有这个事情，我们将无法运维或是管理整个系统。</p>
<p>这里需要注意的是，很多分布式架构在应用层上做到了业务隔离，然而，在数据库结点上并没有。如果一个非关键业务把数据库拖死，那么会导致全站不可用。所以，数据库方面也需要做相应的隔离。也就是说，最好一个业务线用一套自己的数据库。这就是亚马逊服务器的实践——系统间不能读取对方的数据库，只通过服务接口耦合。这也是微服务的要求。我们不但要拆分服务，还要为每个服务拆分相应的数据库。</p>
<h1 id="问题三：故障发生的概率更大"><a href="#问题三：故障发生的概率更大" class="headerlink" title="问题三：故障发生的概率更大"></a>问题三：故障发生的概率更大</h1><p>在分布式系统中，因为使用的机器和服务会非常多，所以，故障发生的频率会比传统的单体应用更大。只不过，单体应用的故障影响面很大，而分布式系统中，虽然故障的影响面可以被隔离，但是因为机器和服务多，出故障的频率也会多。另一方面，因为管理复杂，而且没人知道整个架构中有什么，所以非常容易犯错误。</p>
<p>你会发现，对分布式系统架构的运维，简直就是一场噩梦。我们会慢慢地明白下面这些道理。</p>
<ul>
<li>出现故障不可怕，故障恢复时间过长才可怕。</li>
<li>出现故障不可怕，故障影响面过大才可怕。</li>
</ul>
<p>运维团队在分布式系统下会非常忙，忙到每时每刻都要处理大大小小的故障。我看到，很多大公司，都在自己的系统里拼命地添加各种监控指标，有的能够添加出几万个监控指标。我觉得这完全是在“使蛮力”。一方面，信息太多等于没有信息，另一方面，SLA 要求我们定义出“Key Metrics”，也就是所谓的关键指标。然而，他们却没有。这其实是一种思维上的懒惰。</p>
<p>但是，上述的都是在“救火阶段”而不是“防火阶段”。所谓“防火胜于救火”，我们还要考虑如何防火，这需要我们在设计或运维系统时都要为这些故障考虑，即所谓 Design for Failure。在设计时就要考虑如何减轻故障。如果无法避免，也要使用自动化的方式恢复故障，减少故障影响面。</p>
<p>因为当机器和服务数量越来越多时，你会发现，人类的缺陷就成为了瓶颈。这个缺陷就是人类无法对复杂的事情做到事无巨细的管理，只有机器自动化才能帮助人类。 也就是，人管代码，代码管机器，人不管机器！</p>
<h1 id="问题四：多层架构的运维复杂度更大"><a href="#问题四：多层架构的运维复杂度更大" class="headerlink" title="问题四：多层架构的运维复杂度更大"></a>问题四：多层架构的运维复杂度更大</h1><p>通常来说，我们可以把系统分成四层：基础层、平台层、应用层和接入层。</p>
<ul>
<li>基础层就是我们的机器、网络和存储设备等。</li>
<li>平台层就是我们的中间件层，Tomcat、MySQL、Redis、Kafka 之类的软件。</li>
<li>应用层就是我们的业务软件，比如，各种功能的服务。</li>
<li>接入层就是接入用户请求的网关、负载均衡或是 CDN、DNS 这样的东西。</li>
</ul>
<p>对于这四层，我们需要知道：</p>
<ul>
<li>任何一层的问题都会导致整体的问题；</li>
<li>没有统一的视图和管理，导致运维被割裂开来，造成更大的复杂度。</li>
</ul>
<p>很多公司都是按技能分工是，把技术团队分为产品开发、中间件开发、业务运维、系统运维等子团队。这样的分工导致各管一摊，很多事情完全连不在一起。整个系统会像 “多米诺骨牌”一样，一个环节出现问题，就会倒下去一大片。因为没有一个统一的运维视图，不知道一个服务调用是如何经过每一个服务和资源，也就导致我们在出现故障时要花大量的时间在沟通和定位问题上。</p>
<p>从接入层到负载均衡，再到服务层，再到操作系统底层，设置的 KeepAlive 的参数完全不一致，导致用户发现，软件运行的行为和文档中定义的完全不一样。工程师查错的过程简直就是一场恶梦，以为找到了一个，结果还有一个，来来回回花了大量的时间才把所有 KeepAlive 的参数设置成一致的，浪费了太多的时间。</p>
<p><strong>分工不是问题，问题是分工后的协作是否统一和规范</strong>。这点，一定要重视。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>好了，我们来总结一下今天分享的主要内容。首先，我以亚马逊为例，讲述了它是如何做分布式服务架构的，遇到了哪些问题，以及是如何解决的。我认为，亚马逊在分布式服务系统方面的这些实践和经验积累，是 AWS 出现的基因。随后分享了在分布式系统中需要注意的几个问题，同时给出了应对方案。</p>
<p>我认为，构建分布式服务需要从组织，到软件工程，再到技术上的一次大的改造，需要比较长的时间来磨合和改进，并不断地总结教训和成功经验。下篇文章中，我们讲述分布式系统的技术栈。希望对你有帮助。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/10/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E7%BB%8F%E5%85%B8%E8%B5%84%E6%96%99/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/10/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E7%BB%8F%E5%85%B8%E8%B5%84%E6%96%99/" class="post-title-link" itemprop="url">分布式架构经典资料</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-10 18:18:40 / Modified: 19:25:20" itemprop="dateCreated datePublished" datetime="2020-02-10T18:18:40-05:00">2020-02-10</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="基础理论"><a href="#基础理论" class="headerlink" title="基础理论"></a>基础理论</h1><ul>
<li>CAP 定理</li>
<li>Fallacies of Distributed Computing</li>
</ul>
<h1 id="经典资料"><a href="#经典资料" class="headerlink" title="经典资料"></a>经典资料</h1><ul>
<li>Distributed systems theory for the distributed systems engineer</li>
<li>FLP Impossibility Result</li>
<li>An introduction to distributed systems</li>
<li>Distributed Systems for fun and profit</li>
<li>Distributed Systems: Principles and Paradigms</li>
<li>Scalable Web Architecture and Distributed Systems</li>
<li>Principles of Distributed Systems</li>
<li>Making reliable distributed systems in the presence of software errors</li>
<li>Designing Data Intensive Applications</li>
</ul>
<h1 id="基础理论-1"><a href="#基础理论-1" class="headerlink" title="基础理论"></a>基础理论</h1><h2 id="CAP-定理"><a href="#CAP-定理" class="headerlink" title="CAP 定理"></a><a href="https://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="noopener">CAP 定理</a></h2><p>CAP 定理是分布式系统设计中最基础，也是最为关键的理论。它指出，分布式数据存储不可能同时满足以下三个条件。</p>
<ul>
<li><strong>一致性（Consistency）</strong>：每次读取要么获得最近写入的数据，要么获得一个错误。</li>
<li><strong>可用性（Availability）</strong>：每次请求都能获得一个（非错误）响应，但不保证返回的是最新写入的数据。</li>
<li><strong>分区容忍（Partition tolerance）</strong>：尽管任意数量的消息被节点间的网络丢失（或延迟），系统仍继续运行。</li>
</ul>
<p>也就是说，CAP 定理表明，在存在网络分区的情况下，一致性和可用性必须二选一。而在没有发生网络故障时，即分布式系统正常运行时，一致性和可用性是可以同时被满足的。这里需要注意的是，CAP 定理中的一致性与 ACID 数据库事务中的一致性截然不同。</p>
<p>掌握 CAP 定理，尤其是能够正确理解 C、A、P 的含义，对于系统架构来说非常重要。因为对于分布式系统来说，网络故障在所难免，如何在出现网络故障的时候，维持系统按照正常的行为逻辑运行就显得尤为重要。你可以结合实际的业务场景和具体需求，来进行权衡。</p>
<p>例如，对于大多数互联网应用来说（如门户网站），因为机器数量庞大，部署节点分散，网络故障是常态，可用性是必须要保证的，所以只有舍弃一致性来保证服务的 AP。而对于银行等，需要确保一致性的场景，通常会权衡 CA 和 CP 模型，CA 模型网络故障时完全不可用，CP 模型具备部分可用性。</p>
<ul>
<li>CA (consistency + availability)，这样的系统关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提交”（2PC）。CA 系统不能容忍网络错误或节点错误，一旦出现这样的问题，整个系统就会拒绝写请求，因为它并不知道对面的那个结点是否挂掉了，还是只是网络问题。唯一安全的做法就是把自己变成只读的。</li>
<li>CP (consistency + partition tolerance)，这样的系统关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，比如：Paxos 算法 (Quorum 类的算法)。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。</li>
<li>AP (availability + partition tolerance)，这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。</li>
</ul>
<p>然而，还是有一些人会错误地理解 CAP 定理，甚至误用。Cloudera 工程博客中，<a href="http://blog.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/" target="_blank" rel="noopener">CAP Confusion: Problems with ‘partition tolerance’</a>一文中对此有详细的阐述。</p>
<p>推荐谷歌的<a href="http://www.youtube.com/watch?v=srOgpXECblk" target="_blank" rel="noopener">Transaction Across DataCenter 视频</a>。</p>
<h2 id="Fallacies-of-Distributed-Computing"><a href="#Fallacies-of-Distributed-Computing" class="headerlink" title="Fallacies of Distributed Computing"></a><a href="http://en.wikipedia.org/wiki/Fallacies_of_distributed_computing" target="_blank" rel="noopener">Fallacies of Distributed Computing</a></h2><p>本文是英文维基百科上的一篇文章。它是 Sun 公司的<a href="https://en.wikipedia.org/wiki/L_Peter_Deutsch" target="_blank" rel="noopener">劳伦斯·彼得·多伊奇（Laurence Peter Deutsch）</a>等人于 1994~1997 年提出的，讲的是刚刚进入分布式计算领域的程序员常会有的一系列错误假设。</p>
<p>多伊奇于 1946 年出生在美国波士顿。他创办了阿拉丁企业（Aladdin Enterprises），并在该公司编写出了著名的 Ghostscript 开源软件，于 1988 年首次发布。</p>
<p>他在学生时代就和艾伦·凯（Alan Kay）等比他年长的人一起开发了 Smalltalk，并且他的开发成果激发了后来 Java 语言 JIT 编译技术的创造灵感。他后来在 Sun 公司工作并成为 Sun 的公司院士。在 1994 年，他成为了 ACM 院士。</p>
<p>基本上，每个人刚开始建立一个分布式系统时，都做了以下 8 条假定。随着时间的推移，每一条都会被证明是错误的，也都会导致严重的问题，以及痛苦的学习体验。</p>
<ol>
<li>网络是稳定的。</li>
<li>网络传输的延迟是零。</li>
<li>网络的带宽是无穷大。</li>
<li>网络是安全的。</li>
<li>网络的拓扑不会改变。</li>
<li>只有一个系统管理员。</li>
<li>传输数据的成本为零。</li>
<li>整个网络是同构的。</li>
</ol>
<p>阿尔农·罗特姆 - 盖尔 - 奥兹（Arnon Rotem-Gal-Oz）写了一篇长文<a href="http://www.rgoarchitects.com/Files/fallacies.pdf" target="_blank" rel="noopener">Fallacies of Distributed Computing Explained</a>来解释这些点。</p>
<p>由于他写这篇文章的时候已经是 2006 年了，所以从中能看到这 8 条常见错误被提出十多年后还有什么样的影响：一是，为什么当今的分布式软件系统也需要避免这些设计错误；二是，在当今的软硬件环境里，这些错误意味着什么。比如，文中在谈“延迟为零”假设时，还谈到了 AJAX，而这是 2005 年开始流行的技术。</p>
<p>而<a href="http://blog.fogcreek.com/eight-fallacies-of-distributed-computing-tech-talk/" target="_blank" rel="noopener">加勒思·威尔逊（Gareth Wilson）的文章</a>则用日常生活中的例子，对这些点做了更为通俗的解释。</p>
<p>这 8 个需要避免的错误不仅对于中间件和底层系统开发者及架构师是重要的知识，而且对于网络应用程序开发者也同样重要。分布式系统的其他部分，如容错、备份、分片、微服务等也许可以对应用程序开发者部分透明，但这 8 点则是应用程序开发者也必须知道的。</p>
<p><strong>为什么我们要深刻地认识这 8 个错误？是因为，这要我们清楚地认识到——在分布式系统中错误是不可能避免的，我们能做的不是避免错误，而是要把错误的处理当成功能写在代码中。</strong></p>
<h1 id="经典资料-1"><a href="#经典资料-1" class="headerlink" title="经典资料"></a>经典资料</h1><h2 id="Distributed-systems-theory-for-the-distributed-systems-engineer"><a href="#Distributed-systems-theory-for-the-distributed-systems-engineer" class="headerlink" title="Distributed systems theory for the distributed systems engineer"></a><a href="http://the-paper-trail.org/blog/distributed-systems-theory-for-the-distributed-systems-engineer/" target="_blank" rel="noopener">Distributed systems theory for the distributed systems engineer</a></h2><p>本文作者认为，推荐大量的理论论文是学习分布式系统理论的错误方法，除非这是你的博士课程。因为论文通常难度大又很复杂，需要认真学习，而且需要理解这些研究成果产生的时代背景，才能真正的领悟到其中的精妙之处。</p>
<p>在本文中，作者给出了他整理的分布式工程师必须要掌握的知识列表，并直言掌握这些足够设计出新的分布式系统。首先，作者推荐了 4 份阅读材料，它们共同概括了构建分布式系统的难点，以及所有工程师必须克服的技术难题。</p>
<ul>
<li><a href="http://book.mixu.net/distsys/" target="_blank" rel="noopener">Distributed Systems for Fun and Profit</a>，这是一本小书，涵盖了分布式系统中的关键问题，包括时间的作用和不同的复制策略。后文中对这本书有较详细的介绍。</li>
<li><a href="https://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/" target="_blank" rel="noopener">Notes on distributed systems for young bloods</a>，这篇文章中没有理论，是一份适合新手阅读的分布式系统实践笔记。</li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628" target="_blank" rel="noopener">A Note on Distributed Systems</a>，这是一篇经典的论文，讲述了为什么在分布式系统中，远程交互不能像本地对象那样进行。</li>
<li><a href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing" target="_blank" rel="noopener">The fallacies of distributed computing</a>，每个分布式系统新手都会做的 8 个错误假设，并探讨了其会带来的影响。上文中专门对这篇文章做了介绍。</li>
</ul>
<p>随后，分享了几个关键点。</p>
<ul>
<li><strong>失败和时间（Failure and Time）</strong>。分布式系统工程师面临的很多困难都可以归咎于两个根本原因：1. 进程可能会失败；2. 没有好方法表明进程失败。这就涉及到如何设置系统时钟，以及进程间的通讯机制，在没有任何共享时钟的情况下，如何确定一个事件发生在另一个事件之前。</li>
</ul>
<p>可以参考 Lamport 时钟和 Vector 时钟，还可以看看<a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" target="_blank" rel="noopener">Dynamo 论文</a>。</p>
<ul>
<li><strong>容错的压力（The basic tension of fault tolerance）</strong>。能在不降级的情况下容错的系统一定要像没有错误发生的那样运行。这就意味着，系统的某些部分必须冗余地工作，从而在性能和资源消耗两方面带来成本。</li>
</ul>
<p>最终一致性以及其他技术方案在以系统行为弱保证为代价，来试图避免这种系统压力。阅读<a href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf" target="_blank" rel="noopener">Dynamo 论文</a>和帕特·赫尔兰（Pat Helland）的经典论文<a href="http://www.cloudtran.com/pdfs/LifeBeyondDistTRX.pdf" target="_blank" rel="noopener">Life Beyond Transactions</a>能获很得大启发。</p>
<ul>
<li><strong>基本原语（Basic primitives）</strong>。在分布式系统中几乎没有一致认同的基本构建模块，但目前在越来越多地在出现。比如 Leader 选举，可以参考<a href="https://en.wikipedia.org/wiki/Bully_algorithm" target="_blank" rel="noopener">Bully 算法</a>；分布式状态机复制，可以参考<a href="https://en.wikipedia.org/wiki/State_machine_replication" target="_blank" rel="noopener">维基百科</a>和<a href="https://www.microsoft.com/en-us/research/publication/how-to-build-a-highly-available-system-using-consensus/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fblampson%2F58-consensus%2Facrobat.pdf" target="_blank" rel="noopener">Lampson 的论文</a>，后者更权威，只是有些枯燥。</li>
<li><strong>基本结论（Fundamental Results）</strong>。某些事实是需要吸收理解的，有几点：如果进程之间可能丢失某些消息，那么不可能在实现一致性存储的同时响应所有的请求，这就是 CAP 定理；一致性不可能同时满足以下条件：a. 总是正确，b. 在异步系统中只要有一台机器发生故障，系统总是能终止运行——停止失败（FLP 不可能性）；一般而言，消息交互少于两轮都不可能达成共识（Consensus）。</li>
<li><strong>真实系统（Real systems）</strong>。学习分布式系统架构最重要的是，结合一些真实系统的描述，反复思考和点评其背后的设计决策。如谷歌的 GFS、Spanner、Chubby、BigTable、Dapper 等，以及 Dryad、Cassandra 和 Ceph 等非谷歌系统。</li>
</ul>
<h2 id="FLP-Impossibility-Result"><a href="#FLP-Impossibility-Result" class="headerlink" title="FLP Impossibility Result"></a><a href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf" target="_blank" rel="noopener">FLP Impossibility Result</a></h2><p>FLP 不可能性的名称起源于它的三位作者，Fischer、Lynch 和 Paterson。它是关于理论上能做出的功能最强的共识算法会受到怎样的限制的讨论。</p>
<p>所谓共识问题，就是让网络上的分布式处理者最后都对同一个结果值达成共识。该解决方案对错误有恢复能力，处理者一旦崩溃以后，就不再参与计算。在同步环境下，每个操作步骤的时间和网络通信的延迟都是有限的，要解决共识问题是可能的，方式是：等待一个完整的步长来检测某个处理者是否已失败。如果没有收到回复，那就假定它已经崩溃。</p>
<p>共识问题有几个变种，它们在“强度”方面有所不同——通常，一个更“强”问题的解决方案同时也能解决比该问题更“弱”的问题。共识问题的一个较强的形式如下。</p>
<p>给出一个处理者的集合，其中每一个处理者都有一个初始值：</p>
<ul>
<li>所有无错误的进程（处理过程）最终都将决定一个值；</li>
<li>所有会做决定的无错误进程决定的都将是同一个值；</li>
<li>最终被决定的值必须被至少一个进程提出过。</li>
</ul>
<p>这三个特性分别被称为“终止”、“一致同意”和“有效性”。任何一个具备这三点特性的算法都被认为是解决了共识问题。</p>
<p>FLP 不可能性则讨论了异步模型下的情况，主要结论有两条。</p>
<ol>
<li>在异步模型下不存在一个完全正确的共识算法。不仅上述较“强”形式的共识算法不可能实现，FLP 还证明了比它弱一些的、只需要有一些无错误的进程做决定就足够的共识算法也是不可能实现的。</li>
<li>在异步模型下存在一个部分正确的共识算法，前提是所有无错误的进程都总能做出一个决定，此外没有进程会在它的执行过程中死亡，并且初始情况下超过半数进程都是存活状态。</li>
</ol>
<p>FLP 的结论是，在异步模型中，仅一个处理者可能崩溃的情况下，就已经没有分布式算法能解决共识问题。这是该问题的理论上界。其背后的原因在于，异步模型下对于一个处理者完成工作然后再回复消息所需的时间并没有上界。因此，无法判断出一个处理者到底是崩溃了，还是在用较长的时间来回复，或者是网络有很大的延迟。</p>
<p>FLP 不可能性对我们还有别的启发。一是网络延迟很重要，网络不能长时间处于拥塞状态，否则共识算法将可能因为网络延迟过长而导致超时失败。二是计算时间也很重要。对于需要计算共识的处理过程（进程），如分布式数据库提交，需要在短时间里就计算出能否提交的结果，那就要保证计算结点资源充分，特别是内存容量、磁盘空闲时间和 CPU 时间方面要足够，并在软件层面确保计算不超时。</p>
<p>另一个问题是，像 Paxos 这样的共识算法为什么可行？实际上它并不属于 FLP 不可能性证明中所说的“完全正确”的算法。它的正确性会受超时值的影响。但这并不妨碍它在实践中有效，因为我们可以通过避免网络拥塞等手段来保证超时值是合适的。</p>
<h2 id="An-introduction-to-distributed-systems"><a href="#An-introduction-to-distributed-systems" class="headerlink" title="An introduction to distributed systems"></a><a href="https://github.com/aphyr/distsys-class" target="_blank" rel="noopener">An introduction to distributed systems</a></h2><p>它是<a href="https://github.com/aphyr/distsys-class#review-1" target="_blank" rel="noopener">分布式系统基础课</a>的课程提纲，也是一份很棒的分布式系统介绍，几乎涵盖了所有知识点，并辅以简洁并切中要害的说明文字，非常适合初学者提纲挈领地了解知识全貌，快速与现有知识结合，形成知识体系。此外，还可以把它作为分布式系统的知识图谱，根据其中列出的知识点一一搜索，你能学会所有的东西。</p>
<h2 id="Distributed-Systems-for-fun-and-profit"><a href="#Distributed-Systems-for-fun-and-profit" class="headerlink" title="Distributed Systems for fun and profit"></a><a href="http://book.mixu.net/distsys/single-page.html" target="_blank" rel="noopener">Distributed Systems for fun and profit</a></h2><p>这是一本免费的电子书。作者撰写此书的目的是希望以一种更易于理解的方式，讲述以亚马逊的 Dynamo、谷歌的 BigTable 和 MapReduce 等为代表的分布式系统背后的核心思想。</p>
<p>因而，书中着力撰写分布式系统中的关键概念，以便让读者能够快速了解最为核心的知识，并且进行了足够详实的讲述，方便读者体会和理解，又不至于陷入细节。</p>
<p>全书分为五章，讲述了扩展性、可用性、性能和容错等基础知识，FLP 不可能性和 CAP 定理，探讨了大量的一致性模型；讨论了时间和顺序，及时钟的各种用法。随后，探讨了复制问题，如何防止差异，以及如何接受差异。此外，每章末尾都给出了针对本章内容的扩展阅读资源列表，这些资料是对本书内容的很好补充。</p>
<h2 id="Distributed-Systems-Principles-and-Paradigms-http-barbie-uta-edu-jli-Resources-MapReduce-amp-Hadoop-Distributed-Systems-Principles-and-Paradigms-pdf"><a href="#Distributed-Systems-Principles-and-Paradigms-http-barbie-uta-edu-jli-Resources-MapReduce-amp-Hadoop-Distributed-Systems-Principles-and-Paradigms-pdf" class="headerlink" title="[Distributed Systems: Principles and Paradigms](http://barbie.uta.edu/~jli/Resources/MapReduce&amp;Hadoop/Distributed Systems Principles and Paradigms.pdf)"></a>[Distributed Systems: Principles and Paradigms](<a href="http://barbie.uta.edu/~jli/Resources/MapReduce&amp;Hadoop/Distributed" target="_blank" rel="noopener">http://barbie.uta.edu/~jli/Resources/MapReduce&amp;Hadoop/Distributed</a> Systems Principles and Paradigms.pdf)</h2><p>本书是由计算机科学家安德鲁·斯图尔特·塔能鲍姆（Andrew S. Tanenbaum）和其同事马丁·范·斯蒂恩（Martin van Steen）合力撰写的，是分布式系统方面的经典教材。</p>
<p>语言简洁，内容通俗易懂，介绍了分布式系统的七大核心原理，并给出了大量的例子；系统讲述了分布式系统的概念和技术，包括通信、进程、命名、同步化、一致性和复制、容错以及安全等；讨论了分布式应用的开发方法（即范型）。</p>
<p>但本书不是一本指导“如何做”的手册，仅适合系统性地学习基础知识，了解编写分布式系统的基本原则和逻辑。中文翻译版为<a href="https://item.jd.com/10079452.html" target="_blank" rel="noopener">《分布式系统原理与范型》（第二版）</a>。</p>
<h2 id="Scalable-Web-Architecture-and-Distributed-Systems"><a href="#Scalable-Web-Architecture-and-Distributed-Systems" class="headerlink" title="Scalable Web Architecture and Distributed Systems"></a><a href="http://www.aosabook.org/en/distsys.html" target="_blank" rel="noopener">Scalable Web Architecture and Distributed Systems</a></h2><p>这是一本免费的在线小册子，其中文翻译版为<a href="http://nettee.github.io/posts/2016/Scalable-Web-Architecture-and-Distributed-Systems/" target="_blank" rel="noopener">可扩展的 Web 架构和分布式系统</a>。</p>
<p>本书主要针对面向的互联网（公网）的分布式系统，但其中的原理或许也可以应用于其他分布式系统的设计中。作者的观点是，通过了解大型网站的分布式架构原理，小型网站的构建也能从中受益。本书从大型互联网系统的常见特性，如高可用、高性能、高可靠、易管理等出发，引出了一个类似于 Flickr 的典型的大型图片网站的例子。</p>
<p>首先，从程序模块化易组合的角度出发，引出了面向服务架构（SOA）的概念。同时，引申出写入和读取两者的性能问题，及对此二者如何调度的考量——在当今的软硬件架构上，写入几乎总是比读取更慢，包括软件层面引起的写入慢（如数据库的一致性要求和 B 树的修改）和硬件层面引起的写入慢（如 SSD）。</p>
<p>网络提供商提供的下载带宽也通常比上传带宽更大。读取往往可以异步操作，还可以做 gzip 压缩。写入则往往需要保持连接直到数据上传完成。因此，往往我们会想把服务做成读写分离的形式。然后通过一个 Flickr 的例子，介绍了他们的服务器分片式集群做法。</p>
<p>接下来讲了冗余。数据的冗余异地备份（如 master-slave）、服务的多版本冗余、避免单点故障等。</p>
<p>随后，在冗余的基础上，讲了多分区扩容，亦即横向扩容。横向扩容是在单机容量无法满足需求的情况下不得不做的设计。但横向扩容会带来一个问题，即数据的局域性会变差。本来数据可以存在于同一台服务器上，但现在数据不得不存在于不同服务器上，潜在地降低了系统的性能（主要是可能延长响应时间）。另一个问题是多份数据的不一致性。</p>
<p>之后，本书开始深入讲解数据访问层面的设计。首先抛出一个大型数据（TB 级以上）的存储问题。如果内存都无法缓存该数据量，性能将大幅下降，那么就需要缓存数据。数据可以缓存在每个节点上。</p>
<p>但如果为所有节点使用负载均衡，那么分配到每个节点的请求将十分随机，大大降低缓存命中率，从而导致低效的缓存。接下来考虑全局缓存的设计。再接下来考虑分布式缓存的设计。进一步，介绍了 Memcached，以及 Facebook 的缓存设计方案。</p>
<p>代理服务器则可以用于把多个重复请求合并成一个，对于公网上的公共服务来说，这样做可以大大减少对数据层访问的次数。Squid 和 Varnish 是两个可用于生产的代理服务软件。</p>
<p>当知道所需要读取的数据的元信息时，比如知道一张图片的 URL，或者知道一个要全文搜索的单词时，索引就可以帮助找到那几台存有该信息的服务器，并从它们那里获取数据。文中扩展性地讨论了本话题。</p>
<p>接下来谈负载均衡器，以及一些典型的负载均衡拓扑。然后讨论了对于用户会话数据如何处理。比如，对于电子商务网站，用户的购物车在没有下单之前都必须保持有效。</p>
<p>一种办法是让用户会话与服务器产生关联，但这样做会较难实现自动故障转移，如何做好是个问题。另外，何时该使用负载均衡是个问题。有时节点数量少的情况下，只要使用轮换式 DNS 即可。负载均衡也会让在线性能问题的检测变得更麻烦。</p>
<p>对于写入的负载，可以用队列的方式来减少对服务器的压力，保证服务器的效率。消息队列的开源实现有很多，如 RabbitMQ、ActiveMQ、BeanstalkD，但有些队列方案也使用了如 Zookeeper，甚至是像 Redis 这样的存储服务。</p>
<p>本书主要讲述了高性能互联网分布式服务的架构方案，并介绍了许多实用的工具。作者指出这是一个令人兴奋的设计领域，虽然只讲了一些皮毛，但这一领域不仅现在有很多创新，将来也会越来越多。</p>
<h2 id="Principles-of-Distributed-Systems"><a href="#Principles-of-Distributed-Systems" class="headerlink" title="Principles of Distributed Systems"></a><a href="http://dcg.ethz.ch/lectures/podc_allstars/lecture/podc.pdf" target="_blank" rel="noopener">Principles of Distributed Systems</a></h2><p>本书是苏黎世联邦理工学院的教材。它讲述了多种分布式系统中会用到的算法。虽然分布式系统的不同场景会用到不同算法，但并不表示这些算法都会被用到。不过，对于学生来说，掌握了算法设计的精髓也就能举一反三地设计出解决其他问题的算法，从而得到分布式系统架构设计中所需的算法。</p>
<p>本书覆盖的算法有：</p>
<ul>
<li>顶点涂色算法（可用于解决互相冲突的任务分配问题）</li>
<li>分布式的树算法（广播算法、会聚算法、广度优先搜索树算法、最小生成树算法）</li>
<li>容错以及 Paxos（Paxos 是最经典的共识算法之一）</li>
<li>拜占庭协议（节点可能没有完全宕机，而是输出错误的信息）</li>
<li>全互联网络（服务器两两互联的情况下算法的复杂度）</li>
<li>多核计算的工程实践（事务性存储、资源争用管理）</li>
<li>主导集（又一个用随机化算法打破对称性的例子；这些算法可以用于路由器建立路由）</li>
<li>……</li>
</ul>
<p>这些算法对你迈向更高级更广阔的技术领域真的相当有帮助的。</p>
<h2 id="Making-reliable-distributed-systems-in-the-presence-of-software-errors"><a href="#Making-reliable-distributed-systems-in-the-presence-of-software-errors" class="headerlink" title="Making reliable distributed systems in the presence of software errors"></a><a href="https://github.com/theanalyst/awesome-distributed-systems/blob/master/README.md" target="_blank" rel="noopener">Making reliable distributed systems in the presence of software errors</a></h2><p>这本书的书名直译过来是在有软件错误的情况下，构建可靠的分布式系统，Erlang 之父乔·阿姆斯特朗（Joe Armstrong）的力作。书中撰写的内容是从 1981 年开始的一个研究项目的成果，这个项目是寻找更好的电信应用编程方式。</p>
<p>当时的电信应用都是大型程序，虽然经过了仔细的测试，但投入使用时程序中仍会存在大量的错误。作者及其同事假设这些程序中确实有错误，然后想法设法在这些错误存在的情况下构建可靠的系统。他们测试了所有的编程语言，没有一门语言拥有电信行业所需要的所有特性，所以促使一门全新的编程语言 Erlang 的开发，以及随之出现的构建健壮系统（OTP）的设计方法论和库集。</p>
<p>书中抽象了电信应用的所有需求，定义了问题域，讲述了系统构建思路——模拟现实，简单通用，并给出了指导规范。阿姆斯特朗认为，在存在软件错误的情况下，构建可靠系统的核心问题可以通过编程语言或者编程语言的标准库来解决。所以本书有很大的篇幅来介绍 Erlang，以及如何运用其构建具有容错能力的电信应用。</p>
<p>虽然书中的内容是以构建 20 世纪 80 年代的电信系统为背景，但是这种大规模分布式的系统开发思路，以及对系统容错能力的核心需求，与互联网时代的分布式系统架构思路出奇一致。书中对问题的抽象、总结，以及解决问题的思路和方案，有深刻的洞察和清晰的阐释，所以此书对现在的项目开发和架构有极强的指导和借鉴意义。</p>
<h2 id="Designing-Data-Intensive-Applications"><a href="#Designing-Data-Intensive-Applications" class="headerlink" title="Designing Data Intensive Applications"></a><a href="https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321" target="_blank" rel="noopener">Designing Data Intensive Applications</a></h2><p>这是一本非常好的书。我们知道，在分布式的世界里，数据结点的扩展是一件非常麻烦的事。而这本书则深入浅出地用很多工程案例讲解了如何让数据结点做扩展。</p>
<p>作者马丁·科勒普曼（Martin Kleppmann）在分布式数据系统领域有着很深的功底，并在这本书中完整地梳理各类纷繁复杂设计背后的技术逻辑，不同架构之间的妥协与超越，很值得开发人员与架构设计者阅读。</p>
<p>这本书深入到 B-Tree、SSTables、LSM 这类数据存储结构中，并且从外部的视角来审视这些数据结构对 NoSQL 和关系型数据库所产生的影响。它可以让你很清楚地了解到真正世界的大数据架构中的数据分区、数据复制的一些坑，并提供了很好的解决方案。</p>
<p><strong>最赞的是，作者将各种各样的技术的本质非常好地关联在一起，帮你触类旁通</strong>。而且抽丝剥茧，循循善诱，从“提出问题”，到“解决问题”，到“解决方案”，再到“优化方案”和“对比不同的方案”，一点一点地把非常晦涩的技术和知识展开。</p>
<p>本书的引用相当多，每章后面都有几百个 Reference。通过这些 Reference，你可以看到更为广阔更为精彩的世界。</p>
<p>这本书是 2017 年 3 月份出版的，目前还没有中译版，不过英文也不难读。非常推荐。这里有<a href="http://www.antonfagerberg.com/files/intensive.pdf" target="_blank" rel="noopener">这本书的 PPT</a>，你可从这个 PPT 中管中窥豹一下。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>在今天的文章中，给出了一些分布式系统的基础理论知识和几本很不错的图书和资料，需要慢慢消化吸收。也许你看到这么庞大的书单和资料列表有点望而却步，但是我真的希望你能够花点时间来看看这些资料。相信你看完这些资料后，一定能上一个新的台阶。再加上一些在工程项目中的实践，我保证你，一定能达到大多数人难以企及的技术境界。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/09/Learn-to-Code-by-Competitive-Programming/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/09/Learn-to-Code-by-Competitive-Programming/" class="post-title-link" itemprop="url">Learn_to_Code_by_Competitive_Programming</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-09 22:20:51" itemprop="dateCreated datePublished" datetime="2020-02-09T22:20:51-05:00">2020-02-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-10 00:24:04" itemprop="dateModified" datetime="2020-02-10T00:24:04-05:00">2020-02-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>How do I Learn to Code? This is probably the most nagging question at the back of your mind, once you have decided that you want to learn programming. Like learning anything else, there is no standard process for learning to code. Of course there are guidelines, there are courses, there are ideologies and there are set traditions, but there is no one single correct way.</p>
<p>One school of thought which is very popular and fairly simple to begin with is <a href="http://en.wikipedia.org/wiki/Competitive_programming" target="_blank" rel="noopener">Competitive Programming</a>. Getting started with it is quite easy and if one devotes sufficient amount of time and effort, you can develop a very strong grasp of programming logic in relatively short amount of time.</p>
<hr>
<p>Here are some steps to get started and be good at it.</p>
<ul>
<li>Get comfortable writing code in either of one of these languages <strong>C, C++ or Java</strong>. Why only C, C++ or Java? Because these are the standard languages allowed in any programming competition.</li>
<li>If you are already good at C, it is suggested to <strong>learn C++</strong>. It is the most popular language among competitive programmers because of its speed and an excellent library in the form of STL (Standard Template Library).</li>
<li>Pick an online judge. Recommended ones are <a href="http://community.topcoder.com/tc" target="_blank" rel="noopener"><strong>Topcoder</strong></a> and <a href="http://codeforces.com/" target="_blank" rel="noopener"><strong>Codeforces</strong></a>. These sites have high quality of problems and also allow you to see other’s code post contest completion. These also categorize problems based on the topic. Some other popular judges include <a href="http://www.spoj.com/" target="_blank" rel="noopener">SPOJ</a>, <a href="http://codechef.com/" target="_blank" rel="noopener">CodeChef</a> (powered by SPOJ) and <a href="http://www.hackerearth.com/" target="_blank" rel="noopener">HackerEarth</a>.</li>
<li>To begin with, <strong>start with simple problems</strong> that typically require transforming English to code and does not require any knowledge on algorithms. Solving <a href="http://community.topcoder.com/tc?module=ProblemArchive&sr=&er=&sc=&sd=&class=&cat=&div1l=&div2l=1&mind1s=&mind2s=&maxd1s=&maxd2s=&wr=" target="_blank" rel="noopener">Div 2 250</a> (Division 2, 250 points) in Topcoder or Div 2 Problem A in Codeforces is a good start.</li>
<li>At the early stages of programming one tends to write long pieces of code, which is actually not required. Try to keep codes <strong>short and simple</strong>.</li>
<li><strong>Practice</strong> these problems until you become comfortable that you can submit it for 240 odd points on any day.</li>
<li>Start implementing basic(or standard) algorithms. It is suggested to read them from <a href="http://community.topcoder.com/tc?module=Static&d1=tutorials&d2=alg_index" target="_blank" rel="noopener">Topcoder tutorials</a> or <a href="http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844" target="_blank" rel="noopener">Introduction to algorithms</a>.</li>
</ul>
<hr>
<p>Some basic concepts that you should learn are</p>
<ol>
<li>Graph algorithms: Breadth first search(BFS), Depth first search(DFS), Strongly connected components(SCC), Dijkstra, Floyd-Warshall, Minimum spanning tree(MST), Topological sort.</li>
<li>Dynamic programming: Standard dynamic programming problems such as Rod Cutting, Knapsack, Matrix chain multiplication etc.</li>
<li>Number theory: Modular arithmetic, Fermat’s theorem, Chinese remainder theorem(CRT), Euclidian method for GCD, Logarithmic Exponentiation, Sieve of Eratosthenes, Euler’s totient function.</li>
<li>Greedy: Standard problems such as Activity selection.</li>
<li>Search techniques: Binary search, Ternary search and Meet in the middle.</li>
<li>Data structures (Basic): Stacks, Queues, Trees and Heaps.</li>
<li>Data structures (Advanced): Trie, Segment trees, Fenwick tree or Binary indexed tree(BIT), Disjoint data structures.</li>
<li>Strings: Knuth Morris Pratt(KMP), Z algorithm, Suffix arrays/Suffix trees. These are bit advanced algorithms.</li>
<li>Computational geometry: Graham-Scan for convex hull, Line sweep.</li>
<li>Game theory: Basic principles of Nim game, Grundy numbers, Sprague-Grundy theorem.</li>
</ol>
<p>The list is not complete but these are the ones that you encounter very frequently in the contests. There are other algorithms but are required very rarely in the contests.</p>
<p>You can find description and implementation of standard algorithms <a href="http://e-maxx.ru/algo/" target="_blank" rel="noopener">here</a>.</p>
<ul>
<li>Once you have sufficient knowledge of popular algorithms, you can start solving the medium level problems. That is Div 2 all problems in Topcoder and Codeforces. It is advisable not to go for Div 1 500 at this point.</li>
<li>Learning to code is all about practicing. <strong>Participate regularly</strong> in the programming contests. Solve the ones that you cannot solve in the contest, after the contest. Apart from Topcoder and Codeforces you can also look at <a href="http://www.hackerearth.com/challenges/" target="_blank" rel="noopener">HackerEarth Challenges</a> or <a href="http://www.codechef.com/contests" target="_blank" rel="noopener">Codechef contests</a>.</li>
<li><strong>Read the codes</strong> of high rated programmers. Compare your solution with them. You can observe that it is simple and shorter than your solution. Analyse how they have approached and improve your implementation skills.</li>
<li><strong>Read the editorials</strong> after the contest. You can learn how to solve the problems that you were not able to solve in the contest and learn alternative ways to solve the problems which you could solve.</li>
<li>Always <strong>practice the problems that you could solve in the contest</strong>. Suppose if you are able to solve Div 2 250 and 500 in the contest but not Div 2 1000 then practice as many Div 2 1000 problems as as you can.</li>
<li><strong>Do not spend too much time</strong> if you are not getting the solution or are stuck somewhere.</li>
<li>After you feel that you have spent enough time, look at the editorials. Understand the algorithm and code it. Do not look at the actual solution before you have attempted to write the code on your own.</li>
<li>Programming is a very practical and hands on skill. You have to continuously do it to be good at it. It’s not enough to solve the problem theoretically, <strong>you have to code it and get the solution accepted</strong>. Knowing which algorithm/logic to use and implementing it are two different things. It takes both to be good at programming.</li>
<li>Programming learning phase is going to take a lot of time and the key is <strong>practicing regularly</strong>. It takes some time before you can attempt Div 1 500 and other tough problems. Do not give up on reading the editorials and implementing them, even if it takes many hours/days. Remember everything requires practice to master it.</li>
</ul>
<p>It takes considerable amount of time before you get good at it. You have to keep yourself motivated throughout. Forming a team and practicing is a good choice. <strong>Not giving up is the key here</strong>.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/09/%E8%AF%86%E5%88%AB%E8%A1%A8%E8%B1%A1%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9A%84%E6%96%B9%E6%B3%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/09/%E8%AF%86%E5%88%AB%E8%A1%A8%E8%B1%A1%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9A%84%E6%96%B9%E6%B3%95/" class="post-title-link" itemprop="url">识别表象和本质的方法</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-09 10:32:13 / Modified: 11:42:41" itemprop="dateCreated datePublished" datetime="2020-02-09T10:32:13-05:00">2020-02-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="关于兴趣和投入"><a href="#关于兴趣和投入" class="headerlink" title="关于兴趣和投入"></a>关于兴趣和投入</h1><p>兴趣是学习的助燃剂。对一件事有兴趣是是否愿意对这件事投入更多的前提条件。因此，找到自己的兴趣点的确是非常关键的。不过，我们也能看到下面几点。</p>
<ul>
<li><p><strong>一方面，兴趣是需要保持的</strong>。有的人有的事就是三分钟的兴趣。刚开始兴趣十足，然而时间一长，兴趣因为各种原因不能保持，就会很快地“移情别恋”了。所以，不能持久的兴趣，或是一时兴起的兴趣，都无法让人投入下去。</p>
</li>
<li><p><strong>另一方面，兴趣其实也是可以培养出来的</strong>。有些人对计算机软件毫无兴趣，反而对物理世界里的很多东西非常有兴趣，比如无线电、原子能，或是飞行器之类的。但阴差阳错，最终考了个计算机软件专业，然后发现，自己越来越有兴趣，于是就到了今天。</p>
</li>
</ul>
<p>一个可以持久的兴趣，或是可以培养出来的兴趣，后面都有一个比较本质的东西，那就是你在做这个件事时的一种正反馈，其实就是成就感。也就是说，<strong>兴趣只是开始，而能让人不断投入时间和精力的则是正反馈，是成就感</strong>。</p>
<p>带娃的父母可能对此比较好理解。比如，小孩 3 岁的时候，买了一桶积木给她。她一开始只喜欢把积木胡乱堆，没玩一会就对这种抽象的玩具失去了兴趣，去玩别的更形象的玩具去了。于是，我就搭了一个小城堡给她看，她看完后兴趣就来了，也想自己搭一个。但是，不一会儿，她就受挫了，因为没有掌握好物体在构建时的平衡和支点的方法，所以搭出来的东西会倒。</p>
<p>有时倒了之后，她会从中有一点点的学习总结，但更多的时候总结不出来。于是，就上前帮她做调整，她很快就学会了，并且每一次都比上一次搭得更好……如此反复，最终，小孩玩积木上花的时间大大超过了其它的玩具，直到她无法从中得到成就感。</p>
<p>很显然，把孩子从“天性喜欢破坏的兴趣点”上拉到了“喜欢创造的兴趣点”上。因为创造能带来更多的成就感，不是吗？</p>
<p>所以，你对一件事的兴趣只是一种表象，而内在更多的是你做这件事的成就感是否可以持续。<strong>你需要找到让自己能够更有成就感的事情，兴趣总是可以培养的</strong>。</p>
<h1 id="关于学习和工作"><a href="#关于学习和工作" class="headerlink" title="关于学习和工作"></a>关于学习和工作</h1><p>学习一门语言或者一项技术是否只有找到了相应的工作才学得好。</p>
<p>学好一项技术和是否找到与之相匹配的工作有关联，但它们之间并不是强关联的，因为我们每个人的成长和学习有很多时候是在还没有参加工作的时候。但之所以，我们都觉得通过工作才让我们学习和成长得更快，主要有这些原因。</p>
<ul>
<li>工作中能为我们带来相应的场景和实际的问题，而不是空泛的学习。带着问题去学习，带着场景去解决问题，的确是一种很高效的学习方式。</li>
<li>在工作当中，有同事和高手帮助。和他们的交互和讨论，可以让你更快地学习和成长。</li>
</ul>
<p><strong>本质上来说，并不是只有找到了相应的工作我们才可以学好一项技术，而是，我们在通过解决实际问题，在和他人讨论，获得高手帮助的环境下，能更快更有效率地学习和成长。</strong></p>
<p>有时候，在工作中你反而学不到东西，那是因为你找的这个工作能够提供的场景不够丰富，需要解决的实际问题太过简单，以及你的同事对你的帮助不大。这时，这个工作反而限制了你的学习和成长。</p>
<p>所以，两点。</p>
<ul>
<li>找工作不只是找用这个技术的工作，更是要找场景，找实际问题，找团队。这些才是本质。一项技术很多公司都在用，然而，只有进入到有更多的场景、有挑战性的问题、有靠谱团队的公司，才对学习和成长更有帮助。</li>
<li>不要完全把自己的学习寄希望于找一份工作，才会学得好。在一些开源社区内，有助于学习的场景会更多，要解决的实际问题也更多，同时你能接触到的牛人也更多。特别是一些有大量公司和几万、几十万甚至上百万的开发人员在贡献代码的项目，可以让人成长很快。</li>
</ul>
<p><strong>总之，找到学习的方法，提升自己对新事物学习的能力，才是真正靠谱的。</strong></p>
<h1 id="关于技术和价值"><a href="#关于技术和价值" class="headerlink" title="关于技术和价值"></a>关于技术和价值</h1><p>后面，我们聊到了什么样的技术会是属于未来的技术，以及应该把时间花在什么样的技术上。一个问题：“你觉得，让人登月探索宇宙的技术价值大，还是造高铁的技术价值大？或者是科学种田的技术价值大？……”</p>
<p>是的，对于这个问题，从不同的角度上看，就会得到不同的结论。似乎，我们无法说明白哪项技术创造的价值更大，因为完全没法比较。</p>
<p>于是我又说了一个例子，在第一次工业革命的时候，也就是蒸汽机时代，除了蒸汽机之外还有其它一些技术含量更高的技术，比如化学、冶金、水泥、玻璃……但是，这么一个不起眼的技术引发了人类社会的变革。也许，那个时候，在技术圈中，很多技术专家还鄙视蒸汽机的技术含量太低呢。</p>
<p>我并不是想说高大上的技术无用，我想说的是，技术无贵贱，很多伟大的事就是通过一些不起眼的技术造就的。所以，我们应该关注的是：</p>
<ul>
<li>要用技术解决什么样的问题，场景非常重要；</li>
<li>如何降低技术的学习成本，提高易用性，从而可以让技术更为普及。</li>
</ul>
<p>另外。假设，我们今天没有电，忽然，有人说他发明了电。这个世界上的很多人都会觉得“电”这个东西没什么用，而只有等到“电灯”的发明，人们才明白发明“电”是多么牛。</p>
<p>所以，对于一些“基础技术”来说，通常会在某段时间内被人类社会低估。就像国内前几年低估“云计算”技术一样。基础技术就像是创新的引擎，其不断地成熟和完善会导致更上层的技术不断地衍生，越滚越大。</p>
<p>而在一个基础技术被广泛应用的过程中，如何规模化也会成为一个关键技术。这就好像发电厂一样，没有发电厂，电力就无法做到规模化。记得汽车发明的时候，要组装一个汽车的时间成本、人力成本、物力成本都非常高，所以完全无法做到规模化，而通过模块化分工、自动化生产等技术手段才释放了产能，从而普及。</p>
<p>所以，一项有价值的技术，并不在于这项技术是否有技术含量，而是在于：</p>
<ul>
<li>能否低成本高效率地解决实际问题；</li>
<li>是不是众多产品的基础技术；</li>
<li>是不是可以支持规模化的技术。</li>
</ul>
<p>对于搞计算机软件的人来说，也可以找到相对应的技术点。比如：</p>
<ul>
<li>低成本高效率地解决实际问题的技术，一定是自动化的技术。软件天生就是用来完成重复劳动的，天生就是用来做自动化的。而未来的 AI 和 IoT 也是在拼命数字化和自动化还没有自动化的领域。</li>
<li>基础技术总是枯燥和有价值的。数学、算法、网络、存储等基础技术吃得越透，就越容易服务上层的各种衍生技术或产品。</li>
<li>支持规模化的技术也是很有价值的。在软件行业中，也就是 PaaS 的相关技术。</li>
</ul>
<p>当然，我的意思并不是别的技术都没有价值了。重申一下，<strong>技术无贵贱。我只是想说，能规模化低成本高效率解决实际问题的技术及其基础技术，就算是很 low，也是很有价值的。</strong></p>
<h1 id="关于趋势和未来"><a href="#关于趋势和未来" class="headerlink" title="关于趋势和未来"></a>关于趋势和未来</h1><p>似乎有些规律也是有迹可寻的。</p>
<p><strong>这个世界的技术趋势和未来其实是被人控制的</strong>。就是被那些有权有势有钱的公司或国家来控制的。当然，他们控制的不是长期的未来，但短期的未来（3-5 年）一定是他们控制着的。</p>
<p>也就是说，技术的未来要去哪，主要是看这个世界的投入会到哪。基本上就是这个世界上的有钱有势的人把财富投到哪个领域，也就是这个世界的大公司或大国们的规划。一旦他们把大量的金钱投到某个领域，这个领域就会得到发展，那么发展之后，这个领域也就成为未来了。只要是有一堆公司在往一个方向上不间断地投资或者花钱，这个方向不想成为未来似乎都不可能。</p>
<p>听上去多少有点儿令人沮丧，但世界就是如此简单粗暴运作着的。</p>
<p>所以，对于在这个世界里排不上号的人来说，只能默默地跟随着这些大公司所引领的趋势和未来。对一些缺钱缺人的创业公司，唯一能够做的，也许只是两条路，一是用更为低的成本来提供和大公司相应的技术，另一条路是在细分垂直市场上做得比大公司更专更精。等着自己有一天长大后，也能加入第一梯队从而“引领”未来。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>在我们的生活和工作中，总是会有很多人混淆一些看似有联系，实则却关系不大的词和概念，分辨不清事物的表象和本质。</p>
<p>兴趣和投入。表面上，兴趣是决定一件事儿能否做持久的关键因素。而反观我们自己和他人的经历不难发现，兴趣充演的角色通常是敲门砖，它引发我们关注到某事某物。而真正能让我们坚持下去的，实际上是做一件事之后从中收获到的正反馈，也就是成就感。</p>
<p>同样，人们也经常搞错学习和工作之间的关系。多数人都会认为，在工作中学习和成长速度快。而细细观察下来，就会发现，工作不过是提供了一个能够解决实际问题，能跟人讨论，有高手帮助的环境。所以说，让我们成长的并不是工作本身，而是有利于学习的环境。也就是说，如果我们想学习，除了可以选择有助于学习的工作机会，开源社区提供的环境同样有助于我们的学习和提高，那里高手更多，实际问题不少。</p>
<p>还有，技术和价值。人们通常认为技术含量高的技术其价值会更高，而历史上无数的事实却告诉我们，能规模化、低成本、高效率地解决实际问题的技术及其基础技术，才发挥出了更为深远的影响，甚至其价值更是颠覆性的，难以估量。</p>
<p>趋势和未来也是被误解得很深的一对“孪生兄弟”。虽然大家通常会认为有什么样的技术趋势，必然带来什么样的未来。殊不知，所谓的趋势和未来，其实都是可以由人为控制的，特别是哪些有钱有势的人和公司。也就是，社会的资金和资源流向什么领域，这个领域势必会得到成长和发展，会逐渐形成趋势，进而成为未来。我们遵循这样的规律，就能很容易地判断出未来的，最起码是近几年的，技术流向了。</p>
<p>再如，加班和产出，努力和成功，速度和效率……加班等于高产出吗？显然不是。很努力就一定会成功吗？当然不是。速度快就是效率高吗？更加不是。可以枚举的还有很多，如干得多就等于干得好吗？等等。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/08/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%85%A5%E9%97%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/08/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%85%A5%E9%97%A8/" class="post-title-link" itemprop="url">分布式架构入门</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-08 14:56:10 / Modified: 15:58:49" itemprop="dateCreated datePublished" datetime="2020-02-08T14:56:10-05:00">2020-02-08</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>分布式系统涵盖的面非常广，如下：</p>
<ul>
<li><strong>服务调度</strong>，涉及服务发现、配置管理、弹性伸缩、故障恢复等。</li>
<li><strong>资源调度</strong>，涉及对底层资源的调度使用，如计算资源、网络资源和存储资源等。</li>
<li><strong>流量调度</strong>，涉及路由、负载均衡、流控、熔断等。</li>
<li><strong>数据调度</strong>，涉及数据复本、数据一致性、分布式事务、分库、分表等。</li>
<li><strong>容错处理</strong>，涉及隔离、幂等、重试、业务补偿、异步、降级等。</li>
<li><strong>自动化运维</strong>，涉及持续集成、持续部署、全栈监控、调用链跟踪等。</li>
</ul>
<p>所有这些形成了分布式架构的整体复杂度，也造就了分布式系统中的很多很多论文、图书以及很多很多的项目。要学好分布式系统及其架构，我们需要大量的时间和实践才能真正掌握这些技术。</p>
<p>这里有几点需要你注意一下。</p>
<ul>
<li><strong>分布式系统之所以复杂，就是因为其太容易也太经常出错了</strong>。这意味着，<strong>你要把处理错误的代码当成正常功能的代码来处理</strong>。</li>
<li><strong>开发一个健壮的分布式系统的成本是单体系统的几百倍甚至几万倍</strong>。这意味着，<strong>我们要自己开发一个，需要能力很强的开发人员</strong>。</li>
<li><strong>非常健壮的开源的分布式系统并不多，或者说基本没有</strong>。这意味着，<strong>如果你要用开源的，那么你需要 hold 得住其源码</strong>。</li>
<li><strong>管理或是协调多个服务或机器是非常难的</strong>。这意味着，<strong>我们要去读很多很多的分布式系统的论文</strong>。</li>
<li><strong>在分布式环境下，出了问题是很难 debug 的</strong>。这意味着，<strong>我们需要非常好的监控和跟踪系统，还需要经常做演练和测试</strong>。</li>
<li><strong>在分布式环境下，你需要更科学地分析和统计</strong>。这意味着，<strong>我们要用 P90 这样的统计指标，而不是平均值，我们还需要做容量计划和评估</strong>。</li>
<li><strong>在分布式环境下，需要应用服务化</strong>。这意味着，<strong>我们需要一个服务开发框架，比如 SOA 或微服务</strong>。</li>
<li><strong>在分布式环境下，故障不可怕，可怕的是影响面过大，时间过长</strong>。这意味着，<strong>我们需要花时间来开发我们的自动化运维平台</strong>。</li>
</ul>
<p>总之，在分布式环境下，一切都变得非常复杂。要进入这个领域，你需要有足够多的耐性和足够强的心态来接受各式各样的失败。当拥有丰富的实践和经验后，你才会有所建树。这并不是一日之功，你可能要在这个领域花费数年甚至数十年的时间。</p>
<h2 id="分布式架构入门"><a href="#分布式架构入门" class="headerlink" title="分布式架构入门"></a>分布式架构入门</h2><p>学习如何设计可扩展的架构将会有助于你成为一个更好的工程师。系统设计是一个很宽泛的话题。在互联网上，关于架构设计原则的资源也是多如牛毛。所以，你需要知道一些基本概念，对此，这里你先读一下下面两篇文章，都非常不错。</p>
<ul>
<li><a href="http://www.aosabook.org/en/distsys.html" target="_blank" rel="noopener">Scalable Web Architecture and Distributed Systems</a> ，这篇文章会给你一个大概的分布式架构是怎么来解决系统扩展性问题的粗略方法。</li>
<li><a href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns" target="_blank" rel="noopener">Scalability, Availability &amp; Stability Patterns</a> ，这个 PPT 能在扩展性、可用性、稳定性等方面给你一个非常大的架构设计视野和思想，可以让你感受一下大概的全景图。</li>
</ul>
<p>然后，强烈推荐 GitHub 上的一篇文档 - <a href="https://github.com/donnemartin/system-design-primer" target="_blank" rel="noopener">System Design Primer</a> ，这个仓库主要组织收集分布式系统的一些与扩展性相关的资源，它可以帮助你学习如何构建可扩展的架构。</p>
<p>目前这个仓库收集到了好些系统架构和设计的基本方法。其中包括：CAP 理论、一致性模型、可用性模式、DNS、CDN、负载均衡、反向代理、应用层的微服务和服务发现、关系型数据库和 NoSQL、缓存、异步通讯、安全等。</p>
<p>上面这几篇文章基本足够可以让你入门了，因为其中基本涵盖了所有与系统架构相关的技术。这些技术，足够这世上 90% 以上的公司用了，只有超级巨型的公司才有可能使用更高层次的技术。</p>
<h2 id="分布式理论"><a href="#分布式理论" class="headerlink" title="分布式理论"></a>分布式理论</h2><p>下面学习一下分布式方面的理论知识。</p>
<p>首先，你需要看一下 <a href="https://github.com/aphyr/distsys-class" target="_blank" rel="noopener">An introduction to distributed systems</a>。 这只是某个教学课程的提纲，几乎涵盖了分布式系统方面的所有知识点，而且辅以简洁并切中要害的说明文字，非常适合初学者提纲挈领地了解知识全貌，快速与现有知识结合，形成知识体系。这也是一个分布式系统的知识图谱，可以让你看到分布式系统的整体全貌。你可以根据这个知识图 Google 下去，然后你会学会所有的东西。</p>
<p>然后，你需要了解一下拜占庭将军问题（<a href="https://en.wikipedia.org/wiki/Byzantine_fault_tolerance" target="_blank" rel="noopener">Byzantine Generals Problem</a>）。这个问题是莱斯利·兰波特（Leslie Lamport）于 1982 年提出用来解释一致性问题的一个虚构模型（<a href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/The-Byzantine-Generals-Problem.pdf" target="_blank" rel="noopener">论文地址</a>）。拜占庭是古代东罗马帝国的首都，由于地域宽广，守卫边境的多个将军（系统中的多个节点）需要通过信使来传递消息，达成某些一致的决定。但由于将军中可能存在叛徒（系统中节点出错），这些叛徒将努力向不同的将军发送不同的消息，试图会干扰一致性的达成。拜占庭问题即为在此情况下，如何让忠诚的将军们能达成行动的一致。</p>
<p>对于拜占庭问题来说，假如节点总数为 <code>N</code>，叛变将军数为 <code>F</code>，则当 <code>N &gt;= 3F + 1</code> 时，问题才有解，即拜占庭容错（Byzantine Fault Tolerant，BFT）算法。拜占庭容错算法解决的是，网络通信可靠但节点可能故障情况下一致性该如何达成的问题。</p>
<p>最早由卡斯特罗（Castro）和利斯科夫（Liskov）在 1999 年提出的实用拜占庭容错（Practical Byzantine Fault Tolerant，PBFT）算法，是第一个得到广泛应用的 BFT 算法。只要系统中有 2/3 的节点是正常工作的，则可以保证一致性。PBFT 算法包括三个阶段来达成共识：预准备（Pre-Prepare）、准备（Prepare）和提交（Commit）。</p>
<p>这里有几篇和这个问题相关的文章，推荐阅读。</p>
<ul>
<li><a href="http://www.drdobbs.com/cpp/the-byzantine-generals-problem/206904396" target="_blank" rel="noopener">Dr.Dobb’s - The Byzantine Generals Problem</a></li>
<li><a href="http://blog.jameslarisch.com/the-byzantine-generals-problem" target="_blank" rel="noopener">The Byzantine Generals Problem</a></li>
<li><a href="http://pmg.csail.mit.edu/papers/osdi99.pdf" target="_blank" rel="noopener">Practicle Byzantine Fault Tolerance</a></li>
</ul>
<p>拜占庭容错系统研究中有三个重要理论：CAP、FLP 和 DLS。</p>
<ul>
<li><p><a href="https://en.wikipedia.org/wiki/CAP_theorem" target="_blank" rel="noopener">CAP 定理</a>，CAP 理论相信你应该听说过不下 N 次了。CAP 定理是分布式系统设计中最基础也是最为关键的理论。CAP 定理指出，分布式数据存储不可能同时满足以下三个条件：一致性（Consistency）、可用性（Availability）和 分区容忍（Partition tolerance）。 “在网络发生阻断（partition）时，你只能选择数据的一致性（consistency）或可用性（availability），无法两者兼得”。</p>
<p>论点比较直观：如果网络因阻断而分隔为二，在其中一边我送出一笔交易：“将我的十元给 A”；在另一半我送出另一笔交易：” 将我的十元给 B “。此时系统要不是，a）无可用性，即这两笔交易至少会有一笔交易不会被接受；要不就是，b）无一致性，一半看到的是 A 多了十元而另一半则看到 B 多了十元。要注意的是，CAP 理论和扩展性（scalability）是无关的，在分片（sharded）或非分片的系统皆适用。</p>
</li>
<li><p><a href="http://the-paper-trail.org/blog/a-brief-tour-of-flp-impossibility/" target="_blank" rel="noopener">FLP impossibility</a>- 在异步环境中，如果节点间的网络延迟没有上限，只要有一个恶意的节点存在，就没有算法能在有限的时间内达成共识。但值得注意的是， <a href="https://en.wikipedia.org/wiki/Las_Vegas_algorithm" target="_blank" rel="noopener">“Las Vegas” algorithms</a>（这个算法又叫撞大运算法，其保证结果正确，只是在运算时所用资源上进行赌博，一个简单的例子是随机快速排序，它的 pivot 是随机选的，但排序结果永远一致）在每一轮皆有一定机率达成共识，随着时间增加，机率会越趋近于 1。而这也是许多成功的共识算法会采用的解决问题的办法。</p>
</li>
<li><p>容错的上限 - 由 <a href="http://groups.csail.mit.edu/tds/papers/Lynch/jacm88.pdf" target="_blank" rel="noopener">DLS 论文</a> ，我们可以得到以下结论。</p>
<ul>
<li>在部分同步（partially synchronous）的网络环境中（即网络延迟有一定的上限，但我们无法事先知道上限是多少），协议可以容忍最多 1/3 的拜占庭故障（Byzantine fault）。</li>
<li>在异步（asynchronous）的网络环境中，具有确定性质的协议无法容忍任何错误，但这篇论文并没有提及 <a href="http://link.springer.com/chapter/10.1007%2F978-3-540-77444-0_7" target="_blank" rel="noopener">randomized algorithms</a>，在这种情况下可以容忍最多 1/3 的拜占庭故障。</li>
<li>在同步（synchronous）网络环境中（即网络延迟有上限且上限是已知的），协议可以容忍 100% 的拜占庭故障，但当超过 1/2 的节点为恶意节点时，会有一些限制条件。要注意的是，我们考虑的是 “ 具有认证特性的拜占庭模型（authenticated Byzantine）”，而不是 “ 一般的拜占庭模型 “；具有认证特性指的是将如今已经过大量研究且成本低廉的公私钥加密机制应用在我们的算法中。</li>
</ul>
</li>
</ul>
<p>当然，还有一个著名的“8 条荒谬的分布式假设（<a href="http://en.wikipedia.org/wiki/Fallacies_of_distributed_computing" target="_blank" rel="noopener">Fallacies of Distributed Computing</a>）”。</p>
<ol>
<li>网络是稳定的。</li>
<li>网络传输的延迟是零。</li>
<li>网络的带宽是无穷大。</li>
<li>网络是安全的。</li>
<li>网络的拓扑不会改变。</li>
<li>只有一个系统管理员。</li>
<li>传输数据的成本为零。</li>
<li>整个网络是同构的。</li>
</ol>
<p>阿尔农·罗特姆 - 盖尔 - 奥兹（Arnon Rotem-Gal-Oz）写了一篇长文 <a href="http://www.rgoarchitects.com/Files/fallacies.pdf" target="_blank" rel="noopener">Fallacies of Distributed Computing Explained</a> 来解释为什么这些观点是错误的。另外，<a href="http://blog.fogcreek.com/eight-fallacies-of-distributed-computing-tech-talk/" target="_blank" rel="noopener">加勒思·威尔逊（Gareth Wilson）的文章</a> 则用日常生活中的例子，对这些点做了通俗的解释。为什么我们深刻地认识到这 8 个错误？是因为，这要我们清楚地认识到——在分布式系统中错误是不可能避免的，我们在分布式系统中，能做的不是避免错误，而是要把错误的处理当成功能写在代码中。</p>
<p>下面分享几篇一致性方面的论文。</p>
<ul>
<li><p>当然，关于经典的 CAP 理论，也存在一些误导的地方，这个问题在 2012 年有一篇论文 <a href="https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed" target="_blank" rel="noopener">CAP Twelve Years Later: How the Rules Have Changed</a> （<a href="http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed" target="_blank" rel="noopener">中译版</a>）中做了一些讨论，主要是说，在 CAP 中最大的问题就是分区，也就是 P，在 P 发生的情况下，非常难以保证 C 和 A。然而，这是强一致性的情况。</p>
<p>其实，在很多时候，我们并不需要强一致性的系统，所以后来，人们争论关于数据一致性和可用性时，主要是集中在强一致性的 ACID 或最终一致性的 BASE。当时，BASE 还不怎么为世人所接受，主要是大家都觉得 ACID 是最完美的模型，大家很难接受不完美的 BASE。在 CAP 理论中，大家总是觉得需要 “ 三选二 “，也就是说，P 是必选项，那 “ 三选二 “ 的选择题不就变成数据一致性 (consistency)、服务可用性 (availability) 间的 “ 二选一 “ ？</p>
<p>然而，现实却是，P 很少遇到，而 C 和 A 这两个事，工程实践中一致性有不同程度，可用性也有不同等级，在保证分区容错性的前提下，放宽约束后可以兼顾一致性和可用性，两者不是非此即彼。其实，在一个时间可能允许的范围内是可以取舍并交替选择的。</p>
</li>
<li><p><a href="https://pdfs.semanticscholar.org/5015/8bc1a8a67295ab7bce0550886a9859000dc2.pdf" target="_blank" rel="noopener">Harvest, Yield, and Scalable Tolerant Systems</a> ，这篇论文是基于上面那篇 “CAP 12 年后 “ 的论文写的，它主要提出了 Harvest 和 Yield 概念，并把上面那篇论文中所讨论的东西讲得更为仔细了一些。</p>
</li>
<li><p><a href="https://queue.acm.org/detail.cfm?id=1394128" target="_blank" rel="noopener">Base: An Acid Alternative</a> （<a href="http://www.cnblogs.com/savorboard/p/base-an-acid-alternative.html" target="_blank" rel="noopener">中译版</a>），本文是 eBay 的架构师在 2008 年发表给 ACM 的文章，是一篇解释 BASE 原则，或者说最终一致性的经典文章。文中讨论了 BASE 与 ACID 原则的基本差异, 以及如何设计大型网站以满足不断增长的可伸缩性需求，其中有如何对业务做调整和折中，以及一些具体的折中技术的介绍。一个比较经典的话是——“在对数据库进行分区后, 为了可用性（Availability）牺牲部分一致性（Consistency）可以显著地提升系统的可伸缩性 (Scalability)”。</p>
</li>
<li><p><a href="https://www.allthingsdistributed.com/2008/12/eventually_consistent.html" target="_blank" rel="noopener">Eventually Consistent</a> ，这篇文章是 AWS 的 CTO 维尔纳·沃格尔（Werner Vogels）在 2008 年发布在 ACM Queue 上的一篇数据库方面的重要文章，阐述了 NoSQL 数据库的理论基石——最终一致性，对传统的关系型数据库（ACID，Transaction）做了较好的补充。</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/07/how-database-work/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/07/how-database-work/" class="post-title-link" itemprop="url">How database work</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-07 20:31:19 / Modified: 21:33:19" itemprop="dateCreated datePublished" datetime="2020-02-07T20:31:19-05:00">2020-02-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>When it comes to relational databases, I can’t help thinking that something is missing. They’re used everywhere. There are many different databases: from the small and useful SQLite to the powerful Teradata. But, there are only a few articles that explain how a database works. You can google by yourself “how does a relational database work” to see how few results there are. Moreover, those articles are short. Now, if you look for the last trendy technologies (Big Data, NoSQL or JavaScript), you’ll find more in-depth articles explaining how they work.</p>
<p>Are relational databases too old and too boring to be explained outside of university courses, research papers and books?</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/main_databases.jpg" target="_blank" rel="noopener"><img src="../../public/images/main_databases-20200207203251326.jpg" alt="logos of main databases"></a></p>
<p>As a developer, I HATE using something I don’t understand. And, if databases have been used for 40 years, there must be a reason. Over the years, I’ve spent hundreds of hours to really understand these weird black boxes I use every day. <strong>Relational Databases</strong> <strong>are</strong> very interesting because they’re <strong>based on useful and reusable concepts</strong>. If understanding a database interests you but you’ve never had the time or the will to dig into this wide subject, you should like this article.</p>
<p>Though the title of this article is explicit, <strong>the aim of this article is NOT to understand how to use a database</strong>. Therefore, <strong>you should already know how to write a simple join query and basic CRUD queries</strong>; otherwise you might not understand this article. This is the only thing you need to know, I’ll explain everything else.</p>
<p>I’ll start with some computer science stuff like time complexity. I know that some of you hate this concept but, without it, you can’t understand the cleverness inside a database. Since it’s a huge topic, <strong>I’ll focus on</strong> what I think is essential: <strong>the way a database handles an SQL query</strong>. I’ll only present <strong>the basic concepts behind a database</strong> so that at the end of the article you’ll have a good idea of what’s happening under the hood.</p>
<p>Since it’s a long and technical article that involves many algorithms and data structures, take your time to read it. Some concepts are more difficult to understand; you can skip them and still get the overall idea.</p>
<p>For the more knowledgeable of you, this article is more or less divided into 3 parts:</p>
<ul>
<li>An overview of low-level and high-level database components</li>
<li>An overview of the query optimization process</li>
<li>An overview of the transaction and buffer pool management</li>
</ul>
<p>Contents [<a href="http://coding-geek.com/how-databases-work/#" target="_blank" rel="noopener">show</a>]</p>
<h1 id="Back-to-basics"><a href="#Back-to-basics" class="headerlink" title="Back to basics"></a>Back to basics</h1><p>A long time ago (in a galaxy far, far away….), developers had to know exactly the number of operations they were coding. They knew by heart their algorithms and data structures because they couldn’t afford to waste the CPU and memory of their slow computers.</p>
<p>In this part, I’ll remind you about some of these concepts because they are essential to understand a database. I’ll also introduce the notion of <strong>database index</strong>.</p>
<h2 id="O-1-vs-O-n2"><a href="#O-1-vs-O-n2" class="headerlink" title="O(1) vs O(n2)"></a>O(1) vs O(n2)</h2><p>Nowadays, many developers don’t care about time complexity … and they’re right!</p>
<p>But when you deal with a large amount of data (I’m not talking about thousands) or if you’re fighting for milliseconds, it becomes critical to understand this concept. And guess what, databases have to deal with both situations! I won’t bore you a long time, just the time to get the idea. This will help us later to understand the concept of <strong>cost based optimization</strong>.</p>
<h3 id="The-concept"><a href="#The-concept" class="headerlink" title="The concept"></a>The concept</h3><p>The <strong>time complexity is used to see how long an algorithm will take for a given amount of data</strong>. To describe this complexity, computer scientists use the mathematical big O notation. This notation is used with a function that describes how many operations an algorithm needs for a given amount of input data.</p>
<p>For example, when I say “this algorithm is in O( some_function() )”, it means that for a certain amount of data the algorithm needs some_function(a_certain_amount_of_data) operations to do its job.</p>
<p><strong>What’s important is</strong> not the amount of data but <strong>the way the number of operations increases when the amount of data increases</strong>. The time complexity doesn’t give the exact number of operations but a good idea.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/TimeComplexity.png" target="_blank" rel="noopener"><img src="../../public/images/TimeComplexity-20200207203251326.png" alt="time complexity analysis"></a></p>
<p>In this figure, you can see the evolution of different types of complexities. I used a logarithmic scale to plot it. In other words, the number of data is quickly increasing from 1 to 1 billion. We can see that:</p>
<ul>
<li>The O(1) or constant complexity stays constant (otherwise it wouldn’t be called constant complexity).</li>
<li>The <strong>O(log(n)) stays low even with billions of data</strong>.</li>
<li>The worst complexity is the <strong>O(n2) where the number of operations quickly explodes</strong>.</li>
<li>The two other complexities are quickly increasing.</li>
</ul>
<h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><p>With a low amount of data, the difference between O(1) and O(n2) is negligible. For example, let’s say you have an algorithm that needs to process 2000 elements.</p>
<ul>
<li>An O(1) algorithm will cost you 1 operation</li>
<li>An O(log(n)) algorithm will cost you 7 operations</li>
<li>An O(n) algorithm will cost you 2 000 operations</li>
<li>An O(n*log(n)) algorithm will cost you 14 000 operations</li>
<li>An O(n2) algorithm will cost you 4 000 000 operations</li>
</ul>
<p>The difference between O(1) and O(n2) seems a lot (4 million) but you’ll lose at max 2 ms, just the time to blink your eyes. Indeed, current processors can handle <a href="https://en.wikipedia.org/wiki/Instructions_per_second" target="_blank" rel="noopener">hundreds of millions of operations per second</a>. This is why performance and optimization are not an issue in many IT projects.</p>
<p>As I said, it’s still important to know this concept when facing a huge number of data. If this time the algorithm needs to process 1 000 000 elements (which is not that big for a database):</p>
<ul>
<li>An O(1) algorithm will cost you 1 operation</li>
<li>An O(log(n)) algorithm will cost you 14 operations</li>
<li>An O(n) algorithm will cost you 1 000 000 operations</li>
<li>An O(n*log(n)) algorithm will cost you 14 000 000 operations</li>
<li>An O(n2) algorithm will cost you 1 000 000 000 000 operations</li>
</ul>
<p>I didn’t do the math but I’d say with the O(n2) algorithm you have the time to take a coffee (even a second one!). If you put another 0 on the amount of data, you’ll have the time to take a long nap.</p>
<h3 id="Going-deeper"><a href="#Going-deeper" class="headerlink" title="Going deeper"></a>Going deeper</h3><p>To give you an idea:</p>
<ul>
<li>A search in a good hash table gives an element in O(1)</li>
<li>A search in a well-balanced tree gives a result in O(log(n))</li>
<li>A search in an array gives a result in O(n)</li>
<li>The best sorting algorithms have an O(n*log(n)) complexity.</li>
<li>A bad sorting algorithm has an O(n2) complexity</li>
</ul>
<p>Note: In the next parts, we’ll see these algorithms and data structures.</p>
<p>There are multiple types of time complexity:</p>
<ul>
<li>the average case scenario</li>
<li>the best case scenario</li>
<li>and the worst case scenario</li>
</ul>
<p>The time complexity is often the worst case scenario.</p>
<p>I only talked about time complexity but complexity also works for:</p>
<ul>
<li>the memory consumption of an algorithm</li>
<li>the disk I/O consumption of an algorithm</li>
</ul>
<p>Of course there are worse complexities than n2, like:</p>
<ul>
<li>n4: that sucks! Some of the algorithms I’ll mention have this complexity.</li>
<li>3n: that sucks even more! One of the algorithms we’re going to see in the middle of this article has this complexity (and it’s really used in many databases).</li>
<li>factorial n : you’ll never get your results, even with a low amount of data.</li>
<li>nn: if you end-up with this complexity, you should ask yourself if IT is really your field…</li>
</ul>
<p>Note: I didn’t give you the real definition of the big O notation but just the idea. You can read this article on <a href="https://en.wikipedia.org/wiki/Big_O_notation" target="_blank" rel="noopener">Wikipedia</a> for the real (asymptotic) definition.</p>
<h2 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a>Merge Sort</h2><p>What do you do when you need to sort a collection? What? You call the sort() function … ok, good answer… But for a database you have to understand how this sort() function works.</p>
<p>There are several good sorting algorithms so I’ll focus on the most important one: <strong>the merge sort</strong>. You might not understand right now why sorting data is useful but you should after the part on query optimization. Moreover, understanding the merge sort will help us later to understand a common database join operation called the <strong>merge join</strong>.</p>
<h3 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h3><p>Like many useful algorithms, the merge sort is based on a trick: merging 2 sorted arrays of size N/2 into a N-element sorted array only costs N operations. This operation is called a <strong>merge</strong>.</p>
<p>Let’s see what this means with a simple example:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/merge_sort_3.png" target="_blank" rel="noopener"><img src="../../public/images/merge_sort_3-20200207203251311.png" alt="merge operation during merge sort algorithm"></a></p>
<p>You can see on this figure that to construct the final sorted array of 8 elements, you only need to iterate one time in the 2 4-element arrays. Since both 4-element arrays are already sorted:</p>
<ul>
<li>1) you compare both current elements in the 2 arrays (current=first for the first time)</li>
<li>2) then take the lowest one to put it in the 8-element array</li>
<li>3) and go to the next element in the array you took the lowest element</li>
<li>and repeat 1,2,3 until you reach the last element of one of the arrays.</li>
<li>Then you take the rest of the elements of the other array to put them in the 8-element array.</li>
</ul>
<p>This works because both 4-element arrays are sorted and therefore you don’t need to “go back” in these arrays.</p>
<p>Now that we’ve understood this trick, here is my pseudocode of the merge sort.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array mergeSort(array a)&#96;&#96;  &#96;&#96;if&#96;&#96;(length(a)&#x3D;&#x3D;&#96;&#96;1&#96;&#96;)&#96;&#96;   &#96;&#96;return&#96; &#96;a[&#96;&#96;0&#96;&#96;];&#96;&#96;  &#96;&#96;end &#96;&#96;if&#96; &#96;  &#96;&#96;&#x2F;&#x2F;recursive calls&#96;&#96;  &#96;&#96;[left_array right_array] :&#x3D; split_into_2_equally_sized_arrays(a);&#96;&#96;  &#96;&#96;array new_left_array :&#x3D; mergeSort(left_array);&#96;&#96;  &#96;&#96;array new_right_array :&#x3D; mergeSort(right_array);&#96; &#96;  &#96;&#96;&#x2F;&#x2F;merging the 2 small ordered arrays into a big one&#96;&#96;  &#96;&#96;array result :&#x3D; merge(new_left_array,new_right_array);&#96;&#96;  &#96;&#96;return&#96; &#96;result;</span><br></pre></td></tr></table></figure>

<p>The merge sort breaks the problem into smaller problems then finds the results of the smaller problems to get the result of the initial problem (note: this kind of algorithms is called divide and conquer). If you don’t understand this algorithm, don’t worry; I didn’t understand it the first time I saw it. If it can help you, I see this algorithm as a two-phase algorithm:</p>
<ul>
<li>The division phase where the array is divided into smaller arrays</li>
<li>The sorting phase where the small arrays are put together (using the merge) to form a bigger array.</li>
</ul>
<h3 id="Division-phase"><a href="#Division-phase" class="headerlink" title="Division phase"></a>Division phase</h3><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/merge_sort_1.png" target="_blank" rel="noopener"><img src="../../public/images/merge_sort_1-20200207203251327.png" alt="division phaseduring merge sort algorithm"></a></p>
<p>During the division phase, the array is divided into unitary arrays using 3 steps. The formal number of steps is log(N) (since N=8, log(N) = 3).</p>
<p>How do I know that?</p>
<p>I’m a genius! In one word: mathematics. The idea is that each step divides the size of the initial array by 2. The number of steps is the number of times you can divide the initial array by two. This is the exact definition of logarithm (in base 2).</p>
<h3 id="Sorting-phase"><a href="#Sorting-phase" class="headerlink" title="Sorting phase"></a>Sorting phase</h3><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/merge_sort_2.png" target="_blank" rel="noopener"><img src="../../public/images/merge_sort_2-20200207203251328.png" alt="sort phaseduring merge sort algorithm"></a></p>
<p>In the sorting phase, you start with the unitary arrays. During each step, you apply multiple merges and the overall cost is N=8 operations:</p>
<ul>
<li>In the first step you have 4 merges that cost 2 operations each</li>
<li>In the second step you have 2 merges that cost 4 operations each</li>
<li>In the third step you have 1 merge that costs 8 operations</li>
</ul>
<p>Since there are log(N) steps, <strong>the overall costs N * log(N) operations</strong>.</p>
<h3 id="The-power-of-the-merge-sort"><a href="#The-power-of-the-merge-sort" class="headerlink" title="The power of the merge sort"></a>The power of the merge sort</h3><p>Why this algorithm is so powerful?</p>
<p>Because:</p>
<ul>
<li>You can modify it in order to reduce the memory footprint, in a way that you don’t create new arrays but you directly modify the input array.</li>
</ul>
<p>Note: this kind of algorithms is called <a href="https://en.wikipedia.org/wiki/In-place_algorithm" target="_blank" rel="noopener">in-place</a>.</p>
<ul>
<li>You can modify it in order to use disk space and a small amount of memory at the same time without a huge disk I/O penalty. The idea is to load in memory only the parts that are currently processed. This is important when you need to sort a multi-gigabyte table with only a memory buffer of 100 megabytes.</li>
</ul>
<p>Note: this kind of algorithms is called <a href="https://en.wikipedia.org/wiki/External_sorting" target="_blank" rel="noopener">external sorting</a>.</p>
<ul>
<li>You can modify it to run on multiple processes/threads/servers.</li>
</ul>
<p>For example, the distributed merge sort is one of the key components of <a href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Reducer.html" target="_blank" rel="noopener">Hadoop </a>(which is THE framework in Big Data).</p>
<ul>
<li>This algorithm can turn lead into gold (true fact!).</li>
</ul>
<p>This sorting algorithm is used in most (if not all) databases but it’s not the only one. If you want to know more, you can read this <a href="http://wwwlgis.informatik.uni-kl.de/archiv/wwwdvs.informatik.uni-kl.de/courses/DBSREAL/SS2005/Vorlesungsunterlagen/Implementing_Sorting.pdf" target="_blank" rel="noopener">research paper</a> that discusses the pros and cons of the common sorting algorithms in a database.</p>
<h2 id="Array-Tree-and-Hash-table"><a href="#Array-Tree-and-Hash-table" class="headerlink" title="Array, Tree and Hash table"></a>Array, Tree and Hash table</h2><p>Now that we understand the idea behind time complexity and sorting, I have to tell you about 3 data structures. It’s important because they’re <strong>the backbone of modern databases</strong>. I’ll also introduce the notion of <strong>database index</strong>.</p>
<h3 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h3><p>The two-dimensional array is the simplest data structure. A table can be seen as an array. For example:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/array.png" target="_blank" rel="noopener"><img src="../../public/images/array-20200207203251327.png" alt="array table in databases"></a></p>
<p>This 2-dimensional array is a table with rows and columns:</p>
<ul>
<li>Each row represents a subject</li>
<li>The columns the features that describe the subjects.</li>
<li>Each column stores a certain type of data (integer, string, date …).</li>
</ul>
<p>Though it’s great to store and visualize data, when you need to look for a specific value it sucks.</p>
<p>For example, <strong>if you want to find all the guys who work in the UK</strong>, you’ll have to look at each row to find if the row belongs to the UK. <strong>This will cost you N operations</strong> (N being the number of rows) which is not bad but could there be a faster way? This is where trees come into play.</p>
<p>Note: Most modern databases provide advanced arrays to store tables efficiently like heap-organized tables or index-organized tables. But it doesn’t change the problem of fast searching for a specific condition on a group of columns.</p>
<h3 id="Tree-and-database-index"><a href="#Tree-and-database-index" class="headerlink" title="Tree and database index"></a>Tree and database index</h3><p>A binary search tree is a binary tree with a special property, the key in each node must be:</p>
<ul>
<li>greater than all keys stored in the left sub-tree</li>
<li>smaller than all keys stored in the right sub-tree</li>
</ul>
<p>Let’s see what it means visually</p>
<h4 id="The-idea"><a href="#The-idea" class="headerlink" title="The idea"></a>The idea</h4><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/BST.png" target="_blank" rel="noopener"><img src="../../public/images/BST-20200207203251343.png" alt="binary search tree"></a></p>
<p>This tree has N=15 elements. Let’s say I’m looking for 208:</p>
<ul>
<li>I start with the root whose key is 136. Since 136&lt;208, I look at the right sub-tree of the node 136.</li>
<li>398&gt;208 so, I look at the left sub-tree of the node 398</li>
<li>250&gt;208 so, I look at the left sub-tree of the node 250</li>
<li>200&lt;208 so, I look at the right sub-tree of the node 200. But 200 doesn’t have a right subtree, <strong>the value doesn’t exist</strong> (because if it did exist it would be in the right subtree of 200)</li>
</ul>
<p>Now let’s say I’m looking for 40</p>
<ul>
<li>I start with the root whose key is 136. Since 136&gt;40, I look at the left sub-tree of the node 136.</li>
<li>80&gt;40 so, I look at the left sub-tree of the node 80</li>
<li>40= 40, <strong>the node exists</strong>. I extract the id of the row inside the node (it’s not in the figure) and look at the table for the given row id.</li>
<li>Knowing the row id let me know where the data is precisely on the table and therefore I can get it instantly.</li>
</ul>
<p>In the end, both searches cost me the number of levels inside the tree. If you read carefully the part on the merge sort you should see that there are log(N) levels. So the <strong>cost of the search is log(N)</strong>, not bad!</p>
<h4 id="Back-to-our-problem"><a href="#Back-to-our-problem" class="headerlink" title="Back to our problem"></a>Back to our problem</h4><p>But this stuff is very abstract so let’s go back to our problem. Instead of a stupid integer, imagine the string that represents the country of someone in the previous table. Suppose you have a tree that contains the column “country” of the table:</p>
<ul>
<li>If you want to know who is working in the UK</li>
<li>you look at the tree to get the node that represents the UK</li>
<li>inside the “UK node” you’ll find the locations of the rows of the UK workers.</li>
</ul>
<p>This search only costs you log(N) operations instead of N operations if you directly use the array. What you’ve just imagined was a <strong>database index</strong>.</p>
<p>You can build a tree index for any group of columns (a string, an integer, 2 strings, an integer and a string, a date …) as long as you have a function to compare the keys (i.e. the group of columns) so that you can establish an <strong>order</strong> <strong>among the keys</strong> (which is the case for any basic types in a database).</p>
<h4 id="B-Tree-Index"><a href="#B-Tree-Index" class="headerlink" title="B+Tree Index"></a>B+Tree Index</h4><p>Although this tree works well to get a specific value, there is a BIG problem when you need to <strong>get multiple elements</strong> <strong>between two values</strong>. It will cost O(N) because you’ll have to look at each node in the tree and check if it’s between these 2 values (for example, with an in-order traversal of the tree). Moreover this operation is not disk I/O friendly since you’ll have to read the full tree. We need to find a way to efficiently do a <strong>range query</strong>. To answer this problem, modern databases use a modified version of the previous tree called B+Tree. In a B+Tree:</p>
<ul>
<li>only the lowest nodes (the leaves) <strong>store information</strong> (the location of the rows in the associated table)</li>
<li>the other nodes are just here <strong>to route</strong> to the right node <strong>during the search</strong>.</li>
</ul>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/database_index.png" target="_blank" rel="noopener"><img src="../../public/images/database_index-20200207203251357.png" alt="B+Tree index in databases"></a></p>
<p>As you can see, there are more nodes (twice more). Indeed, you have additional nodes, the “decision nodes” that will help you to find the right node (that stores the location of the rows in the associated table). But the search complexity is still in O(log(N)) (there is just one more level). The big difference is that <strong>the lowest nodes are linked to their successors</strong>.</p>
<p>With this B+Tree, if you’re looking for values between 40 and 100:</p>
<ul>
<li>You just have to look for 40 (or the closest value after 40 if 40 doesn’t exist) like you did with the previous tree.</li>
<li>Then gather the successors of 40 using the direct links to the successors until you reach 100.</li>
</ul>
<p>Let’s say you found M successors and the tree has N nodes. The search for a specific node costs log(N) like the previous tree. But, once you have this node, you get the M successors in M operations with the links to their successors. <strong>This search only costs M + log(N)</strong> operations vs N operations with the previous tree. Moreover, you don’t need to read the full tree (just M + log(N) nodes), which means less disk usage. If M is low (like 200 rows) and N large (1 000 000 rows) it makes a BIG difference.</p>
<p>But there are new problems (again!). If you add or remove a row in a database (and therefore in the associated B+Tree index):</p>
<ul>
<li>you have to keep the order between nodes inside the B+Tree otherwise you won’t be able to find nodes inside the mess.</li>
<li>you have to keep the lowest possible number of levels in the B+Tree otherwise the time complexity in O(log(N)) will become O(N).</li>
</ul>
<p>I other words, the B+Tree needs to be self-ordered and self-balanced. Thankfully, this is possible with smart deletion and insertion operations. But this comes with a cost: the insertion and deletion in a B+Tree are in O(log(N)). This is why some of you have heard that <strong>using too many indexes is not a good idea.</strong> Indeed, <strong>you’re slowing down the fast insertion/update/deletion of a row</strong> in a table since the database needs to update the indexes of the table with a costly O(log(N)) operation per index. Moreover, adding indexes means more workload for the <strong>transaction manager</strong> (we will see this manager at the end of the article).</p>
<p>For more details, you can look at the Wikipedia <a href="https://en.wikipedia.org/wiki/B%2B_tree" target="_blank" rel="noopener">article about B+Tree</a>. If you want an example of a B+Tree implementation in a database, look at <a href="http://blog.jcole.us/2013/01/07/the-physical-structure-of-innodb-index-pages/" target="_blank" rel="noopener">this article</a> and <a href="http://blog.jcole.us/2013/01/10/btree-index-structures-in-innodb/" target="_blank" rel="noopener">this article</a> from a core developer of MySQL. They both focus on how innoDB (the engine of MySQL) handles indexes.</p>
<p>Note: I was told by a reader that, because of low-level optimizations, the B+Tree needs to be fully balanced.</p>
<h3 id="Hash-table"><a href="#Hash-table" class="headerlink" title="Hash table"></a>Hash table</h3><p>Our last important data structure is the hash table. It’s very useful when you want to quickly look for values.  Moreover, understanding the hash table will help us later to understand a common database join operation called the <strong>hash join</strong>. This data structure is also used by a database to store some internal stuff (like the <strong>lock table</strong> or the <strong>buffer pool</strong>, we’ll see both concepts later)</p>
<p>The hash table is a data structure that quickly finds an element with its key. To build a hash table you need to define:</p>
<ul>
<li><strong>a key</strong> for your elements</li>
<li><strong>a hash function</strong> for the keys. The computed hashes of the keys give the locations of the elements (called <strong>buckets</strong>).</li>
<li><strong>a function to compare the keys</strong>. Once you found the right bucket you have to find the element you’re looking for inside the bucket using this comparison.</li>
</ul>
<h4 id="A-simple-example"><a href="#A-simple-example" class="headerlink" title="A simple example"></a>A simple example</h4><p>Let’s have a visual example:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/hash_table.png" target="_blank" rel="noopener"><img src="../../public/images/hash_table-20200207203251356.png" alt="hash table"></a></p>
<p>This hash table has 10 buckets. Since I’m lazy I only drew 5 buckets but I know you’re smart so I let you imagine the 5 others. The Hash function I used is the modulo 10 of the key. In other words I only keep the last digit of the key of an element to find its bucket:</p>
<ul>
<li>if the last digit is 0 the element ends up in the bucket 0,</li>
<li>if the last digit is 1 the element ends up in the bucket 1,</li>
<li>if the last digit is 2 the element ends up in the bucket 2,</li>
<li>…</li>
</ul>
<p>The compare function I used is simply the equality between 2 integers.</p>
<p>Let’s say you want to get the element 78:</p>
<ul>
<li>The hash table computes the hash code for 78 which is 8.</li>
<li>It looks in the bucket 8, and the first element it finds is 78.</li>
<li>It gives you back the element 78</li>
<li><strong>The</strong> <strong>search only costs 2 operations</strong> (1 for computing the hash value and the other for finding the element inside the bucket).</li>
</ul>
<p>Now, let’s say you want to get the element 59:</p>
<ul>
<li>The hash table computes the hash code for 59 which is 9.</li>
<li>It looks in the bucket 9, and the first element it finds is 99. Since 99!=59, element 99 is not the right element.</li>
<li>Using the same logic, it looks at the second element (9), the third (79), … , and the last (29).</li>
<li>The element doesn’t exist.</li>
<li><strong>The search costs 7 operations</strong>.</li>
</ul>
<h4 id="A-good-hash-function"><a href="#A-good-hash-function" class="headerlink" title="A good hash function"></a>A good hash function</h4><p>As you can see, depending on the value you’re looking for, the cost is not the same!</p>
<p>If I now change the hash function with the modulo 1 000 000 of the key (i.e. taking the last 6 digits), the second search only costs 1 operation because there are no elements in the bucket 000059. <strong>The real challenge is to find a good hash function that will create buckets that contain a very small amount of elements</strong>.</p>
<p>In my example, finding a good hash function is easy. But this is a simple example, finding a good hash function is more difficult when the key is:</p>
<ul>
<li>a string (for example the last name of a person)</li>
<li>2 strings (for example the last name and the first name of a person)</li>
<li>2 strings and a date (for example the last name, the first name and the birth date of a person)</li>
<li>…</li>
</ul>
<p><strong>With a good hash function,</strong> <strong>the search in a hash table is in O(1)</strong>.</p>
<h4 id="Array-vs-hash-table"><a href="#Array-vs-hash-table" class="headerlink" title="Array vs hash table"></a>Array vs hash table</h4><p>Why not using an array?</p>
<p>Hum, you’re asking a good question.</p>
<ul>
<li>A hash table can be <strong>half loaded in memory</strong> and the other buckets can stay on disk.</li>
<li>With an array you have to use a contiguous space in memory. If you’re loading a large table it’s <strong>very difficult to have enough contiguous space</strong>.</li>
<li>With a hash table you can <strong>choose the key you want</strong> (for example the country AND the last name of a person).</li>
</ul>
<p>For more information, you can read my article on the <a href="http://coding-geek.com/how-does-a-hashmap-work-in-java/" target="_blank" rel="noopener">Java HashMap</a> which is an efficient hash table implementation; you don’t need to understand Java to understand the concepts inside this article.</p>
<h1 id="Global-overview"><a href="#Global-overview" class="headerlink" title="Global overview"></a>Global overview</h1><p>We’ve just seen the basic components inside a database. We now need to step back to see the big picture.</p>
<p>A database is a collection of information that can easily be accessed and modified. But a simple bunch of files could do the same. In fact, the simplest databases like SQLite are nothing more than a bunch of files. But SQLite is a well-crafted bunch of files because it allows you to:</p>
<ul>
<li>use transactions that ensure data are safe and coherent</li>
<li>quickly process data even when you’re dealing with millions of data</li>
</ul>
<p>More generally, a database can be seen as the following figure:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/global_overview.png" target="_blank" rel="noopener"><img src="../../public/images/global_overview-20200207203251364.png" alt="global overview of a database"></a></p>
<p>Before writing this part, I’ve read multiple books/papers and every source had its on way to represent a database. So, don’t focus too much on how I organized this database or how I named the processes because I made some choices to fit the plan of this article. What matters are the different components; the overall idea is that <strong>a database is divided into multiple components that interact with each other</strong>.</p>
<p>The core components:</p>
<ul>
<li><strong>The process manager</strong>: Many databases have a <strong>pool of processes/threads</strong> that needs to be managed. Moreover, in order to gain nanoseconds, some modern databases use their own threads instead of the Operating System threads.</li>
<li><strong>The network manager</strong>: Network I/O is a big issue, especially for distributed databases. That’s why some databases have their own manager.</li>
<li><strong>File system manager</strong>: <strong>Disk I/O is the first bottleneck of a database</strong>. Having a manager that will perfectly handle the Operating System file system or even replace it is important.</li>
<li><strong>The memory manager</strong>: To avoid the disk I/O penalty a large quantity of ram is required. But if you handle a large amount of memory, you need an efficient memory manager. Especially when you have many queries using memory at the same time.</li>
<li><strong>Security Manager</strong>: for managing the authentication and the authorizations of the users</li>
<li><strong>Client manager</strong>: for managing the client connections</li>
<li>…</li>
</ul>
<p>The tools:</p>
<ul>
<li><strong>Backup manager</strong>: for saving and restoring a database.</li>
<li><strong>Recovery manager</strong>: for restarting the database in a <strong>coherent state</strong> after a crash</li>
<li><strong>Monitor manager</strong>: for logging the activity of the database and providing tools to monitor a database</li>
<li><strong>Administration manager</strong>: for storing metadata (like the names and the structures of the tables) and providing tools to manage databases, schemas, tablespaces, …</li>
<li>…</li>
</ul>
<p>The query Manager:</p>
<ul>
<li><strong>Query parser</strong>: to check if a query is valid</li>
<li><strong>Query rewriter</strong>: to pre-optimize a query</li>
<li><strong>Query optimizer</strong>: to optimize a query</li>
<li><strong>Query executor</strong>: to compile and execute a query</li>
</ul>
<p>The data manager:</p>
<ul>
<li><strong>Transaction manager</strong>: to handle transactions</li>
<li><strong>Cache manager</strong>: to put data in memory before using them and put data in memory before writing them on disk</li>
<li><strong>Data access manager</strong>: to access data on disk</li>
</ul>
<p>For the rest of this article, I’ll focus on how a database manages an SQL query through the following processes:</p>
<ul>
<li>the client manager</li>
<li>the query manager</li>
<li>the data manager (I’ll also include the recovery manager in this part)</li>
</ul>
<h1 id="Client-manager"><a href="#Client-manager" class="headerlink" title="Client manager"></a>Client manager</h1><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/client_manager.png" target="_blank" rel="noopener"><img src="../../public/images/client_manager-20200207203251358.png" alt="client manager in databases"></a></p>
<p>The client manager is the part that handles the communications with the client. The client can be a (web) server or an end-user/end-application. The client manager provides different ways to access the database through a set of well-known APIs: JDBC, ODBC, OLE-DB …</p>
<p>It can also provide proprietary database access APIs.</p>
<p>When you connect to a database:</p>
<ul>
<li>The manager first checks your <strong>authentication</strong> (your login and password) and then checks if you have the <strong>authorizations</strong> to use the database. These access rights are set by your DBA.</li>
<li>Then, it checks if there is a process (or a thread) available to manage your query.</li>
<li>It also checks if the database if not under heavy load.</li>
<li>It can wait a moment to get the required resources. If this wait reaches a timeout, it closes the connection and gives a readable error message.</li>
<li>Then it <strong>sends your query to the query manager</strong> and your query is processed</li>
<li>Since the query processing is not an “all or nothing” thing, as soon as it gets data from the query manager, it <strong>stores</strong> <strong>the partial results in a buffer and start sending</strong> them to you.</li>
<li>In case of problem, it stops the connection, gives you a <strong>readable explanation</strong> and releases the resources.</li>
</ul>
<h1 id="Query-manager"><a href="#Query-manager" class="headerlink" title="Query manager"></a>Query manager</h1><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/query_manager.png" target="_blank" rel="noopener"><img src="../../public/images/query_manager-20200207203251359.png" alt="query manager in databases"></a></p>
<p><strong>This part is where the power of a database lies</strong>. During this part, an ill-written query is transformed into a <strong>fast</strong> executable code. The code is then executed and the results are returned to the client manager. It’s a multiple-step operation:</p>
<ul>
<li>the query is first <strong>parsed</strong> to see if it’s valid</li>
<li>it’s then <strong>rewritten</strong> to remove useless operations and add some pre-optimizations</li>
<li>it’s then <strong>optimized</strong> to improve the performances and transformed into an execution and data access plan.</li>
<li>then the plan is <strong>compiled</strong></li>
<li>at last, it’s <strong>executed</strong></li>
</ul>
<p>In this part, I won’t talk a lot about the last 2 points because they’re less important.</p>
<p>After reading this part, if you want a better understanding I recommend reading:</p>
<ul>
<li>The initial research paper (1979) on cost based optimization: <a href="http://www.cs.berkeley.edu/~brewer/cs262/3-selinger79.pdf" target="_blank" rel="noopener">Access Path Selection in a Relational Database Management System</a>. This article is only 12 pages and understandable with an average level in computer science.</li>
<li>A very good and in-depth presentation on how DB2 9.X optimizes queries <a href="http://infolab.stanford.edu/~hyunjung/cs346/db2-talk.pdf" target="_blank" rel="noopener">here</a></li>
<li>A very good presentation on how PostgreSQL optimizes queries <a href="http://momjian.us/main/writings/pgsql/optimizer.pdf" target="_blank" rel="noopener">here</a>. It’s the most accessible document since it’s more a presentation on “let’s see what query plans PostgreSQL gives in these situations“ than a “let’s see the algorithms used by PostgreSQL”.</li>
<li>The official <a href="https://www.sqlite.org/optoverview.html" target="_blank" rel="noopener">SQLite documentation</a> about optimization. It’s “easy” to read because SQLite uses simple rules. Moreover, it’s the only official documentation that really explains how it works.</li>
<li>A good presentation on how SQL Server 2005 optimizes queries <a href="https://blogs.msdn.com/cfs-filesystemfile.ashx/__key/communityserver-components-postattachments/00-08-50-84-93/QPTalk.pdf" target="_blank" rel="noopener">here</a></li>
<li>A white paper about optimization in Oracle 12c <a href="http://www.oracle.com/technetwork/database/bi-datawarehousing/twp-optimizer-with-oracledb-12c-1963236.pdf" target="_blank" rel="noopener">here</a></li>
<li>2 theoretical courses on query optimization from the authors of the book “<em>DATABASE SYSTEM CONCEPTS”</em> <a href="http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch12.ppt" target="_blank" rel="noopener">here</a> and t<a href="http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch13.ppt" target="_blank" rel="noopener">here</a>. A good read that focuses on disk I/O cost but a good level in CS is required.</li>
<li>Another <a href="https://www.informatik.hu-berlin.de/de/forschung/gebiete/wbi/teaching/archive/sose05/dbs2/slides/09_joins.pdf" target="_blank" rel="noopener">theoretical course</a> that I find more accessible but that only focuses on join operators and disk I/O.</li>
</ul>
<h2 id="Query-parser"><a href="#Query-parser" class="headerlink" title="Query parser"></a>Query parser</h2><p>Each SQL statement is sent to the parser where it is checked for correct syntax. If you made a mistake in your query the parser will reject the query. For example, if you wrote “SLECT …” instead of “SELECT …”,  the story ends here.</p>
<p>But this goes deeper. It also checks that the keywords are used in the right order. For example a WHERE before a SELECT will be rejected.</p>
<p>Then, the tables and the fields inside the query are analyzed. The parser uses the metadata of the database to check:</p>
<ul>
<li>If the <strong>tables exist</strong></li>
<li>If the <strong>fields</strong> of the tables exist</li>
<li>If the <strong>operations</strong> for the types of the fields <strong>are possible</strong> (for example you can’t compare an integer with a string, you can’t use a substring() function on an integer)</li>
</ul>
<p>Then it checks if you have the <strong>authorizations</strong> to read (or write) the tables in the query. Again, these access rights on tables are set by your DBA.</p>
<p>During this parsing, the SQL query is transformed into an internal representation (often a tree)</p>
<p>If everything is ok then the internal representation is sent to the query rewriter.</p>
<h2 id="Query-rewriter"><a href="#Query-rewriter" class="headerlink" title="Query rewriter"></a>Query rewriter</h2><p>At this step, we have an internal representation of a query. The aim of the rewriter is:</p>
<ul>
<li>to pre-optimize the query</li>
<li>to avoid unnecessary operations</li>
<li>to help the optimizer to find the best possible solution</li>
</ul>
<p>The rewriter executes a list of known rules on the query. If the query fits a pattern of a rule, the rule is applied and the query is rewritten. Here is a non-exhaustive list of (optional) rules:</p>
<ul>
<li><strong>View merging:</strong> If you’re using a view in your query, the view is transformed with the SQL code of the view.</li>
<li><strong>Subquery flattening</strong>: Having subqueries is very difficult to optimize so the rewriter will try to modify a query with a subquery to remove the subquery.</li>
</ul>
<p>For example</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;PERSON.*&#96;&#96;FROM&#96; &#96;PERSON&#96;&#96;WHERE&#96; &#96;PERSON.person_key &#96;&#96;IN&#96;&#96;(&#96;&#96;SELECT&#96; &#96;MAILS.person_key&#96;&#96;FROM&#96; &#96;MAILS&#96;&#96;WHERE&#96; &#96;MAILS.mail &#96;&#96;LIKE&#96; &#96;&#39;christophe%&#39;&#96;&#96;);</span><br></pre></td></tr></table></figure>

<p>Will be replaced by</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;PERSON.*&#96;&#96;FROM&#96; &#96;PERSON, MAILS&#96;&#96;WHERE&#96; &#96;PERSON.person_key &#x3D; MAILS.person_key&#96;&#96;and&#96; &#96;MAILS.mail &#96;&#96;LIKE&#96; &#96;&#39;christophe%&#39;&#96;&#96;;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Removal of unnecessary operators</strong>: For example if you use a DISTINCT whereas you have a UNIQUE constraint that prevents the data from being non-unique, the DISTINCT keyword is removed.</li>
<li><strong>Redundant join elimination:</strong> If you have twice the same join condition because one join condition is hidden in a view or if by transitivity there is a useless join, it’s removed.</li>
<li><strong>Constant arithmetic evaluation:</strong> If you write something that requires a calculus, then it’s computed once during the rewriting. For example WHERE AGE &gt; 10+2 is transformed into WHERE AGE &gt; 12 and TODATE(“some date”) is transformed into the date in the datetime format</li>
<li><strong>(**</strong>Advanced) Partition Pruning:** If you’re using a partitioned table, the rewriter is able to find what partitions to use.</li>
<li><strong>(Advanced) Materialized view rewrite</strong>: If you have a materialized view that matches a subset of the predicates in your query, the rewriter checks if the view is up to date and modifies the query to use the materialized view instead of the raw tables.</li>
<li><strong>(Advanced) Custom rules:</strong> If you have custom rules to modify a query (like Oracle policies), then the rewriter executes these rules</li>
<li><strong>(Advanced) Olap transformations</strong>: analytical/windowing functions, star joins, rollup … are also transformed (but I’m not sure if it’s done by the rewriter or the optimizer, since both processes are very close it must depends on the database).</li>
</ul>
<p>This rewritten query is then sent to the query optimizer where the fun begins!</p>
<h2 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h2><p>Before we see how a database optimizes a query we need to speak about <strong>statistics</strong> because <strong>without them</strong> <strong>a database is stupid</strong>. If you don’t tell the database to analyze its own data, it will not do it and it will make (very) bad assumptions.</p>
<p>But what kind of information does a database need?</p>
<p>I have to (briefly) talk about how databases and Operating systems store data. They’re using a minimum unit called <strong>a</strong> <strong>page</strong> or a block (4 or 8 kilobytes by default). This means that if you only need 1 Kbytes it will cost you one page anyway. If the page takes 8 Kbytes then you’ll waste 7 Kbytes.</p>
<p>Back to the statistics! When you ask a database to gather statistics, it computes values like:</p>
<ul>
<li>The number of rows/pages in a table</li>
<li>For each column in a table:<ul>
<li>distinct data values</li>
<li>the length of data values (min, max, average)</li>
<li>data range information (min, max, average)</li>
</ul>
</li>
<li>Information on the indexes of the table.</li>
</ul>
<p><strong>These statistics will help the optimizer to estimate the</strong> <strong>disk I/O, CPU and memory usages of the query.</strong></p>
<p>The statistics for each column are very important. For example if a table PERSON needs to be joined on 2 columns: LAST_NAME, FIRST_NAME. With the statistics, the database knows that there are only 1 000 different values on FIRST_NAME and 1 000 000 different values on LAST_NAME. Therefore, the database will join the data on LAST_NAME, FIRST_NAME instead of FIRST_NAME,LAST_NAME because it produces way less comparisons since the LAST_NAME are unlikely to be the same so most of the time a comparison on the 2 (or 3) first characters of the LAST_NAME is enough.</p>
<p>But these are basic statistics. You can ask a database to compute advanced statistics called <strong>histograms</strong>. Histograms are statistics that inform about the distribution of the values inside the columns. For example</p>
<ul>
<li>the most frequent values</li>
<li>the quantiles</li>
<li>…</li>
</ul>
<p>These extra statistics will help the database to find an even better query plan. Especially for equality predicate (ex: WHERE AGE = 18 ) or range predicates (ex: WHERE AGE &gt; 10 and AGE &lt;40 ) because the database will have a better idea of the number rows concerned by these predicates (note: the technical word for this concept is selectivity).</p>
<p>The statistics are stored in the metadata of the database. For example you can see the statistics for the (non-partitioned) tables:</p>
<ul>
<li>in USER/ALL/DBA_TABLES and USER/ALL/DBA_TAB_COLUMNS for Oracle</li>
<li>in SYSCAT.<em>TABLES</em> and <em>SYSCAT.COLUMNS for DB2</em>.</li>
</ul>
<p>The <strong>statistics have to be up to date</strong>. There is nothing worse than a database thinking a table has only 500 rows whereas it has 1 000 000 rows. The only drawback of the statistics is that <strong>it takes time to compute them</strong>. This is why they’re not automatically computed by default in most databases. It becomes difficult with millions of data to compute them. In this case, you can choose to compute only the basics statistics or to compute the stats on a sample of the database.</p>
<p>For example, when I was working on a project dealing with hundreds of millions rows in each tables, I chose to compute the statistics on only 10%, which led to a huge gain in time. For the story it turned out to be a bad decision because occasionally the 10% chosen by Oracle 10G for a specific column of a specific table were very different from the overall 100% (which is very unlikely to happen for a table with 100M rows). This wrong statistic led to a query taking occasionally 8 hours instead of 30 seconds; a nightmare to find the root cause. This example shows how important the statistics are.</p>
<p>Note: Of course, there are more advanced statistics specific for each database. If you want to know more, read the documentations of the databases. That being said, I’ve tried to understand how the statistics are used and the best official documentation I found was the <a href="http://www.postgresql.org/docs/9.4/static/row-estimation-examples.html" target="_blank" rel="noopener">one from PostgreSQL</a>.</p>
<h2 id="Query-optimizer"><a href="#Query-optimizer" class="headerlink" title="Query optimizer"></a>Query optimizer</h2><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/15-McDonalds_CBO.jpg" target="_blank" rel="noopener"><img src="../../public/images/15-McDonalds_CBO-20200207203251382.jpg" alt="CBO"></a></p>
<p>All modern databases are using a <strong>Cost Based Optimization</strong> (or <strong>CBO</strong>) to optimize queries. The idea is to put a cost an every operation and find the best way to reduce the cost of the query by using the cheapest chain of operations to get the result.</p>
<p>To understand how a cost optimizer works I think it’s good to have an example to “feel” the complexity behind this task. In this part I’ll present you the 3 common ways to join 2 tables and we will quickly see that even a simple join query is a nightmare to optimize. After that, we’ll see how real optimizers do this job.</p>
<p>For these joins, I’ll focus on their time complexity but <strong>a</strong> <strong>database optimizer computes</strong> their <strong>CPU cost, disk I/O cost and memory requirement</strong>. The difference between time complexity and CPU cost is that time cost is very approximate (it’s for lazy guys like me). For the CPU cost, I should count every operation like an addition, an “if statement”, a multiplication, an iteration … Moreover:</p>
<ul>
<li>Each high level code operation has a specific number of low level CPU operations.</li>
<li>The cost of a CPU operation is not the same (in terms of CPU cycles) whether you’re using an Intel Core i7, an Intel Pentium 4, an AMD Opteron…. In other words it depends on the CPU architecture.</li>
</ul>
<p>Using the time complexity is easier (at least for me) and with it we can still get the concept of CBO. I’ll sometimes speak about disk I/O since it’s an important concept. Keep in mind that <strong>the bottleneck is most of the time the disk I/O and not the CPU usage</strong>.</p>
<h3 id="Indexes"><a href="#Indexes" class="headerlink" title="Indexes"></a>Indexes</h3><p>We talked about indexes when we saw the B+Trees. Just remember that these <strong>indexes are already sorted</strong>.</p>
<p>FYI, there are other types of indexes like <strong>bitmap indexes</strong>. They don’t offer the same cost in terms of CPU, disk I/O and memory than B+Tree indexes.</p>
<p>Moreover, many modern databases can <strong>dynamically create temporary indexes</strong> just for the current query if it can improve the cost of the execution plan.</p>
<h3 id="Access-Path"><a href="#Access-Path" class="headerlink" title="Access Path"></a>Access Path</h3><p>Before applying your join operators, you first need to get your data. Here is how you can get your data.</p>
<p>Note: Since the real problem with all the access paths is the disk I/O, I won’t talk a lot about time complexity.</p>
<h4 id="Full-scan"><a href="#Full-scan" class="headerlink" title="Full scan"></a>Full scan</h4><p>If you’ve ever read an execution plan you must have seen the word <strong>full scan</strong> (or just scan). A full scan is simply the database reading a table or an index entirely. <strong>In terms of disk I/O, a table full scan is obviously more expensive than an index full scan</strong>.</p>
<h4 id="Range-Scan"><a href="#Range-Scan" class="headerlink" title="Range Scan"></a>Range Scan</h4><p>There are other types of scan like <strong>index range scan</strong>. It is used for example when you use a predicate like “WHERE AGE &gt; 20 AND AGE &lt;40”.</p>
<p>Of course you need have an index on the field AGE to use this index range scan.</p>
<p>We already saw in the first part that the time cost of a range query is something like log(N) +M, where N is the number of data in this index and M an estimation of the number of rows inside this range. <strong>Both N and M values are known thanks to the statistics</strong> (Note: M is the selectivity for the predicate AGE &gt;20 AND AGE&lt;40). Moreover, for a range scan you don’t need to read the full index so it’s <strong>less expensive in terms of disk I/O than a full scan</strong>.</p>
<h4 id="Unique-scan"><a href="#Unique-scan" class="headerlink" title="Unique scan"></a>Unique scan</h4><p>If you only need one value from an index you can use the <strong>unique scan</strong>.</p>
<h4 id="Access-by-row-id"><a href="#Access-by-row-id" class="headerlink" title="Access by row id"></a>Access by row id</h4><p>Most of the time, if the database uses an index, it will have to look for the rows associated to the index. To do so it will use an access by row id.</p>
<p>For example, if you do something like</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;LASTNAME, FIRSTNAME &#96;&#96;from&#96; &#96;PERSON &#96;&#96;WHERE&#96; &#96;AGE &#x3D; 28</span><br></pre></td></tr></table></figure>

<p>If you have an index for person on column age, the optimizer will use the index to find all the persons who are 28 then it will ask for the associate rows in the table because the index only has information about the age and you want to know the lastname and the firstname.</p>
<p>But, if now you do something like</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;TYPE_PERSON.CATEGORY &#96;&#96;from&#96; &#96;PERSON ,TYPE_PERSON&#96;&#96;WHERE&#96; &#96;PERSON.AGE &#x3D; TYPE_PERSON.AGE</span><br></pre></td></tr></table></figure>

<p>The index on PERSON will be used to join with TYPE_PERSON but the table PERSON will not be accessed by row id since you’re not asking information on this table.</p>
<p>Though it works great for a few accesses, the real issue with this operation is the disk I/O. If you need too many accesses by row id the database might choose a full scan.</p>
<h4 id="Others-paths"><a href="#Others-paths" class="headerlink" title="Others paths"></a>Others paths</h4><p>I didn’t present all the access paths. If you want to know more, you can read the <a href="https://docs.oracle.com/database/121/TGSQL/tgsql_optop.htm" target="_blank" rel="noopener">Oracle documentation</a>. The names might not be the same for the other databases but the concepts behind are the same.</p>
<h3 id="Join-operators"><a href="#Join-operators" class="headerlink" title="Join operators"></a>Join operators</h3><p>So, we know how to get our data, let’s join them!</p>
<p>I’ll present the 3 common join operators: Merge Join, Hash Join and Nested Loop Join. But before that, I need to introduce new vocabulary: <strong>inner relation</strong> and <strong>outer relation</strong>. A relation can be:</p>
<ul>
<li>a table</li>
<li>an index</li>
<li>an intermediate result from a previous operation (for example the result of a previous join)</li>
</ul>
<p>When you’re joining two relations, the join algorithms manage the two relations differently. In the rest of the article, I’ll assume that:</p>
<ul>
<li>the outer relation is the left data set</li>
<li>the inner relation is the right data set</li>
</ul>
<p>For example, A JOIN B is the join between A and B where A is the outer relation and B the inner relation.</p>
<p>Most of the time, <strong>the cost of A JOIN B is not the same as the cost of B JOIN A.</strong></p>
<p><strong>In this part, I’ll also assume that the outer relation has N elements</strong> <strong>and the inner relation M elements</strong>. Keep in mind that a real optimizer knows the values of N and M with the statistics.</p>
<p>Note: N and M are the cardinalities of the relations.</p>
<h4 id="Nested-loop-join"><a href="#Nested-loop-join" class="headerlink" title="Nested loop join"></a>Nested loop join</h4><p>The nested loop join is the easiest one.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/nested_loop_join.png" target="_blank" rel="noopener"><img src="../../public/images/nested_loop_join-20200207203251391.png" alt="nested loop join in databases"></a></p>
<p>Here is the idea:</p>
<ul>
<li>for each row in the outer relation</li>
<li>you look at all the rows in the inner relation to see if there are rows that match</li>
</ul>
<p>Here is a pseudo code:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nested_loop_join(array outer, array inner)&#96;&#96; &#96;&#96;for&#96; &#96;each row a in outer&#96;&#96;  &#96;&#96;for&#96; &#96;each row b in inner&#96;&#96;   &#96;&#96;if&#96; &#96;(match_join_condition(a,b))&#96;&#96;    &#96;&#96;write_result_in_output(a,b)&#96;&#96;   &#96;&#96;end &#96;&#96;if&#96;&#96;  &#96;&#96;end &#96;&#96;for&#96;&#96;  &#96;&#96;end &#96;&#96;for</span><br></pre></td></tr></table></figure>

<p>Since it’s a double iteration, the <strong>time complexity is O(N*M)</strong></p>
<p>In term of disk I/O, for each of the N rows in the outer relation, the inner loop needs to read M rows from the inner relation. This algorithm needs to read N + N<em>M rows from disk. But, if the inner relation is small enough, you can put the relation in memory and just have M +N reads. With this modification, *</em>the inner relation must be the smallest one** since it has more chance to fit in memory.</p>
<p>In terms of time complexity it makes no difference but in terms of disk I/O it’s way better to read only once both relations.   </p>
<p>Of course, the inner relation can be replaced by an index, it will be better for the disk I/O.</p>
<p>Since this algorithm is very simple, here is another version that is more disk I/O friendly if the inner relation is too big to fit in memory. Here is the idea:</p>
<ul>
<li>instead of reading both relation row by row,</li>
<li>you read them bunch by bunch and keep 2 bunches of rows (from each relation) in memory,</li>
<li>you compare the rows inside the two bunches and keep the rows that match,</li>
<li>then you load new bunches from disk and compare them</li>
<li>and so on until there are no bunches to load.</li>
</ul>
<p>Here is a possible algorithm:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; improved version to reduce the disk I&#x2F;O.&#96;&#96;nested_loop_join_v2(file outer, file inner)&#96;&#96; &#96;&#96;for&#96; &#96;each bunch ba in outer&#96;&#96; &#96;&#96;&#x2F;&#x2F; ba is now in memory&#96;&#96;  &#96;&#96;for&#96; &#96;each bunch bb in inner&#96;&#96;    &#96;&#96;&#x2F;&#x2F; bb is now in memory&#96;&#96;    &#96;&#96;for&#96; &#96;each row a in ba&#96;&#96;     &#96;&#96;for&#96; &#96;each row b in bb&#96;&#96;      &#96;&#96;if&#96; &#96;(match_join_condition(a,b))&#96;&#96;       &#96;&#96;write_result_in_output(a,b)&#96;&#96;      &#96;&#96;end &#96;&#96;if&#96;&#96;     &#96;&#96;end &#96;&#96;for&#96;&#96;    &#96;&#96;end &#96;&#96;for&#96;&#96;  &#96;&#96;end &#96;&#96;for&#96;&#96;  &#96;&#96;end &#96;&#96;for</span><br></pre></td></tr></table></figure>



<p><strong>With this version, the time complexity remains the same, but the number of disk access decreases</strong>:</p>
<ul>
<li>With the previous version, the algorithm needs N + N*M accesses (each access gets one row).</li>
<li>With this new version, the number of disk accesses becomes number_of_bunches_for(outer)+ number_of_ bunches_for(outer)* number_of_ bunches_for(inner).</li>
<li>If you increase the size of the bunch you reduce the number of disk accesses.</li>
</ul>
<p>Note: Each disk access gathers more data than the previous algorithm but it doesn’t matter since they’re sequential accesses (the real issue with mechanical disks is the time to get the first data).</p>
<h4 id="Hash-join"><a href="#Hash-join" class="headerlink" title="Hash join"></a>Hash join</h4><p>The hash join is more complicated but gives a better cost than a nested loop join in many situations.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/hash_join.png" target="_blank" rel="noopener"><img src="../../public/images/hash_join-20200207203251391.png" alt="hash join in a database"></a></p>
<p>The idea of the hash join is to:</p>
<ul>
<li>1) Get all elements from the inner relation</li>
<li>2) Build an in-memory hash table</li>
<li>3) Get all elements of the outer relation one by one</li>
<li>4) Compute the hash of each element (with the hash function of the hash table) to find the associated bucket of the inner relation</li>
<li>5) find if there is a match between the elements in the bucket and the element of the outer table</li>
</ul>
<p>In terms of time complexity I need to make some assumptions to simplify the problem:</p>
<ul>
<li>The inner relation is divided into X buckets</li>
<li>The hash function distributes hash values almost uniformly for both relations. In other words the buckets are equally sized.</li>
<li>The matching between an element of the outer relation and all elements inside a bucket costs the number of elements inside the buckets.</li>
</ul>
<p>The time complexity is (M/X) * N + cost_to_create_hash_table(M) + cost_of_hash_function*N</p>
<p>If the Hash function creates enough small-sized buckets then <strong>the time complexity is O(M+N)</strong></p>
<p>Here is another version of the hash join which is more memory friendly but less disk I/O friendly. This time:</p>
<ul>
<li>1) you compute the hash tables for both the inner and outer relations</li>
<li>2) then you put them on disk</li>
<li>3) then you compare the 2 relations bucket by bucket (with one loaded in-memory and the other read row by row)</li>
</ul>
<h4 id="Merge-join"><a href="#Merge-join" class="headerlink" title="Merge join"></a>Merge join</h4><p><strong>The merge join is the only join that produces a sorted result.</strong></p>
<p>Note: In this simplified merge join, there are no inner or outer tables; they both play the same role. But real implementations make a difference, for example, when dealing with duplicates.</p>
<p>The merge join can be divided into of two steps:</p>
<ol>
<li>(Optional) Sort join operations: Both the inputs are sorted on the join key(s).</li>
<li>Merge join operation: The sorted inputs are merged together.</li>
</ol>
<p>Sort</p>
<p>We already spoke about the merge sort, in this case a merge sort in a good algorithm (but not the best if memory is not an issue).</p>
<p>But sometimes the data sets are already sorted, for example:</p>
<ul>
<li>If the table is natively ordered, for example an index-organized table on the join condition</li>
<li>If the relation is an index on the join condition</li>
<li>If this join is applied on an intermediate result already sorted during the process of the query</li>
</ul>
<p>Merge join</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/merge_join.png" target="_blank" rel="noopener"><img src="../../public/images/merge_join-20200207203251391.png" alt="merge join in a database"></a></p>
<p>This part is very similar to the merge operation of the merge sort we saw. But this time, instead of picking every element from both relations, we only pick the elements from both relations that are equals. Here is the idea:</p>
<ul>
<li>1) you compare both current elements in the 2 relations (current=first for the first time)</li>
<li>2) if they’re equal, then you put both elements in the result and you go to the next element for both relations</li>
<li>3) if not, you go to the next element for the relation with the lowest element (because the next element might match)</li>
<li>4) and repeat 1,2,3 until you reach the last element of one of the relation.</li>
</ul>
<p>This works because both relations are sorted and therefore you don’t need to “go back” in these relations.</p>
<p>This algorithm is a simplified version because it doesn’t handle the case where the same data appears multiple times in both arrays (in other words a multiple matches). The real version is more complicated “just” for this case; this is why I chose a simplified version.</p>
<p>If both relations are already sorted then <strong>the time complexity is O(N+M)</strong></p>
<p>If both relations need to be sorted then the time complexity is the cost to sort both relations: <strong>O(N*Log(N) + M*Log(M))</strong></p>
<p>For the CS geeks, here is a possible algorithm that handles the multiple matches (note: I’m not 100% sure about my algorithm):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mergeJoin(relation a, relation b)&#96;&#96; &#96;&#96;relation output&#96;&#96; &#96;&#96;integer a_key:&#x3D;&#96;&#96;0&#96;&#96;;&#96;&#96; &#96;&#96;integer b_key:&#x3D;&#96;&#96;0&#96;&#96;;&#96;&#96; &#96; &#96; &#96;&#96;while&#96; &#96;(a[a_key]!&#x3D;&#96;&#96;null&#96; &#96;or b[b_key]!&#x3D;&#96;&#96;null&#96;&#96;)&#96;&#96;  &#96;&#96;if&#96; &#96;(a[a_key] &lt; b[b_key])&#96;&#96;   &#96;&#96;a_key++;&#96;&#96;  &#96;&#96;else&#96; &#96;if&#96; &#96;(a[a_key] &gt; b[b_key])&#96;&#96;   &#96;&#96;b_key++;&#96;&#96;  &#96;&#96;else&#96; &#96;&#x2F;&#x2F;Join predicate satisfied&#96;&#96;  &#96;&#96;&#x2F;&#x2F;i.e. a[a_key] &#x3D;&#x3D; b[b_key]&#96; &#96;   &#96;&#96;&#x2F;&#x2F;count the number of duplicates in relation a&#96;&#96;   &#96;&#96;integer nb_dup_in_a &#x3D; &#96;&#96;1&#96;&#96;:&#96;&#96;   &#96;&#96;while&#96; &#96;(a[a_key]&#x3D;&#x3D;a[a_key+nb_dup_in_a])&#96;&#96;    &#96;&#96;nb_dup_in_a++;&#96;&#96;    &#96; &#96;   &#96;&#96;&#x2F;&#x2F;count the number of duplicates in relation b&#96;&#96;   &#96;&#96;integer dup_in_b &#x3D; &#96;&#96;1&#96;&#96;:&#96;&#96;   &#96;&#96;while&#96; &#96;(b[b_key]&#x3D;&#x3D;b[b_key+nb_dup_in_b])&#96;&#96;    &#96;&#96;nb_dup_in_b++;&#96;&#96;    &#96; &#96;   &#96;&#96;&#x2F;&#x2F;write the duplicates in output&#96;&#96;    &#96;&#96;for&#96; &#96;(&#96;&#96;int&#96; &#96;i &#x3D; &#96;&#96;0&#96; &#96;; i&lt; nb_dup_in_a ; i++)&#96;&#96;     &#96;&#96;for&#96; &#96;(&#96;&#96;int&#96; &#96;j &#x3D; &#96;&#96;0&#96; &#96;; i&lt; nb_dup_in_b ; i++)   &#96;&#96;      &#96;&#96;write_result_in_output(a[a_key+i],b[b_key+j])&#96;&#96;      &#96; &#96;   &#96;&#96;a_key&#x3D;a_key + nb_dup_in_a-&#96;&#96;1&#96;&#96;;&#96;&#96;   &#96;&#96;b_key&#x3D;b_key + nb_dup_in_b-&#96;&#96;1&#96;&#96;;&#96; &#96;  &#96;&#96;end &#96;&#96;if&#96;&#96; &#96;&#96;end &#96;&#96;while</span><br></pre></td></tr></table></figure>



<h4 id="Which-one-is-the-best"><a href="#Which-one-is-the-best" class="headerlink" title="Which one is the best?"></a>Which one is the best?</h4><p>If there was a best type of joins, there wouldn’t be multiple types. This question is very difficult because many factors come into play like:</p>
<ul>
<li>The <strong>amount of free memory</strong>: without enough memory you can say goodbye to the powerful hash join (at least the full in-memory hash join)</li>
<li>The <strong>size of the 2 data sets</strong>. For example if you have a big table with a very small one, a nested loop join will be faster than a hash join because the hash join has an expensive creation of hashes. If you have 2 very large tables the nested loop join will be very CPU expensive.</li>
<li>The <strong>presence</strong> <strong>of</strong> <strong>indexes</strong>. With 2 B+Tree indexes the smart choice seems to be the merge join</li>
<li>If <strong>the result need to be sorted</strong>: Even if you’re working with unsorted data sets, you might want to use a costly merge join (with the sorts) because at the end the result will be sorted and you’ll be able to chain the result with another merge join (or maybe because the query asks implicitly/explicitly for a sorted result with an ORDER BY/GROUP BY/DISTINCT operation)</li>
<li>If <strong>the relations are already sorted</strong>: In this case the merge join is the best candidate</li>
<li>The type of joins you’re doing: is it an <strong>equijoin</strong> (i.e.: tableA.col1 = tableB.col2)? Is it an <strong>inner join</strong>, an <strong>outer join,</strong> a <strong>cartesian product</strong> or a <strong>self-join</strong>? Some joins can’t work in certain situations.</li>
<li>The <strong>distribution of data</strong>. If the data on the join condition are <strong>skewed</strong> (For example you’re joining people on their last name but many people have the same), using a hash join will be a disaster because the hash function will create ill-distributed buckets.</li>
<li>If you want the join to be executed by <strong>multiple threads/process</strong></li>
</ul>
<p>For more information, you can read the <a href="https://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.admin.perf.doc/doc/c0005311.html" target="_blank" rel="noopener">DB2</a>, <a href="http://docs.oracle.com/cd/B28359_01/server.111/b28274/optimops.htm#i76330" target="_blank" rel="noopener">ORACLE</a> or <a href="https://technet.microsoft.com/en-us/library/ms191426(v=sql.105).aspx" target="_blank" rel="noopener">SQL Server</a> documentations.</p>
<h3 id="Simplified-example"><a href="#Simplified-example" class="headerlink" title="Simplified example"></a>Simplified example</h3><p>We’ve just seen 3 types of join operations.</p>
<p>Now let’s say we need to join 5 tables to have a full view of a person. A PERSON can have:</p>
<ul>
<li>multiple MOBILES</li>
<li>multiple MAILS</li>
<li>multiple ADRESSES</li>
<li>multiple BANK_ACCOUNTS</li>
</ul>
<p>In other words we need a quick answer for the following query:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;* &#96;&#96;from&#96; &#96;PERSON, MOBILES, MAILS,ADRESSES, BANK_ACCOUNTS&#96;&#96;WHERE&#96;&#96;PERSON.PERSON_ID &#x3D; MOBILES.PERSON_ID&#96;&#96;AND&#96; &#96;PERSON.PERSON_ID &#x3D; MAILS.PERSON_ID&#96;&#96;AND&#96; &#96;PERSON.PERSON_ID &#x3D; ADRESSES.PERSON_ID&#96;&#96;AND&#96; &#96;PERSON.PERSON_ID &#x3D; BANK_ACCOUNTS.PERSON_ID</span><br></pre></td></tr></table></figure>

<p>As a query optimizer, I have to find the best way to process the data. But there are 2 problems:</p>
<ul>
<li>What kind of join should I use for each join?</li>
</ul>
<p>I have 3 possible joins (Hash Join, Merge Join, Nested Join) with the possibility to use 0,1 or 2 indexes (not to mention that there are different types of indexes).</p>
<ul>
<li>What order should I choose to compute the join?</li>
</ul>
<p>For example, the following figure shows different possible plans for only 3 joins on 4 tables</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/join_ordering_problem.png" target="_blank" rel="noopener"><img src="../../public/images/join_ordering_problem-20200207203251393.png" alt="join ordering optimization problem in a database"></a></p>
<p>So here are my possibilities:</p>
<ul>
<li>1) I use a brute force approach</li>
</ul>
<p>Using the database statistics, I <strong>compute the cost for every possible plan</strong> and I keep the best one. But there are many possibilities. For a given order of joins, each join has 3 possibilities: HashJoin, MergeJoin, NestedJoin. So, for a given order of joins there are 34 possibilities. The join ordering is a <a href="https://en.wikipedia.org/wiki/Catalan_number" target="_blank" rel="noopener">permutation problem on a binary tree</a> and there are (2<em>4)!/(4+1)! possible orders. For this very simplified problem, I end up with 34</em>(2*4)!/(4+1)! possibilities.</p>
<p>In non-geek terms, it means 27 216 possible plans. If I now add the possibility for the merge join to take 0,1 or 2 B+Tree indexes, the number of possible plans becomes 210 000. Did I forget to mention that this query is VERY SIMPLE?</p>
<ul>
<li>2) I cry and quit this job</li>
</ul>
<p>It’s very tempting but you wouldn’t get your result and I need money to pay the bills.</p>
<ul>
<li>3) I only try a few plans and take the one with the lowest cost.</li>
</ul>
<p>Since I’m not superman, I can’t compute the cost of every plan. Instead, I can <strong>arbitrary choose a subset of all the possible plans</strong>, compute their costs and give you the best plan of this subset.</p>
<ul>
<li>4) I apply smart <strong>rules to reduce the number of possible plans</strong>.</li>
</ul>
<p>There are 2 types of rules:</p>
<p>I can use “logical” rules that will remove useless possibilities but they won’t filter a lot of possible plans. For example: “the inner relation of the nested loop join must be the smallest data set”</p>
<p>I accept not finding the best solution and apply more aggressive rules to reduce a lot the number of possibilities. For example “If a relation is small, use a nested loop join and never use a merge join or a hash join”</p>
<p>In this simple example, I end up with many possibilities. But <strong>a real query can have other relational operators</strong> like OUTER JOIN, CROSS JOIN, GROUP BY, ORDER BY, PROJECTION, UNION, INTERSECT, DISTINCT … <strong>which means even more possibilities</strong>.</p>
<p>So, how a database does it?</p>
<h3 id="Dynamic-programming-greedy-algorithm-and-heuristic"><a href="#Dynamic-programming-greedy-algorithm-and-heuristic" class="headerlink" title="Dynamic programming, greedy algorithm and heuristic"></a>Dynamic programming, greedy algorithm and heuristic</h3><p>A relational database tries the multiple approaches I’ve just said. The real job of an optimizer is to find a good solution on a limited amount of time.</p>
<p><strong>Most of the time an optimizer doesn’t find the best solution but a “good” one</strong>.</p>
<p>For small queries, doing a brute force approach is possible. But there is a way to avoid unnecessary computations so that even medium queries can use the brute force approach. This is called dynamic programming.</p>
<h4 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a>Dynamic Programming</h4><p>The idea behind these 2 words is that many executions plan are very similar. If you look at the following plans:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/overlapping_trees.png" target="_blank" rel="noopener"><img src="../../public/images/overlapping_trees-20200207203251396.png" alt="overlapping trees optimization dynamic programming"></a></p>
<p>They share the same (A JOIN B) subtree. So, instead of computing the cost of this subtree in every plan, we can compute it once, save the computed cost and reuse it when we see this subtree again. More formally, we’re facing an overlapping problem. To avoid the extra-computation of the partial results we’re using memoization.</p>
<p>Using this technique, instead of having a (2<em>N)!/(N+1)! time complexity, we “just” have 3N. In our previous example with 4 joins, it means passing from 336 ordering to 81. If you take a bigger *</em>query with 8 joins** (which is not big)<strong>, it means passing from 57 657 600 to 6561</strong>.</p>
<p>For the CS geeks, here is an algorithm I found on the <a href="http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch13.ppt" target="_blank" rel="noopener">formal course I already gave you</a>. I won’t explain this algorithm so read it only if you already know dynamic programming or if you’re good with algorithms (you’ve been warned!):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">procedure findbestplan(S)&#96;&#96;if&#96; &#96;(bestplan[S].cost infinite)&#96;&#96;  &#96;&#96;return&#96; &#96;bestplan[S]&#96;&#96;&#x2F;&#x2F; else bestplan[S] has not been computed earlier, compute it now&#96;&#96;if&#96; &#96;(S contains only &#96;&#96;1&#96; &#96;relation)&#96;&#96;     &#96;&#96;set bestplan[S].plan and bestplan[S].cost based on the best way&#96;&#96;     &#96;&#96;of accessing S &#96;&#96;&#x2F;* Using selections on S and indices on S *&#x2F;&#96;&#96;   &#96;&#96;else&#96; &#96;for&#96; &#96;each non-empty subset S1 of S such that S1 !&#x3D; S&#96;&#96;  &#96;&#96;P1&#x3D; findbestplan(S1)&#96;&#96;  &#96;&#96;P2&#x3D; findbestplan(S - S1)&#96;&#96;  &#96;&#96;A &#x3D; best algorithm &#96;&#96;for&#96; &#96;joining results of P1 and P2&#96;&#96;  &#96;&#96;cost &#x3D; P1.cost + P2.cost + cost of A&#96;&#96;  &#96;&#96;if&#96; &#96;cost &lt; bestplan[S].cost&#96;&#96;    &#96;&#96;bestplan[S].cost &#x3D; cost&#96;&#96;   &#96;&#96;bestplan[S].plan &#x3D; “execute P1.plan; execute P2.plan;&#96;&#96;         &#96;&#96;join results of P1 and P2 using A”&#96;&#96;return&#96; &#96;bestplan[S]</span><br></pre></td></tr></table></figure>



<p>For bigger queries you can still do a dynamic programming approach but with extra rules (or <strong>heuristics</strong>) to remove possibilities:</p>
<ul>
<li>If we analyze only a certain type of plan (for example: the left-deep trees) we end up with n*2n instead of 3n</li>
</ul>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/left-deep-tree.png" target="_blank" rel="noopener"><img src="../../public/images/left-deep-tree-20200207203251418.png" alt="left deep tree example"></a></p>
<ul>
<li>If we add logical rules to avoid plans for some patterns (like “if a table as an index for the given predicate, don’t try a merge join on the table but only on the index”) it will reduce the number of possibilities without hurting to much the best possible solution.</li>
<li>If we add rules on the flow (like “perform the join operations BEFORE all the other relational operations”) it also reduces a lot of possibilities.</li>
<li>…</li>
</ul>
<h4 id="Greedy-algorithms"><a href="#Greedy-algorithms" class="headerlink" title="Greedy algorithms"></a>Greedy algorithms</h4><p>But for a very big query or to have a very fast answer (but not a very fast query), another type of algorithms is used, the greedy algorithms.</p>
<p>The idea is to follow a rule (or <strong>heuristic</strong>) to build a query plan in an incremental way. With this rule, a greedy algorithm finds the best solution to a problem one step at a time. The algorithm starts the query plan with one JOIN. Then, at each step, the algorithm adds a new JOIN to the query plan using the same rule.</p>
<p>Let’s take a simple example. Let’s say we have a query with 4 joins on 5 tables (A, B, C, D and E). To simplify the problem we just take the nested join as a possible join. Let’s use the rule “use the join with the lowest cost”</p>
<ul>
<li>we arbitrary start on one of the 5 tables (let’s choose A)</li>
<li>we compute the cost of every join with A (A being the inner or outer relation).</li>
<li>we find that A JOIN B gives the lowest cost.</li>
<li>we then compute the cost of every join with the result of A JOIN B (A JOIN B being the inner or outer relation).</li>
<li>we find that (A JOIN B) JOIN C gives the best cost.</li>
<li>we then compute the cost of every join with the result of the (A JOIN B) JOIN C …</li>
<li>….</li>
<li>At the end we find the plan (((A JOIN B) JOIN C) JOIN D) JOIN E)</li>
</ul>
<p>Since we arbitrary started with A, we can apply the same algorithm for B, then C then D then E. We then keep the plan with the lowest cost.</p>
<p>By the way, this algorithm has a name: it’s called the <a href="https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm" target="_blank" rel="noopener">Nearest neighbor algorithm</a>.</p>
<p>I won’t go into details, but with a good modeling and a sort in N<em>log(N) this problem can <a href="http://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&cad=rja&uact=8&ved=0CE0QFjAEahUKEwjR8OLUmv3GAhUJuxQKHdU-DAA&url=http%3A%2F%2Fwww.cs.bu.edu%2F~steng%2Fteaching%2FSpring2004%2Flectures%2Flecture3.ppt&ei=hyK3VZGRAYn2UtX9MA&usg=AFQjCNGL41kMNkG5cH" target="_blank" rel="noopener">easily be solved</a>. The *</em>cost of this algorithm is in O(N*log(N)) vs O(3N) for the full dynamic programming version**. If you have a big query with 20 joins, it means 26 vs 3 486 784 401, a BIG difference!</p>
<p>The problem with this algorithm is that we assume that finding the best join between 2 tables will give us the best cost if we keep this join and add a new join. But:</p>
<ul>
<li>even if A JOIN B gives the best cost between A, B and C</li>
<li>(A JOIN C) JOIN B might give a better result than (A JOIN B) JOIN C.</li>
</ul>
<p>To improve the result, you can run multiple greedy algorithms using different rules and keep the best plan.</p>
<h4 id="Other-algorithms"><a href="#Other-algorithms" class="headerlink" title="Other algorithms"></a>Other algorithms</h4><p>[If you’re already fed up with algorithms, skip to the next part, what I’m going to say is not important for the rest of the article]</p>
<p>The problem of finding the best possible plan is an active research topic for many CS researchers. They often try to find better solutions for more precise problems/patterns. For example,</p>
<ul>
<li>if the query is a star join (it’s a certain type of multiple-join query), some databases will use a specific algorithm.</li>
<li>if the query is a parallel query, some databases will use a specific algorithm</li>
<li>…</li>
</ul>
<p>Other algorithms are also studied to replace dynamic programming for large queries. Greedy algorithms belong to larger family called <strong>heuristic algorithms</strong>. A greedy algorithm follows a rule (or heuristic), keeps the solution it found at the previous step and “appends” it to find the solution for the current step. Some algorithms follow a rule and apply it in a step-by-step way but don’t always keep the best solution found in the previous step. They are called heuristic algorithms.</p>
<p>For example, <strong>genetic algorithms</strong> follow a rule but the best solution of the last step is not often kept:</p>
<ul>
<li>A solution represents a possible full query plan</li>
<li>Instead of one solution (i.e. plan) there are P solutions (i.e. plans) kept at each step.</li>
<li>0) P query plans are randomly created</li>
<li>1) Only the plans with the best costs are kept</li>
<li>2) These best plans are mixed up to produce P news plans</li>
<li>3) Some of the P new plans are randomly modified</li>
<li>4) The step 1,2,3 are repeated T times</li>
<li>5) Then you keep the best plan from the P plans of the last loop.</li>
</ul>
<p>The more loops you do the better the plan will be.</p>
<p>Is it magic? No, it’s the laws of nature: only the fittest survives!</p>
<p>FYI, genetic algorithms are implemented in <a href="http://www.postgresql.org/docs/9.4/static/geqo-intro.html" target="_blank" rel="noopener">PostgreSQL</a> but I wasn’t able to find if they’re used by default.</p>
<p>There are other heuristic algorithms used in databases like Simulated Annealing, Iterative Improvement, Two-Phase Optimization… But I don’t know if they’re currently used in enterprise databases or if they’re only used in research databases.</p>
<p>For more information, you can read the following research article that presents more possible algorithms: <a href="http://www.acad.bg/rismim/itc/sub/archiv/Paper6_1_2009.PDF" target="_blank" rel="noopener">Review of Algorithms for the Join Ordering Problem in Database Query Optimization</a></p>
<h3 id="Real-optimizers"><a href="#Real-optimizers" class="headerlink" title="Real optimizers"></a>Real optimizers</h3><p>[You can skip to the next part, what I’m going to say is not important]</p>
<p>But, all this blabla is very theoretical. Since I’m a developer and not a researcher, I like <strong>concrete examples</strong>.</p>
<p>Let’s see how the <a href="https://www.sqlite.org/optoverview.html" target="_blank" rel="noopener">SQLite optimizer</a> works. It’s a light database so it uses a simple optimization based on a greedy algorithm with extra-rules to limit the number of possibilities:</p>
<ul>
<li>SQLite chooses to never reorder tables in a CROSS JOIN operator</li>
<li><strong>joins are implemented as nested joins</strong></li>
<li>outer joins are always evaluated in the order in which they occur</li>
<li>…</li>
<li>Prior to version 3.8.0, <strong>SQLite uses the “Nearest Neighbor” greedy algorithm when searching for the best query plan</strong></li>
</ul>
<p>Wait a minute … we’ve already seen this algorithm! What a coincidence!</p>
<ul>
<li>Since version 3.8.0 (released in 2015), SQLite uses the “<a href="https://www.sqlite.org/queryplanner-ng.html" target="_blank" rel="noopener">N Nearest Neighbors</a>” <strong>greedy algorithm</strong> when searching for the best query plan</li>
</ul>
<p>Let’s see how another optimizer does his job. IBM DB2 is like all the enterprise databases but I’ll focus on this one since it’s the last one I’ve really used before switching to Big Data.</p>
<p>If we look at the <a href="https://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.admin.perf.doc/doc/r0005278.html" target="_blank" rel="noopener">official documentation</a>, we learn that the DB2 optimizer let you use 7 different levels of optimization:</p>
<ul>
<li>Use greedy algorithms for the joins<ul>
<li>0 – minimal optimization, use index scan and nested-loop join and avoid some Query Rewrite</li>
<li>1 – low optimization</li>
<li>2 – full optimization</li>
</ul>
</li>
<li>Use dynamic programming for the joins<ul>
<li>3 – moderate optimization and rough approximation</li>
<li>5 – full optimization, uses all techniques with heuristics</li>
<li>7 – full optimization similar to 5, without heuristics</li>
<li>9 – maximal optimization spare no effort/expense <strong>considers all possible join orders, including Cartesian products</strong></li>
</ul>
</li>
</ul>
<p>We can see that <strong>DB2 uses greedy algorithms and dynamic programming</strong>. Of course, they don’t share the heuristics they use since the query optimizer is the main power of a database.</p>
<p>FYI, <strong>the default level is 5.</strong> By default the optimizer uses the following characteristics:</p>
<ul>
<li><p><strong>All available statistics</strong>, including frequent-value and quantile statistics, are used.</p>
</li>
<li><p><strong>All query rewrite rules</strong> (including materialized query table routing) are applied, except computationally intensive rules that are applicable only in very rare cases.</p>
</li>
<li><p>Dynamic programming join enumeration</p>
</li>
</ul>
<p>  is used, with:</p>
<ul>
<li>Limited use of composite inner relation</li>
<li>Limited use of Cartesian products for star schemas involving lookup tables</li>
</ul>
<ul>
<li>A wide range of access methods is considered, including list prefetch (note: will see what is means), index ANDing (note: a special operation with indexes), and materialized query table routing.</li>
</ul>
<p>By default, <strong>DB2 uses dynamic programming limited by heuristics for the join ordering</strong>.</p>
<p>The others conditions (GROUP BY, DISTINCT…) are handled by simple rules.</p>
<h3 id="Query-Plan-Cache"><a href="#Query-Plan-Cache" class="headerlink" title="Query Plan Cache"></a>Query Plan Cache</h3><p>Since the creation of a plan takes time, most databases store the plan into a <strong>query plan cache</strong> to avoid useless re-computations of the same query plan. It’s kind of a big topic since the database needs to know when to update the outdated plans. The idea is to put a threshold and if the statistics of a table have changed above this threshold then the query plan involving this table is purged from the cache.</p>
<h2 id="Query-executor"><a href="#Query-executor" class="headerlink" title="Query executor"></a>Query executor</h2><p>At this stage we have an optimized execution plan. This plan is compiled to become an executable code. Then, if there are enough resources (memory, CPU) it is executed by the query executor. The operators in the plan (JOIN, SORT BY …) can be executed in a sequential or parallel way; it’s up to the executor. To get and write its data, the query executor interacts with the data manager, which is the next part of the article.</p>
<h1 id="Data-manager"><a href="#Data-manager" class="headerlink" title="Data manager"></a>Data manager</h1><p><a href="http://coding-geek.com/wp-content/uploads/2015/08/data_manager.png" target="_blank" rel="noopener"><img src="../../public/images/data_manager-20200207203251432.png" alt="data manager in databases"></a></p>
<p>At this step, the query manager is executing the query and needs the data from the tables and indexes. It asks the data manager to get the data, but there are 2 problems:</p>
<ul>
<li>Relational databases use a transactional model. So, you can’t get any data at any time because someone else might be using/modifying the data at the same time.</li>
<li><strong>Data retrieval is the slowest operation in a database</strong>, therefore the data manager needs to be smart enough to get and keep data in memory buffers.</li>
</ul>
<p>In this part, we’ll see how relational databases handle these 2 problems. I won’t talk about the way the data manager gets its data because it’s not the most important (and this article is long enough!).</p>
<h2 id="Cache-manager"><a href="#Cache-manager" class="headerlink" title="Cache manager"></a>Cache manager</h2><p>As I already said, the main bottleneck of databases is disk I/O. To improve performance, modern databases use a cache manager.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/cache_manager.png" target="_blank" rel="noopener"><img src="../../public/images/cache_manager-20200207203251433.png" alt="cache manager in databases"></a></p>
<p>Instead of directly getting the data from the file system, the query executor asks for the data to the cache manager. The cache manager has an in-memory cache called <strong>buffer pool</strong>. <strong>Getting data from memory dramatically speeds up a database</strong>. It’s difficult to give an order of magnitude because it depends on the operation you need to do:</p>
<ul>
<li>sequential access (ex: full scan) vs random access (ex: access by row id),</li>
<li>read vs write</li>
</ul>
<p>and the type of disks used by the database:</p>
<ul>
<li>7.2k/10k/15k rpm HDD</li>
<li>SSD</li>
<li>RAID 1/5/…</li>
</ul>
<p>but I’d say <strong>memory is 100 to 100k times faster than disk</strong>.</p>
<p>But, this leads to another problem (as always with databases…). The cache manager needs to get the data in memory BEFORE the query executor uses them; otherwise the query manager has to wait for the data from the slow disks.</p>
<h3 id="Prefetching"><a href="#Prefetching" class="headerlink" title="Prefetching"></a>Prefetching</h3><p>This problem is called prefetching. A query executor knows the data it’ll need because it knows the full flow of the query and has knowledge of the data on disk with the statistics. Here is the idea:</p>
<ul>
<li>When the query executor is processing its first bunch of data</li>
<li>It asks the cache manager to pre-load the second bunch of data</li>
<li>When it starts processing the second bunch of data</li>
<li>It asks the CM to pre-load the third bunch and informs the CM that the first bunch can be purged from cache.</li>
<li>…</li>
</ul>
<p>The CM stores all these data in its buffer pool. In order to know if a data is still needed, the cache manager adds an extra-information about the cached data (called a <strong>latch</strong>).</p>
<p>Sometimes the query executor doesn’t know what data it’ll need and some databases don’t provide this functionality. Instead, they use a speculative prefetching (for example: if the query executor asked for data 1,3,5 it’ll likely ask for 7,9,11 in a near future) or a sequential prefetching (in this case the CM simply loads from disks the next contiguous data after the ones asked).</p>
<p>To monitor how well the prefetching is working, modern databases provide a metric called <strong>buffer/cache hit ratio</strong>. The hit ratio shows how often a requested data has been found in the buffer cache without requiring disk access.</p>
<p>Note: a poor cache hit ratio doesn’t always mean that the cache is ill-working. For more information, you can read the <a href="http://docs.oracle.com/database/121/TGDBA/tune_buffer_cache.htm" target="_blank" rel="noopener">Oracle documentation</a>.</p>
<p>But, a buffer is a <strong>limited</strong> amount of memory. Therefore, it needs to remove some data to be able to load new ones. Loading and purging the cache has a cost in terms of disk and network I/O. If you have a query that is often executed, it wouldn’t be efficient to always load then purge the data used by this query. To handle this problem, modern databases use a buffer replacement strategy.</p>
<h3 id="Buffer-Replacement-strategies"><a href="#Buffer-Replacement-strategies" class="headerlink" title="Buffer-Replacement strategies"></a>Buffer-Replacement strategies</h3><p>Most modern databases (at least SQL Server, MySQL, Oracle and DB2) use an LRU algorithm.</p>
<h4 id="LRU"><a href="#LRU" class="headerlink" title="LRU"></a>LRU</h4><p><strong>LRU</strong> stands for <strong>L</strong>east <strong>R</strong>ecently <strong>U</strong>sed. The idea behind this algorithm is to keep in the cache the data that have been recently used and, therefore, are more likely to be used again.</p>
<p>Here is a visual example:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/LRU.png" target="_blank" rel="noopener"><img src="../../public/images/LRU-20200207203251433.png" alt="LRU algorithm in a database"></a></p>
<p>For the sake of comprehension, I’ll assume that the data in the buffer are not locked by latches (and therefore can be removed). In this simple example the buffer can store 3 elements:</p>
<ul>
<li>1: the cache manager uses the data 1 and puts the data into the empty buffer</li>
<li>2: the CM uses the data 4 and puts the data into the half-loaded buffer</li>
<li>3: the CM uses the data 3 and puts the data into the half-loaded buffer</li>
<li>4: the CM uses the data 9. The buffer is full so <strong>data 1 is removed</strong> <strong>since it’s the last recently used data</strong>. Data 9 is added into the buffer</li>
<li>5: the CM uses the data 4. <strong>Data 4 is already in the buffer therefore it becomes the first recently used data again</strong>.</li>
<li>6: the CM uses the data 1. The buffer is full so <strong>data 9 is removed</strong> <strong>since it’s the last recently used data</strong>. Data 1 is added into the buffer</li>
<li>…</li>
</ul>
<p>This algorithm works well but there are some limitations. What if there is a full scan on a large table? In other words, what happens when the size of the table/index is above the size of the buffer? Using this algorithm will remove all the previous values in the cache whereas the data from the full scan are likely to be used only once.</p>
<h4 id="Improvements"><a href="#Improvements" class="headerlink" title="Improvements"></a>Improvements</h4><p>To prevent this to happen, some databases add specific rules. For example according to <a href="http://docs.oracle.com/database/121/CNCPT/memory.htm#i10221" target="_blank" rel="noopener">Oracle documentation</a>:</p>
<blockquote>
<p>“For very large tables, the database typically uses a direct path read, which loads blocks directly […], to avoid populating the buffer cache. For medium size tables, the database may use a direct read or a cache read. If it decides to use a cache read, then the database places the blocks at the end of the LRU list to prevent the scan from effectively cleaning out the buffer cache.”</p>
</blockquote>
<p>There are other possibilities like using an advanced version of LRU called LRU-K. For example SQL Server uses LRU-K for K =2.</p>
<p>This idea behind this algorithm is to take into account more history. With the simple LRU (which is also LRU-K for K=1), the algorithm only takes into account the last time the data was used. With the LRU-K:</p>
<ul>
<li>It takes into account the <strong>K last times the data was used</strong>.</li>
<li><strong>A weight is put</strong> on the number of times the data was used</li>
<li>If a bunch of new data is loaded into the cache, the old but often used data are not removed (because their weights are higher).</li>
<li>But the algorithm can’t keep old data in the cache if they aren’t used anymore.</li>
<li>So the <strong>weights decrease</strong> <strong>over time if the data is not used</strong>.</li>
</ul>
<p>The computation of the weight is costly and this is why SQL Server only uses K=2. This value performs well for an acceptable overhead.</p>
<p>For a more in-depth knowledge of LRU-K, you can read the original research paper (1993): <a href="http://www.cs.cmu.edu/~christos/courses/721-resources/p297-o_neil.pdf" target="_blank" rel="noopener">The LRU-K page replacement algorithm for database disk buffering</a>.</p>
<h4 id="Other-algorithms-1"><a href="#Other-algorithms-1" class="headerlink" title="Other algorithms"></a>Other algorithms</h4><p>Of course there are other algorithms to manage cache like</p>
<ul>
<li>2Q (a LRU-K like algorithm)</li>
<li>CLOCK (a LRU-K like algorithm)</li>
<li>MRU (most recently used, uses the same logic than LRU but with another rule)</li>
<li>LRFU (Least Recently and Frequently Used)</li>
<li>…</li>
</ul>
<p>Some databases let the possibility to use another algorithm than the default one.</p>
<h3 id="Write-buffer"><a href="#Write-buffer" class="headerlink" title="Write buffer"></a>Write buffer</h3><p>I only talked about read buffers that load data before using them. But in a database you also have write buffers that store data and flush them on disk by bunches instead of writing data one by one and producing many single disk accesses.</p>
<p>Keep in mind that buffers store <strong>pages</strong> (the smallest unit of data) and not rows (which is a logical/human way to see data). A page in a buffer pool is <strong>dirty</strong> if the page has been modified and not written on disk. There are multiple algorithms to decide the best time to write the dirty pages on disk but it’s highly linked to the notion of transaction, which is the next part of the article.</p>
<h2 id="Transaction-manager"><a href="#Transaction-manager" class="headerlink" title="Transaction manager"></a>Transaction manager</h2><p>Last but not least, this part is about the transaction manager. We’ll see how this process ensures that each query is executed in its own transaction. But before that, we need to understand the concept of ACID transactions.</p>
<h3 id="I’m-on-acid"><a href="#I’m-on-acid" class="headerlink" title="I’m on acid"></a>I’m on acid</h3><p>An ACID transaction is a <strong>unit of work</strong> that ensures 4 things:</p>
<ul>
<li><strong>Atomicity</strong>: the transaction is “all or nothing”, even if it lasts 10 hours. If the transaction crashes, the state goes back to before the transaction (the transaction is <strong>rolled back</strong>).</li>
<li><strong>Isolation</strong>: if 2 transactions A and B run at the same time, the result of transactions A and B must be the same whether A finishes before/after/during transaction B.</li>
<li><strong>Durability</strong>: once the transaction is <strong>committed</strong> (i.e. ends successfully), the data stay in the database no matter what happens (crash or error).</li>
<li><strong>Consistency</strong>: only valid data (in terms of relational constraints and functional constraints) are written to the database. The consistency is related to atomicity and isolation.</li>
</ul>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/dollar_low.jpg" target="_blank" rel="noopener"><img src="../../public/images/dollar_low-20200207203251437.jpg" alt="one dollar"></a></p>
<p>During the same transaction, you can run multiple SQL queries to read, create, update and delete data. The mess begins when two transactions are using the same data. The classic example is a money transfer from an account A to an account B. Imagine you have 2 transactions:</p>
<ul>
<li>Transaction 1 that takes 100$ from account A and gives them to account B</li>
<li>Transaction 2 that takes 50$ from account A and gives them to account B</li>
</ul>
<p>If we go back to the <strong>ACID</strong> properties:</p>
<ul>
<li><p><strong>Atomicity</strong> ensures that no matter what happens during T1 (a server crash, a network failure …), you can’t end up in a situation where the 100$ are withdrawn from A and not given to B (this case is an inconsistent state).</p>
</li>
<li><p>I<strong>solation</strong> ensures that if T1 and T2 happen at the same time, in the end A will be taken 150$ and B given 150$ and not, for example, A taken 150$ and B given just $50 because T2 has partially erased the actions of T1 (this case is also an inconsistent state).</p>
</li>
<li><p><strong>Durability</strong> ensures that T1 won’t disappear into thin air if the database crashes just after T1 is committed.</p>
</li>
<li><p><strong>Consistency</strong> ensures that no money is created or destroyed in the system.</p>
</li>
</ul>
<p>[You can skip to the next part if you want, what I’m going to say is not important for the rest of the article]</p>
<p>Many modern databases don’t use a pure isolation as a default behavior because it comes with a huge performance overhead. The SQL norm defines 4 levels of isolation:</p>
<ul>
<li><p><strong>Serializable</strong> (default behaviour in SQLite): The highest level of isolation. Two transactions happening at the same time are 100% isolated. Each transaction has its own “world”.</p>
</li>
<li><p><strong>Repeatable read</strong> (default behavior in MySQL): Each transaction has its own “world” except in one situation. If a transaction ends up successfully and adds new data, these data will be visible in the other and still running transactions. But if A modifies a data and ends up successfully, the modification won’t be visible in the still running transactions. So, this break of isolation between transactions is only about new data, not the existing ones.</p>
</li>
</ul>
<p>For example, if a transaction A does a “SELECT count(1) from TABLE_X” and then a new data is added and committed in TABLE_X by Transaction B, if transaction A does again a count(1) the value won’t be the same.</p>
<p>This is called a <strong>phantom read</strong>.</p>
<ul>
<li><strong>Read committed</strong> (default behavior in Oracle, PostgreSQL and SQL Server): It’s a repeatable read + a new break of isolation. If a transaction A reads a data D and then this data is modified (or deleted) and committed by a transaction B, if A reads data D again it will see the modification (or deletion) made by B on the data.</li>
</ul>
<p>This is called a <strong>non-repeatable read</strong>.</p>
<ul>
<li><strong>Read uncommitted</strong>: the lowest level of isolation. It’s a read committed + a new break of isolation. If a transaction A reads a data D and then this data D is modified by a transaction B (that is not committed and still running), if A reads data D again it will see the modified value. If transaction B is rolled back, then data D read by A the second time doesn’t make no sense since it has been modified by a transaction B that never happened (since it was rolled back).</li>
</ul>
<p>This is called a <strong>dirty read</strong>.</p>
<p>Most databases add their own custom levels of isolation (like the snapshot isolation used by PostgreSQL, Oracle and SQL Server). Moreover, most databases don’t implement all the levels of the SQL norm (especially the read uncommitted level).</p>
<p>The default level of isolation can be overridden by the user/developer at the beginning of the connection (it’s a very simple line of code to add).</p>
<h3 id="Concurrency-Control"><a href="#Concurrency-Control" class="headerlink" title="Concurrency Control"></a>Concurrency Control</h3><p>The real issue to ensure isolation, coherency and atomicity is the <strong>write operations on the same data</strong> (add, update and delete):</p>
<ul>
<li>if all transactions are only reading data, they can work at the same time without modifying the behavior of another transaction.</li>
<li>if (at least) one of the transactions is modifying a data read by other transactions, the database needs to find a way to hide this modification from the other transactions. Moreover, it also needs to ensure that this modification won’t be erased by another transaction that didn’t see the modified data.</li>
</ul>
<p>This problem is a called <strong>concurrency control</strong>.</p>
<p>The easiest way to solve this problem is to run each transaction one by one (i.e. sequentially). But that’s not scalable at all and only one core is working on the multi-processor/core server, not very efficient…</p>
<p>The ideal way to solve this problem is, every time a transaction is created or cancelled:</p>
<ul>
<li>to monitor all the operations of all the transactions</li>
<li>to check if the parts of 2 (or more) transactions are in conflict because they’re reading/modifying the same data.</li>
<li>to reorder the operations inside the conflicting transactions to reduce the size of the conflicting parts</li>
<li>to execute the conflicting parts in a certain order (while the non-conflicting transactions are still running concurrently).</li>
<li>to take into account that a transaction can be cancelled.</li>
</ul>
<p>More formally it’s a scheduling problem with conflicting schedules. More concretely, it’s a very difficult and CPU-expensive optimization problem. Enterprise databases can’t afford to wait hours to find the best schedule for each new transaction event. Therefore, they use less ideal approaches that lead to more time wasted between conflicting transactions.</p>
<h3 id="Lock-manager"><a href="#Lock-manager" class="headerlink" title="Lock manager"></a>Lock manager</h3><p>To handle this problem, most databases are using <strong>locks</strong> and/or <strong>data versioning</strong>. Since it’s a big topic, I’ll focus on the locking part then I’ll speak a little bit about data versioning.</p>
<h4 id="Pessimistic-locking"><a href="#Pessimistic-locking" class="headerlink" title="Pessimistic locking"></a>Pessimistic locking</h4><p>The idea behind locking is:</p>
<ul>
<li>if a transaction needs a data,</li>
<li>it locks the data</li>
<li>if another transaction also needs this data,</li>
<li>it’ll have to wait until the first transaction releases the data.</li>
</ul>
<p>This is called an <strong>exclusive lock</strong>.</p>
<p>But using an exclusive lock for a transaction that only needs to read a data is very expensive since <strong>it forces other transactions that only want to read the same data to wait</strong>. This is why there is another type of lock, the <strong>shared lock</strong>.</p>
<p>With the shared lock:</p>
<ul>
<li>if a transaction needs only to read a data A,</li>
<li>it “shared locks” the data and reads the data</li>
<li>if a second transaction also needs only to read data A,</li>
<li>it “shared locks” the data and reads the data</li>
<li>if a third transaction needs to modify data A,</li>
<li>it “exclusive locks” the data but it has to wait until the 2 other transactions release their shared locks to apply its exclusive lock on data A.</li>
</ul>
<p>Still, if a data as an exclusive lock, a transaction that just needs to read the data will have to wait the end of the exclusive lock to put a shared lock on the data.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/lock_manager.png" target="_blank" rel="noopener"><img src="../../public/images/lock_manager-20200207203251437.png" alt="lock manager in a database"></a></p>
<p>The lock manager is the process that gives and releases locks. Internally, it stores the locks in a hash table (where the key is the data to lock) and knows for each data:</p>
<ul>
<li>which transactions are locking the data</li>
<li>which transactions are waiting for the data</li>
</ul>
<h4 id="Deadlock"><a href="#Deadlock" class="headerlink" title="Deadlock"></a>Deadlock</h4><p>But the use of locks can lead to a situation where 2 transactions are waiting forever for a data:</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/deadlock.png" target="_blank" rel="noopener"><img src="../../public/images/deadlock-20200207203251451.png" alt="deadlock with database transactions"></a></p>
<p>In this figure:</p>
<ul>
<li>transaction A has an exclusive lock on data1 and is waiting to get data2</li>
<li>transaction B has an exclusive lock on data2 and is waiting to get data1</li>
</ul>
<p>This is called a <strong>deadlock</strong>.</p>
<p>During a deadlock, the lock manager chooses which transaction to cancel (rollback) in order to remove the deadlock. This decision is not easy:</p>
<ul>
<li>Is it better to kill the transaction that modified the least amount of data (and therefore that will produce the least expensive rollback)?</li>
<li>Is it better to kill the least aged transaction because the user of the other transaction has waited longer?</li>
<li>Is it better to kill the transaction that will take less time to finish (and avoid a possible starvation)?</li>
<li>In case of rollback, how many transactions will be impacted by this rollback?</li>
</ul>
<p>But before making this choice, it needs to check if there are deadlocks.</p>
<p>The hash table can be seen as a graph (like in the previous figures). There is a deadlock if there is a cycle in the graph. Since it’s expensive to check for cycles (because the graph with all the locks is quite big), a simpler approach is often used: using a <strong>timeout</strong>. If a lock is not given within this timeout, the transaction enters a deadlock state.</p>
<p>The lock manager can also check before giving a lock if this lock will create a deadlock. But again it’s computationally expensive to do it perfectly. Therefore, these pre-checks are often a set of basic rules.</p>
<h4 id="Two-phase-locking"><a href="#Two-phase-locking" class="headerlink" title="Two-phase locking"></a>Two-phase locking</h4><p>The <strong>simplest way</strong> to ensure a pure isolation is if a lock is acquired at the beginning of the transaction and released at the end of the transaction. This means that a transaction has to wait for all its locks before it starts and the locks held by a transaction are released when the transaction ends. It works but it <strong>produces a lot of time wasted</strong> to wait for all locks.</p>
<p>A faster way is the <strong>Two-Phase Locking Protocol</strong> (used by DB2 and SQL Server) where a transaction is divided into 2 phases:</p>
<ul>
<li>the <strong>growing phase</strong> where a transaction can obtain locks, but can’t release any lock.</li>
<li>the <strong>shrinking phase</strong> where a transaction can release locks (on the data it has already processed and won’t process again), but can’t obtain new locks.</li>
</ul>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/two-phase-locking.png" target="_blank" rel="noopener"><img src="../../public/images/two-phase-locking-20200207203251473.png" alt="a problem avoided with two phase locking"></a></p>
<p>The idea behind these 2 simple rules is:</p>
<ul>
<li>to release the locks that aren’t used anymore to reduce the wait time of other transactions waiting for these locks</li>
<li>to prevent from cases where a transaction gets data modified after the transaction started and therefore aren’t coherent with the first data the transaction acquired.</li>
</ul>
<p>This protocol works well except if a transaction that modified a data and released the associated lock is cancelled (rolled back). You could end up in a case where another transaction reads the modified value whereas this value is going to be rolled back. To avoid this problem, <strong>all the exclusive locks must be released at the end of the transaction</strong>.</p>
<h4 id="A-few-words"><a href="#A-few-words" class="headerlink" title="A few words"></a>A few words</h4><p>Of course a real database uses a more sophisticated system involving more types of locks (like intention locks) and more granularities (locks on a row, on a page, on a partition, on a table, on a tablespace) but the idea remains the same.</p>
<p>I only presented the pure lock-based approach. <strong>Data versioning is another way to deal with this problem</strong>.</p>
<p>The idea behind versioning is that:</p>
<ul>
<li>every transaction can modify the same data at the same time</li>
<li>each transaction has its own copy (or version) of the data</li>
<li>if 2 transactions modify the same data, only one modification will be accepted, the other will be refused and the associated transaction will be rolled back (and maybe re-run).</li>
</ul>
<p>It increases the performance since:</p>
<ul>
<li><strong>reader transactions don’t block writer transactions</strong></li>
<li><strong>writer transactions don’t block reader transactions</strong></li>
<li>there is no overhead from the “fat and slow” lock manager</li>
</ul>
<p>Everything is better than locks except when 2 transactions write the same data. Moreover, you can quickly end up with a huge disk space overhead.</p>
<p>Data versioning and locking are two different visions: <strong>optimistic locking vs pessimistic locking</strong>. They both have pros and cons; it really depends on the use case (more reads vs more writes). For a presentation on data versioning, I recommend <a href="http://momjian.us/main/writings/pgsql/mvcc.pdf" target="_blank" rel="noopener">this very good presentation</a> on how PostgreSQL implements multiversion concurrency control.</p>
<p>Some databases like DB2 (until DB2 9.7) and SQL Server (except for snapshot isolation) are only using locks. Other like PostgreSQL, MySQL and Oracle use a mixed approach involving locks and data versioning. I’m not aware of a database using only data versioning (if you know a database based on a pure data versioning, feel free to tell me).</p>
<p>[UPDATE 08/20/2015] I was told by a reader that:</p>
<blockquote>
<p>Firebird and Interbase use versioning without record locking.<br>Versioning has an interesting effect on indexes: sometimes a unique index contains duplicates, the index can have more entries than the table has rows, etc.</p>
</blockquote>
<p>If you read the part on the different levels of isolation, when you increase the isolation level you increase the number of locks and therefore the time wasted by transactions to wait for their locks. This is why most databases don’t use the highest isolation level (Serializable) by default.</p>
<p>As always, you can check by yourself in the documentation of the main databases (for example <a href="http://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-model.html" target="_blank" rel="noopener">MySQL</a>, <a href="http://www.postgresql.org/docs/9.4/static/mvcc.html" target="_blank" rel="noopener">PostgreSQL</a> or <a href="http://docs.oracle.com/cd/B28359_01/server.111/b28318/consist.htm#i5337" target="_blank" rel="noopener">Oracle</a>).</p>
<h3 id="Log-manager"><a href="#Log-manager" class="headerlink" title="Log manager"></a>Log manager</h3><p>We’ve already seen that to increase its performances, a database stores data in memory buffers. But if the server crashes when the transaction is being committed, you’ll lose the data still in memory during the crash, which breaks the Durability of a transaction.</p>
<p>You can write everything on disk but if the server crashes, you’ll end up with the data half written on disk, which breaks the Atomicity of a transaction.</p>
<p><strong>Any modification written by a transaction must be undone or finished</strong>.</p>
<p>To deal with this problem, there are 2 ways:</p>
<ul>
<li><strong>Shadow copies/pages</strong>: Each transaction creates its own copy of the database (or just a part of the database) and works on this copy. In case of error, the copy is removed. In case of success, the database switches instantly the data from the copy with a filesystem trick then it removes the “old” data.</li>
<li><strong>Transaction log</strong>: A transaction log is a storage space. Before each write on disk, the database writes an info on the transaction log so that in case of crash/cancel of a transaction, the database knows how to remove (or finish) the unfinished transaction.</li>
</ul>
<h4 id="WAL"><a href="#WAL" class="headerlink" title="WAL"></a>WAL</h4><p>The shadow copies/pages creates a huge disk overhead when used on large databases involving many transactions. That’s why modern databases use a <strong>transaction log</strong>. The transaction log must be stored on a <strong>stable storage</strong>. I won’t go deeper on storage technologies but using (at least) RAID disks is mandatory to prevent from a disk failure.</p>
<p>Most databases (at least Oracle, <a href="https://technet.microsoft.com/en-us/library/ms186259(v=sql.105).aspx" target="_blank" rel="noopener">SQL Server</a>, <a href="http://www.ibm.com/developerworks/data/library/techarticle/0301kline/0301kline.html" target="_blank" rel="noopener">DB2</a>, <a href="http://www.postgresql.org/docs/9.4/static/wal.html" target="_blank" rel="noopener">PostgreSQL</a>, MySQL and <a href="https://www.sqlite.org/wal.html" target="_blank" rel="noopener">SQLite</a>) deal with the transaction log using the <strong>Write-Ahead Logging protocol</strong> (WAL). The WAL protocol is a set of 3 rules:</p>
<ul>
<li>1) Each modification into the database produces a log record, and <strong>the log record must be written into the transaction log before the data is written on disk</strong>.</li>
<li>2) The log records must be written in order; a log record A that happens before a log record B must but written before B</li>
<li>3) When a transaction is committed, the commit order must be written on the transaction log before the transaction ends up successfully.</li>
</ul>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/log_manager.png" target="_blank" rel="noopener"><img src="../../public/images/log_manager-20200207203251465.png" alt="log manager in a database"></a></p>
<p>This job is done by a log manager. An easy way to see it is that between the cache manager and the data access manager (that writes data on disk) the log manager writes every update/delete/create/commit/rollback on the transaction log before they’re written on disk. Easy, right?</p>
<p>WRONG ANSWER! After all we’ve been through, you should know that everything related to a database is cursed by the “database effect”. More seriously, the problem is to find a way to write logs while keeping good performances. If the writes on the transaction log are too slow they will slow down everything.</p>
<h4 id="ARIES"><a href="#ARIES" class="headerlink" title="ARIES"></a>ARIES</h4><p>In 1992, IBM researchers “invented” an enhanced version of WAL called ARIES. ARIES is more or less used by most modern databases. The logic might not be the same but the concepts behind ARIES are used everywhere. I put the quotes on invented because, according to this <a href="http://db.csail.mit.edu/6.830/lectures/lec15-notes.pdf" target="_blank" rel="noopener">MIT course</a>, the IBM researchers did “nothing more than writing the good practices of transaction recovery”. Since I was 5 when the ARIES paper was published, I don’t care about this old gossip from bitter researchers. In fact, I only put this info to give you a break before we start this last technical part. I’ve read a huge part of the <a href="http://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf" target="_blank" rel="noopener">research paper on ARIES</a> and I find it very interesting! In this part I’ll only give you an overview of ARIES but I strongly recommend to read the paper if you want a real knowledge.</p>
<p>ARIES stands for <strong>A</strong>lgorithms for <strong>R</strong>ecovery and <strong>I</strong>solation <strong>E</strong>xploiting <strong>S</strong>emantics.</p>
<p>The aim of this technique is double:</p>
<ul>
<li>1) Having <strong>good performances when writing logs</strong></li>
<li>2) Having a fast and <strong>reliable recovery</strong></li>
</ul>
<p>There are multiple reasons a database has to rollback a transaction:</p>
<ul>
<li>Because the user cancelled it</li>
<li>Because of server or network failures</li>
<li>Because the transaction has broken the integrity of the database (for example you have a UNIQUE constraint on a column and the transaction adds a duplicate)</li>
<li>Because of deadlocks</li>
</ul>
<p>Sometimes (for example, in case of network failure), the database can recover the transaction.</p>
<p>How is that possible? To answer this question, we need to understand the information stored in a log record.</p>
<h5 id="The-logs"><a href="#The-logs" class="headerlink" title="The logs"></a>The logs</h5><p>Each <strong>operation (add/remove/modify) during a transaction produces a log</strong>. This log record is composed of:</p>
<ul>
<li><strong>LSN:</strong> A unique <strong>L</strong>og <strong>S</strong>equence <strong>N</strong>umber. This LSN is given in a chronological order*. This means that if an operation A happened before an operation B the LSN of log A will be lower than the LSN of log B.</li>
<li><strong>TransID:</strong> the id of the transaction that produced the operation.</li>
<li><strong>PageID:</strong> the location on disk of the modified data. The minimum amount of data on disk is a page so the location of the data is the location of the page that contains the data.</li>
<li><strong>PrevLSN:</strong> A link to the previous log record produced by the same transaction.</li>
<li><strong>UNDO:</strong> a way to remove the effect of the operation</li>
</ul>
<p>For example, if the operation is an update, the UNDO will store either the value/state of the updated element before the update (physical UNDO) or the reverse operation to go back at the previous state (logical UNDO)**.</p>
<ul>
<li><strong>REDO</strong>: a way replay the operation</li>
</ul>
<p>Likewise, there are 2 ways to do that. Either you store the value/state of the element after the operation or the operation itself to replay it.</p>
<ul>
<li>…: (FYI, an ARIES log has 2 others fields: the UndoNxtLSN and the Type).</li>
</ul>
<p>Moreover, each page on disk (that stores the data, not the log) has id of the log record (LSN) of the last operation that modified the data.</p>
<p>*The way the LSN is given is more complicated because it is linked to the way the logs are stored. But the idea remains the same.</p>
<p>**ARIES uses only logical UNDO because it’s a real mess to deal with physical UNDO.</p>
<p>Note: From my little knowledge, only PostgreSQL is not using an UNDO. It uses instead a garbage collector daemon that removes the old versions of data. This is linked to the implementation of the data versioning in PostgreSQL.</p>
<p>To give you a better idea, here is a visual and simplified example of the log records produced by the query “UPDATE FROM PERSON SET AGE = 18;”. Let’s say this query is executed in transaction 18.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/ARIES_logs.png" target="_blank" rel="noopener"><img src="../../public/images/ARIES_logs-20200207203251473.png" alt="simplified logs of ARIES protocole"></a></p>
<p>Each log has a unique LSN. The logs that are linked belong to the same transaction. The logs are linked in a chronological order (the last log of the linked list is the log of the last operation).</p>
<h5 id="Log-Buffer"><a href="#Log-Buffer" class="headerlink" title="Log Buffer"></a>Log Buffer</h5><p>To avoid that log writing becomes a major bottleneck, a <strong>log buffer</strong> is used.</p>
<p><a href="http://coding-geek.com/wp-content/uploads/2015/08/ARIES_log_writing.png" target="_blank" rel="noopener"><img src="../../public/images/ARIES_log_writing-20200207203251473.png" alt="log writing process in databases"></a></p>
<p>When the query executor asks for a modification:</p>
<ul>
<li>1) The cache manager stores the modification in its buffer.</li>
<li>2) The log manager stores the associated log in its buffer.</li>
<li>3) At this step, the query executor considers the operation is done (and therefore can ask for other modifications)</li>
<li>4) Then (later) the log manager writes the log on the transaction log. The decision when to write the log is done by an algorithm.</li>
<li>5) Then (later) the cache manager writes the modification on disk. The decision when to write data on disk is done by an algorithm.</li>
</ul>
<p><strong>When a transaction is committed, it means that for every operation in the transaction the steps 1, 2, 3,4,5 are done</strong>. Writing in the transaction log is fast since it’s just “adding a log somewhere in the transaction log” whereas writing data on disk is more complicated because it’s “writing the data in a way that it’s fast to read them”.</p>
<h5 id="STEAL-and-FORCE-policies"><a href="#STEAL-and-FORCE-policies" class="headerlink" title="STEAL and FORCE policies"></a>STEAL and FORCE policies</h5><p>For performance reasons the <strong>step 5 might be done after the commit</strong> because in case of crashes it’s still possible to recover the transaction with the REDO logs. This is called a <strong>NO-FORCE policy</strong>.</p>
<p>A database can choose a FORCE policy (i.e. step 5 must be done before the commit) to lower the workload during the recovery.</p>
<p>Another issue is to choose whether <strong>the data are written step-by-step on disk (STEAL policy)</strong> or if the buffer manager needs to wait until the commit order to write everything at once (NO-STEAL). The choice between STEAL and NO-STEAL depends on what you want: fast writing with a long recovery using UNDO logs or fast recovery?</p>
<p>Here is a summary of the impact of these policies on recovery:</p>
<ul>
<li><strong>STEAL/NO-FORCE</strong> <strong>needs UNDO and REDO</strong>: <strong>highest performances</strong> but gives more complex logs and recovery processes (like ARIES). <strong>This is the choice made by most databases</strong>. Note: I read this fact on multiple research papers and courses but I couldn’t find it (explicitly) on the official documentations.</li>
<li>STEAL/ FORCE needs only UNDO.</li>
<li>NO-STEAL/NO-FORCE needs only REDO.</li>
<li>NO-STEAL/FORCE needs nothing: <strong>worst performances</strong> and a huge amount of ram is needed.</li>
</ul>
<h5 id="The-recovery-part"><a href="#The-recovery-part" class="headerlink" title="The recovery part"></a>The recovery part</h5><p>Ok, so we have nice logs, let’s use them!</p>
<p>Let’s say the new intern has crashed the database (rule n°1: it’s always the intern’s fault). You restart the database and the recovery process begins.</p>
<p>ARIES recovers from a crash in three passes:</p>
<ul>
<li><strong>1) The Analysis pass</strong>: The recovery process reads the full transaction log* to recreate the timeline of what was happening during the crash. It determines which transactions to rollback (all the transactions without a commit order are rolled back) and which data needed to be written on disk at the time of the crash.</li>
<li><strong>2) The Redo pass</strong>: This pass starts from a log record determined during analysis, and uses the REDO to update the database to the state it was before the crash.</li>
</ul>
<p>During the redo phase, the REDO logs are processed in a chronological order (using the LSN).</p>
<p>For each log, the recovery process reads the LSN of the page on disk containing the data to modify.</p>
<p>If LSN(page_on_disk)&gt;=LSN(log_record), it means that the data has already been written on disk before the crash (but the value was overwritten by an operation that happened after the log and before the crash) so nothing is done.</p>
<p>If LSN(page_on_disk)&lt;LSN(log_record) then the page on disk is updated.</p>
<p>The redo is done even for the transactions that are going to be rolled back because it simplifies the recovery process (but I’m sure modern databases don’t do that).</p>
<ul>
<li><strong>3) The Undo pass</strong>: This pass rolls back all transactions that were incomplete at the time of the crash. The rollback starts with the last logs of each transaction and processes the UNDO logs in an anti-chronological order (using the PrevLSN of the log records).</li>
</ul>
<p>During the recovery, the transaction log must be warned of the actions made by the recovery process so that the data written on disk are synchronized with what’s written in the transaction log. A solution could be to remove the log records of the transactions that are being undone but that’s very difficult. Instead, ARIES writes compensation logs in the transaction log that delete logically the log records of the transactions being removed.</p>
<p>When a transaction is cancelled “manually” or by the lock manager (to stop a deadlock) or just because of a network failure, then the analysis pass is not needed. Indeed, the information about what to REDO and UNDO is available in 2 in-memory tables:</p>
<ul>
<li>a <strong>transaction table</strong> (stores the state of all current transactions)</li>
<li>a <strong>dirty page table</strong> (stores which data need to be written on disk).</li>
</ul>
<p>These tables are updated by the cache manager and the transaction manager for each new transaction event. Since they are in-memory, they are destroyed when the database crashes.</p>
<p>The job of the analysis phase is to recreate both tables after a crash using the information in the transaction log. <em>To speed up the analysis pass, ARIES provides the notion of *</em>checkpoint**. The idea is to write on disk from time to time the content of the transaction table and the dirty page table and the last LSN at the time of this write so that during the analysis pass, only the logs after this LSN are analyzed.</p>
<h1 id="To-conclude"><a href="#To-conclude" class="headerlink" title="To conclude"></a>To conclude</h1><p>Before writing this article, I knew how big the subject was and I knew it would take time to write an in-depth article about it. It turned out that I was very optimistic and I spent twice more time than expected, but I learned a lot.</p>
<p>If you want a good overview about databases, I recommend reading the research paper “<a href="http://db.cs.berkeley.edu/papers/fntdb07-architecture.pdf" target="_blank" rel="noopener">Architecture of a Database System</a> “. This is a good introduction on databases (110 pages) and for once it’s readable by non-CS guys. This paper helped me a lot to find a plan for this article and it’s not focused on data structures and algorithms like my article but more on the architecture concepts.</p>
<p>If you read this article carefully you should now understand how powerful a database is. Since it was a very long article, let me remind you about what we’ve seen:</p>
<ul>
<li>an overview of the B+Tree indexes</li>
<li>a global overview of a database</li>
<li>an overview of the cost based optimization with a strong focus on join operators</li>
<li>an overview of the buffer pool management</li>
<li>an overview of the transaction management</li>
</ul>
<p>But a database contains even more cleverness. For example, I didn’t speak about some touchy problems like:</p>
<ul>
<li>how to manage clustered databases and global transactions</li>
<li>how to take a snapshot when the database is still running</li>
<li>how to efficiently store (and compress) data</li>
<li>how to manage memory</li>
</ul>
<p>So, think twice when you have to choose between a buggy NoSQL database and a rock-solid relational database. Don’t get me wrong, some NoSQL databases are great. But they’re still young and answering specific problems that concern a few applications.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/07/The-Best-Programming-Language/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/07/The-Best-Programming-Language/" class="post-title-link" itemprop="url">The_Best_Programming_Language</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-07 20:26:11 / Modified: 21:29:44" itemprop="dateCreated datePublished" datetime="2020-02-07T20:26:11-05:00">2020-02-07</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>When I want to take a break at work, I sometimes read technology forums. And there is one kind of posts that I really like: the flame wars between programming languages. I like these posts because you can see passionate and smart people who are arguing as if their lives were at play.</p>
<p>These posts have 2 advantages:</p>
<ul>
<li>they make me laugh</li>
<li>I learn new stuff</li>
</ul>
<p>If I had to sum up this kind of posts, it would be something like:</p>
<p>Post Title “Java is the best language” by NewJavaFanBoy</p>
<blockquote>
<p><strong>NewJavaFanBoy</strong>: Java is the best language because of its community. Moreover, it has really cool features like lambdas. Why so many people hate Java?</p>
<p><strong>FormerJavaFanBoy</strong>: Oracle killed Java.</p>
<p><strong>DotNetFanBoy</strong>: The evolution of Java is too slow, C# had lambdas a while ago. Moreover, some critical features like optional and named parameters are not in Java. Now that dotnet is more open sourced and can be run on Linux with Mono, Java is going to die.</p>
<p><strong>TrollRoxxoR</strong>: BecauseJavaDevelopersDontKnowHowToWriteCode</p>
<p><strong>RealG33k</strong>: Both your languages are for kids, C++ is way better but it’s for real developers only. Do you even know what SOLID means?</p>
<p><strong>HipsterGeek</strong>: So old and lame … you should try Node.js, it’s based on asynchronous calls and it’s very fast.</p>
<p><strong>LinusTorvalds</strong>: Pussies, a real developer uses C or assembly. You can’t have performances with those high level shits.</p>
</blockquote>
<p>I hate PHP. I can’t explain why; it must be because I tried to learn it when I was 14 and it messed with my brain. But guess what, you’re reading this post on a server using PHP/NGINX (which is a kickass server by the way). I’m good with Java. So, I could have used a Java framework running on a fast fat JVM. But, WordPress is a great platform. It’s often looked down by purists but it clearly answers my needs. The aim of my blog is not to be the fastest in the world (though it has surprisingly but painfully survived 2 Hacker News and Reddit front pages involving 500 simultaneous connections). I just want a user-friendly interface where I can share my thoughts.</p>
<p>Which leads to my point: there is no best programming language, it depends on the situation.</p>
<p><strong>1) Do you need performances?</strong></p>
<p>If yes, what kind of performances are we talking about?</p>
<ul>
<li>Seconds? Every language can do it!</li>
<li>Milliseconds? Every language with good programmers can do it.</li>
<li>Microseconds? At this step, you can remove all the interpreted languages (like python, which is a good language). I know that a well-tuned JVM with very good Java programmers can do it. I imagine that it’s the same for C#. Of course, a pure-compiled language can deal with that.</li>
</ul>
<p>But in all these cases the programmer’s skills are more important than the language.</p>
<ul>
<li>Nanoseconds? Only assembly or maybe C can deal with that.</li>
</ul>
<p>So, in most situations developers’ skills are what matters.</p>
<p><strong>2) What’s about the ecosystem?</strong></p>
<p>More than the language itself, the ecosystem is important.</p>
<p>I’ve used Visual Studio during my scholarship and I have been amazed by the coherency of Microsoft’s ecosystem.</p>
<p>Now, I’m more an Eclipse guy. Even in the Java community, Eclipse is looked down by purists who now use IntelliJ IDEA. Eclipse is an open source software developed by different people and it’s clearly visible (in a bad way). Compared to the coherency of Visual Studio, you’ll find different logics in the different plugins of Eclipse.</p>
<p>But, if having tools is great, knowing how to use them is better. For example, when I started in Java, I was very slow. I learned by hearth some Eclipse keywords and it’s changed my developer life. I’ve also looked for useful plugins, and Eclipse has plenty of them, because it’s a rich ecosystem.</p>
<p><strong>3) What’s about the online help?</strong></p>
<p>Ok, you’re using your kickass programming language but don’t tell me you know every side of this language by hearth. Having a well known language is useful when you need help. A simple Google or StackOverflow search and you get your answer by Ninja_Guru_666 and I_AM_THE_EXPERT. If you’re more like an in-depth programmer, you can also check for the official documentation assuming it exists for the problem you’re looking for.</p>
<p><strong>4) What are the skills of the team?</strong></p>
<p>If the developers don’t really know how a computer works, using a compiled language is a suicidal move. And, compared to the purists, I don’t see why knowing (exactly) how a computer works makes you a good developer (though, I must admit, it helps; but there are more important skills).</p>
<p>It’s better not using the best tool but a known one. Moreover, many developers are fan boys. Using their preferred language will help them to stay motivated on the project.</p>
<p><strong>5) The business side</strong></p>
<p>An objective point of view is to see what the most in-demand languages are. It doesn’t mean they’re the best but at least you’ll get a job. In this case, Java, C#, PHP, SQL and JavaScript are clearly above all (at least in France).</p>
<p>Moreover, as a technical leader, it’s always good to check the skills in the market before choosing a technology. If you choose the best but rare technology to deal with your problem, good luck for finding skilled developers on the technology.</p>
<p>But what’s true in 2015 might change in 2018. ActionScript was a must have not so long ago. Likewise, with Swift, all the hours spent on Objective C will become obsolete in a few years.</p>
<p>To conclude, I’ll end up with a lame and (I hope) obvious conclusion: there are no best programming languages or best frameworks; what’s best now might not exist tomorrow. A programming language is just a tool; what matters is the way you overcome your problems.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%90%86%E8%AE%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%90%86%E8%AE%BA/" class="post-title-link" itemprop="url">计算机理论</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-06 16:58:37 / Modified: 18:02:37" itemprop="dateCreated datePublished" datetime="2020-02-06T16:58:37-05:00">2020-02-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="数据结构和算法"><a href="#数据结构和算法" class="headerlink" title="数据结构和算法"></a>数据结构和算法</h1><p>算法是比较难学习的，而且学习 “ 算法 “ 是需要智商的。数组、链表、哈希表、二叉树、排序算法等一些基础知识，对大多数人来说是没什么问题的。但是一旦进入到路径规划、背包问题、字符串匹配、动态规划、递归遍历等一些比较复杂的问题上，就会让很多人跟不上了，不但跟不上，而且还会非常痛苦。是的，解决算法问题的确是可以区分人类智商的一个比较好的方式，这也是为什么好些公司用算法题当面试题来找到智商比较高的程序员。</p>
<p>然而，在很多时候，我们在工作中却发现根本用不到算法，或是一些基本的算法也没有必要实现，只需要使用一下第三方的库就好了。于是，导致社会上出现很多 “ 算法无用论 “ 的声音。</p>
<p>对此，我想说，算法真的很重要。无论是做业务还是做底层系统，经常需要使用算法处理各种各样的问题。比如，业务上需要用算法比较两个数组中差异的布隆过滤器，或是在做监控系统时实时计算过去一分钟的 P99 统计时的蓄水池算法，或是数据库的 B+ 树索引，还有 Linux 内核中的 epoll 的红黑树，还有在做服务调度里的 “ 背包问题 “ 等都会用算法，真的是会本质上帮助到你，也是会让你非常有成就感的一件事。</p>
<p>虽然算法很难，需要智商，但我还是想鼓励你，这其中是有很多的套路是可以学习的，一旦学会这些套路，你会受益无穷的。</p>
<p>这里有几本书着重推荐一下。</p>
<ul>
<li><strong>基础知识</strong>。《<a href="https://book.douban.com/subject/10432347/" target="_blank" rel="noopener">算法</a>》，是算法领域经典的参考书，不但全面介绍了关于算法和数据结构的必备知识，还给出了每位程序员应知应会的 50 个算法，并提供了实际代码。最不错的是，其深入浅出的算法介绍，让一些比较难的算法也变得容易理解，尤其是书中对红黑树的讲解非常精彩。其中，还有大量的图解，详尽的代码和讲解，也许是最好的数据结构入门图书。不好的是不深，缺乏进一步的算法设计内容，甚至连动态规划都未提及。另外，如果你觉得算法书比较枯燥的话，你可以看看这本有趣的《<a href="https://book.douban.com/subject/26979890/" target="_blank" rel="noopener">算法图解</a>》。</li>
<li><strong>理论加持</strong>。如果说上面这本书偏于实践和工程，而你看完后，对算法和数据结构的兴趣更浓了，那么你可以再看看另一本也是很经典的偏于理论方面的书——《<a href="https://book.douban.com/subject/20432061/" target="_blank" rel="noopener">算法导论</a>》。虽然其中的一些理论知识在《算法》那本书中也有提过，但《算法导论》这本书更为专业一些，是美国计算机科学本科生的教科书。</li>
<li><strong>思维改善</strong>。还有一本叫《<a href="https://book.douban.com/subject/3227098/" target="_blank" rel="noopener">编程珠玑</a>》的书，写这本书的人是世界著名计算机科学家乔恩·本特利（Jon Bentley），被誉为影响算法发展的十位大师之一。你可能不认识这个人，但是你知道他的学生有多厉害吗？一个是 Tcl 语言设计者约翰·奥斯德奥特（John Ousterhout），另一个是 Java 语言设计者詹姆斯·高斯林（James Gosling），还有一个是《算法导论》作者之一查尔斯·雷斯尔森（Charles Leiserson），还有好多好多。这本书也是很经典的算法书，其中都是一些非常实际的问题，并以其独有的洞察力和创造力，来引导读者理解并学会解决这些问题的方法，也是一本可以改善你思维方式的书。</li>
</ul>
<p>然后，你需要去做一些题来训练一下自己的算法能力，这里就要推荐 <a href="https://leetcode.com/" target="_blank" rel="noopener">LeetCode</a> 这个网站了。它是一个很不错的做算法训练的地方。现在也越做越好了。基本上来说，这里会有两类题。</p>
<ul>
<li><strong>基础算法题</strong>。其中有大量的算法题，解这些题都是有套路的，不是用递归（深度优先 DFS，广度优先 BFS），就是要用动态规划（Dynamic Programming），或是折半查找（Binary Search），或是回溯（Back tracing），或是分治法（Divide and Conquer），还有大量的对树、数组、链表、字符串和 hash 表的操作。通过做这些题能让你对这些最基础的算法的思路有非常扎实的了解和训练。</li>
<li><strong>编程题</strong>。比如：atoi，strstr，add two nums，括号匹配，字符串乘法，通配符匹配，文件路径简化，Text Justification，反转单词等，这些题的 Edge Case 和 Corner Case 有很多。这些题需要你想清楚了再干，只要你稍有疏忽，就会有几个 case 让你痛不欲生，而且一不小心就会让你的代码写得又臭又长，无法阅读。通过做这些题，可以非常好地训练你对各种情况的考虑，以及你对程序代码组织的掌控（其实就是其中的状态变量）。</li>
</ul>
<p>我觉得每个程序员都应该花时间和精力做这些题，因为你会从这些题中得到很大的收益。</p>
<p>如果能够把这些算法能力都掌握了，那么你就有很大的概率可以很容易地通过这世界上最优的公司的面试，比如：Google、Amazon、Facebook 之类的公司。对你来说，如果能够进入到这些公司里工作，那么你未来的想像空间也会大得多得多。</p>
<p>最后，我们要知道这个世界上的数据结构和算法很多很多，下面给出了两个网站。</p>
<ul>
<li><strong><a href="https://www.wikiwand.com/en/List_of_algorithms" target="_blank" rel="noopener">List of Algorithms</a></strong> ，这个网站罗列了非常多的算法，完全可以当成一个算法字典，或是用来开阔眼界。</li>
<li>还有一个数据结构动画图的网站 <a href="https://www.cs.usfca.edu/~galles/visualization/Algorithms.html" target="_blank" rel="noopener">Data Structure Visualizations</a>。</li>
</ul>
<h1 id="其它理论基础知识"><a href="#其它理论基础知识" class="headerlink" title="其它理论基础知识"></a>其它理论基础知识</h1><p>下面这些书，基本上是计算机科学系的大学教材。如果你想有科班出生的理论基础，那么这些书是必读的。当然，这些理论基础知识比较枯燥，但我觉得如果你想成为专业的程序员，那么应该要找时间读一下。</p>
<ul>
<li>《<a href="https://book.douban.com/subject/1139426/" target="_blank" rel="noopener">数据结构与算法分析</a>》，这本书曾被评为 20 世纪顶尖的 30 部计算机著作之一，作者 Mark Allen Weiss 在数据结构和算法分析方面卓有建树，他在数据结构和算法分析等方面的著作尤其畅销，并广受好评，已被世界 500 余所大学用作教材。</li>
<li>《<a href="https://book.douban.com/subject/1929984/" target="_blank" rel="noopener">数据库系统概念</a>》，它是数据库系统方面的经典教材之一。国际上许多著名大学包括斯坦福大学、耶鲁大学、德克萨斯大学、康奈尔大学、伊利诺伊大学、印度理工学院等都采用本书作为教科书。这本书全面介绍了数据库系统的各种知识，透彻阐释数据库管理的基本概念。不仅讨论了数据库查询语言、模式设计、数据仓库、数据库应用开发、基于对象的数据库和 XML、数据存储和查询、事务管理、数据挖掘与信息检索以及数据库系统体系结构等方面的内容，而且对性能评测标准、性能调整、标准化以及空间与地理数据、事务处理监控等高级应用主题进行了广泛讨论。</li>
<li>《<a href="https://book.douban.com/subject/3852290/" target="_blank" rel="noopener">现代操作系统</a>》，这本书是操作系统领域的经典之作，书中集中讨论了操作系统的基本原理，包括进程、线程、存储管理、文件系统、输入 / 输出、死锁等，同时还包含了有关计算机安全、多媒体操作系统、掌上计算机操作系统、微内核、多核处理机上的虚拟机以及操作系统设计等方面的内容。</li>
<li>《<a href="https://book.douban.com/subject/1391207/" target="_blank" rel="noopener">计算机网络</a>》，这本书采用了独创的自顶向下方法，即从应用层开始沿协议栈向下讲解计算机网络的基本原理，强调应用层范例和应用编程接口，内容深入浅出，注重教学方法，理论与实践相结合。新版中还增加了无线和移动网络一章，并扩充了对等网络、BGP、MPLS、网络安全、广播选路和因特网编址及转发方面的材料。是一本不可多得的教科书。</li>
<li>《<a href="https://book.douban.com/subject/1148282/" target="_blank" rel="noopener">计算机程序的构造和解释</a>》，这本书也很经典，是 MIT 的计算机科学系的教材。这本书中主要证实了很多程序是怎么构造出来的，以及程序的本质是什么。整本书主要是使用 Scheme/Lisp 语言，从数据抽象、过程抽象、迭代、高阶函数等编程和控制系统复杂性的思想，到数据结构和算法，到编译器 / 解释器、编程语言设计。</li>
<li>《<a href="https://book.douban.com/subject/3296317/" target="_blank" rel="noopener">编译原理</a>》，这本书又叫 “ 龙书 “，其全面、深入地探讨了编译器设计方面的重要主题，包括词法分析、语法分析、语法制导定义和语法制导翻译、运行时刻环境、目标代码生成、代码优化技术、并行性检测以及过程间分析技术，并在相关章节中给出大量的实例。与上一版相比，本书进行了全面的修订，涵盖了编译器开发方面的最新进展。每章中都提供了大量的系统及参考文献。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/06/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/" class="post-title-link" itemprop="url">计算机编程语言</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-06 16:55:49 / Modified: 17:57:46" itemprop="dateCreated datePublished" datetime="2020-02-06T16:55:49-05:00">2020-02-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>编程语言</strong>。你需要学习 C、C++ 和 Java 这三个工业级的编程语言。为什么说它们是工业级的呢？主要是，C 和 C++ 语言规范都由 ISO 标准化过，而且都有工业界厂商组成的标准化委员会来制定工业标准。次要原因是，它们已经在业界应用于许多重要的生产环境中。</p>
<ul>
<li>C 语言不用多说，现今这个世界上几乎所有重要的软件都跟 C 有直接和间接的关系，操作系统、网络、硬件驱动等等。说得霸气一点儿，这个世界就是在 C 语言之上运行的。</li>
<li>而对于 C++ 来说，现在主流的浏览器、数据库、Microsoft Office、主流的图形界面、著名的游戏引擎等都是用 C++ 编写的。而且，很多公司都用 C++ 开发核心架构，如 Google、腾讯、百度、阿里云等。</li>
<li>而金融电商公司则广泛地使用 Java 语言，因为 Java 的好处太多了，代码稳定性超过 C 和 C++，生产力远超 C 和 C++。有 JVM 在，可以轻松地跨平台，做代码优化，做 AOP 和 IoC 这样的高级技术。以 Spring 为首的由庞大的社区开发的高质量的各种轮子让你只需关注业务，是能够快速搭建企业级应用的不二之选。</li>
</ul>
<p>此外，推荐学习 Go 语言。一方面，Go 语言现在很受关注，它是取代 C 和 C++ 的另一门有潜力的语言。C 语言太原始了，C++ 太复杂了，Java 太高级了，所以 Go 语言就在这个夹缝中出现了。这门语言已经 10 多年了，其已成为云计算领域事实上的标准语言，尤其是在 Docker/Kubernetes 等项目中。Go 语言社区正在不断地从 Java 社区移植各种 Java 的轮子过来，Go 社区现在也很不错。</p>
<p>如果要写一些 PaaS 层的应用，Go 语言会比 C 和 C++ 更好，目前和 Java 有一拼。而且，Go 语言在国内外一些知名公司中有了一定的应用和实践，所以，是可以学习的（参看：《<a href="https://coolshell.cn/articles/18190.html" target="_blank" rel="noopener">Go 语言、Docker 和新技术</a>》一文）。此外，Go 语言语法特别简单，有了 C 和 C++ 的基础，学习 Go 的学习成本基本为零。</p>
<p><strong>理论学科</strong>。你需要学习像算法、数据结构、网络模型、计算机原理等计算机科学专业需要学习的知识。为什么要学好这些理论上的知识呢？</p>
<ul>
<li>其一，这些理论知识可以说是计算机科学这门学科最精华的知识了。说得大一点，这些是人类智慧的精华。你只要想成为高手，这些东西是你必需要掌握和学习的。</li>
<li>其二，当你在解决一些很复杂或是很难的问题时，这些基础理论知识可以帮到你很多。我过去这 20 年从这些基础理论知识中受益匪浅。</li>
<li>其三，这些理论知识的思维方式可以让你有触类旁通，一通百通的感觉。虽然知识比较难啃，但啃过以后，你将获益终生。</li>
</ul>
<p>另外，你千万不要觉得在你的日常工作或是生活当中根本用不上，学了也白学，这样的思维方式千万不要有，因为这是平庸的思维方式。如果你想等我用到了再学也不晚，那么你有必要看一下这篇文章《<a href="https://coolshell.cn/articles/4235.html" target="_blank" rel="noopener">程序员的荒谬之言还是至理名言？</a>》。</p>
<p><strong>系统知识</strong>。系统知识是理论知识的工程实践，这里面有很多很多的细节。比如像 Unix/Linux、TCP/IP、C10K 挑战等这样专业的系统知识。这些知识是你能不能把理论应用到实际项目当中，能不能搞定实际问题的重要知识。</p>
<p>当你在编程的时候，如何和系统进行交互或是获取操作系统的资源，如何进行通讯，当系统出了性能问题，当系统出了故障等，你有大量需要落地的事需要处理和解决。这个时候，这些系统知识就会变得尤为关键和重要了。</p>
<p>这些东西，你可以认为是计算机世界的物理世界，上层无论怎么玩，无论是 Java NIO，还是 Nginx，还是 Node.js，它们都逃脱不掉最下层的限制。所以，你要好好学习这方面的知识。</p>
<h1 id="编程语言"><a href="#编程语言" class="headerlink" title="编程语言"></a>编程语言</h1><h2 id="Java-语言"><a href="#Java-语言" class="headerlink" title="Java 语言"></a>Java 语言</h2><p>学习 Java 语言有以下<strong>入门级的书</strong>（注意：下面一些书在入门篇中有所提及，但为了完整性，还是要在这里提一下，因为可能有朋友是跳着看的）。</p>
<ul>
<li>《<a href="https://book.douban.com/subject/26880667/" target="_blank" rel="noopener">Java 核心技术：卷 1 基础知识</a>》，这本书本来是 Sun 公司的官方用书，是一本 Java 的入门参考书。对于 Java 初学者来说，是一本非常不错的值得时常翻阅的技术手册。书中有较多地方进行 Java 与 C++ 的比较，因为当时 Java 面世的时候，又被叫作 “C++ Killer”。而我在看这本书的时候，发现书中有很多 C++ 的东西，于是又去学习了 C++。学习 C++ 的时候，发现有很多 C 的东西不懂，又顺着去学习了 C。然后，C -&gt; C++ -&gt; Java 整条线融汇贯通，这对我未来的技术成长有非常大的帮助。</li>
<li>有了上述的入门后，Java 的 Spring 框架是你玩 Java 所无法回避的东西，所以接下来是两本 Spring 相关的书，《<a href="https://book.douban.com/subject/26767354/" target="_blank" rel="noopener">Spring 实战</a>》和《<a href="https://book.douban.com/subject/26857423/" target="_blank" rel="noopener">Spring Boot 实战</a>》。前者是传统的 Spring，后者是新式的微服务的 Spring。如果你只想看一本的话，那么就看后者吧。</li>
</ul>
<p>认真学习前面的书可以让你成功入门 Java，但想要进一步成长，就要看下面我推荐的几本<strong>提升级的书</strong>。</p>
<ul>
<li>接下来，你需要了解了一下如何编写高效的代码，于是必需看一下《<a href="https://book.douban.com/subject/27047716/" target="_blank" rel="noopener">Effective Java</a>》（注意，这里我给的引用是第三版的，也是 2017 年末出版的书），这本书是模仿 Scott Meyers 的经典图书《Effective C++》的。Effective 这种书基本上都是各种经验之谈，所以，这是一本非常不错的书，你一定要读。这里需要推荐一下 <a href="https://github.com/google/guava" target="_blank" rel="noopener">Google Guava 库</a> ，这个库不但是 JDK 的升级库，其中有如：集合（collections）、缓存（caching）、原生类型支持（primitives support）、并发库（concurrency libraries）、通用注解（common annotations）、字符串处理（string processing）、I/O 等库，其还是 Effective Java 这本书中的那些经验的实践代表。</li>
<li>《<a href="https://book.douban.com/subject/10484692/" target="_blank" rel="noopener">Java 并发编程实战</a>》，是一本完美的 Java 并发参考手册。书中从并发性和线程安全性的基本概念出发，介绍了如何使用类库提供的基本并发构建块，用于避免并发危险、构造线程安全的类及验证线程安全的规则，如何将小的线程安全类组合成更大的线程安全类，如何利用线程来提高并发应用程序的吞吐量，如何识别可并行执行的任务，如何提高单线程子系统的响应性，如何确保并发程序执行预期任务，如何提高并发代码的性能和可伸缩性等内容。最后介绍了一些高级主题，如显式锁、原子变量、非阻塞算法以及如何开发自定义的同步工具类。</li>
<li>了解如何编写出并发的程序，你还需要了解一下如何优化 Java 的性能。我推荐《<a href="https://book.douban.com/subject/26740520/" target="_blank" rel="noopener">Java 性能权威指南</a>》。通过学习这本书，你可以比较大程度地提升性能测试的效果。其中包括：使用 JDK 中自带的工具收集 Java 应用的性能数据，理解 JIT 编译器的优缺点，调优 JVM 垃圾收集器以减少对程序的影响，学习管理堆内存和 JVM 原生内存的方法，了解如何最大程度地优化 Java 线程及同步的性能，等等。看完这本书后，如果你还有余力，想了解更多的底层细节，那么，你有必要去读一下《<a href="https://book.douban.com/subject/24722612/" target="_blank" rel="noopener">深入理解 Java 虚拟机</a>》。</li>
<li>《<a href="https://book.douban.com/subject/2130190/" target="_blank" rel="noopener">Java 编程思想</a>》，真是一本透着编程思想的书。上面的书让你从微观角度了解 Java，而这本书则可以让你从一个宏观角度了解 Java。这本书和 Java 核心技术的厚度差不多，但这本书的信息密度比较大。所以，读起来是非常耗大脑的，因为它会让你不断地思考。对于想学好 Java 的程序员来说，这是一本必读的书。</li>
<li>《<a href="https://book.douban.com/subject/26952826/" target="_blank" rel="noopener">精通 Spring 4.x</a>》，也是一本很不错的书，就是有点厚，一共有 800 多页，都是干货。我认为其中最不错的是在分析原理，尤其是针对前面提到的 Spring 技术，应用与原理都讲得很透彻，IOC 和 AOP 也分析得很棒，娓娓道来。其对任何一个技术都分析得很细致和全面，不足之处就是内容太多了，所以导致很厚，但这并不影响它是一本不错的工具书。</li>
</ul>
<p>当然，学 Java 你一定要学面向对象的设计模式，这里就只有一本经典的书《<a href="https://book.douban.com/subject/1052241/" target="_blank" rel="noopener">设计模式</a>》。如果你觉得有点儿难度了，那么可以看一下《<a href="https://book.douban.com/subject/2243615/" target="_blank" rel="noopener">Head First 设计模式</a>》。学习面向对象的设计模式时，你不要迷失在那 23 个设计模式中，你一定要明白这两个原则：</p>
<ul>
<li><strong>Program to an ‘interface’, not an ‘implementation’</strong><ul>
<li>使用者不需要知道数据类型、结构、算法的细节。</li>
<li>使用者不需要知道实现细节，只需要知道提供的接口。</li>
<li>利于抽象、封装，动态绑定，多态。符合面向对象的特质和理念。</li>
</ul>
</li>
<li><strong>Favor ‘object composition’ over ‘class inheritance’</strong><ul>
<li>继承需要给子类暴露一些父类的设计和实现细节。</li>
<li>父类实现的改变会造成子类也需要改变。</li>
<li>我们以为继承主要是为了代码重用，但实际上在子类中需要重新实现很多父类的方法。</li>
<li>继承更多的应该是为了多态。</li>
</ul>
</li>
</ul>
<p>至此，如果你把上面的这些知识都融汇贯通的话，那么，你已是一个高级的 Java 程序员了，我保证你已经超过了绝大多数程序员了。基本上来说，你在技术方面是可以进入到一线公司的，而且还不是一般的岗位，至少是高级程序员或是初级架构师的级别了。</p>
<h2 id="C-C-语言"><a href="#C-C-语言" class="headerlink" title="C/C++ 语言"></a>C/C++ 语言</h2><p>不像我出道那个时候，几乎所有的软件都要用 C 语言来写。现在，可能不会有多少人学习 C 语言了，因为一方面有 Java、Python 这样的高级语言为你屏蔽了很多的底层细节，另一方面也有像 Go 语言这样的新兴语言可以让你更容易地写出来也是高性能的软件。但是，我还是想说，C 语言是你必须学习的语言，因为这个世界上绝大多数编程语言都是 C-like 的语言，也是在不同的方面来解决 C 语言的各种问题。<strong>这里，我想放个比较武断话——如果你不学 C 语言，你根本没有资格说你是一个合格的程序员！</strong></p>
<ul>
<li>这里尤其推荐，已故的 C 语言之父 Dennis M. Ritchie 和著名科学家 Brian W. Kernighan 合作的圣经级的教科书《<a href="https://book.douban.com/subject/1139336/" target="_blank" rel="noopener">C 程序设计语言</a>》。注意，这本书是 C 语言原作者写的，其 C 语言的标准不是我们平时常说的 ANSI 标准，而是原作者的标准，又被叫作 K&amp;R C。但是这本书很轻薄，也简洁，不枯燥，是一本你可以拿着躺在床上看还不会看着看着睡着的书。</li>
<li>然后，还有一本非常经典的 C 语言的书《<a href="https://book.douban.com/subject/2280547/" target="_blank" rel="noopener">C 语言程序设计现代方法</a>》。有人说，这本书配合之前的 <a href="https://en.wikipedia.org/wiki/The_C_Programming_Language" target="_blank" rel="noopener">The C Programming Language</a> 那本书简真是无敌。我想说，这本书更实用，也够厚，完整覆盖了 C99 标准，习题的质量和水准也比较高。更好的是，探讨了现代编译器的实现，以及和 C++ 的兼容，还揭穿了各种古老的 C 语言的神话和信条……是相当相当干的一本学习 C 语言的书。</li>
</ul>
<p><strong>对了，千万不要看谭浩强的 C 语言的书。各种误导，我大学时就是用这本书学的 C，后来工作时被坑得不行</strong>。</p>
<p>在学习 C 语言的过程中，你一定会感到，C 语言这么底层，而且代码经常性地崩溃，经过一段时间的挣扎，你才开始觉得你从这个烂泥坑里快要爬出来了。但你还需要看看《<a href="https://book.douban.com/subject/2778632/" target="_blank" rel="noopener">C 陷阱与缺陷</a>》这本书，你会发现，这里面的坑不是一般大。</p>
<p>此时，如果你看过我的《编程范式游记》那个系列文章，你可能会发现 C 语言在泛型编程上的各种问题，这个时候我推荐你学习一下 C++ 语言。可能会有很多人觉得我说的 C++ 是个大坑。是的，这是世界目前来说最复杂也是最难的编程语言了。但是，<strong>C++ 是目前世界上范式最多的语言了，其做得最好的范式就是 “ 泛型编程 “，这在静态语言中，是绝对地划时代的一个事</strong>。</p>
<p>所以，你有必要学习一下 C++，看看 C++ 是如何解决 C 语言中的各种问题的。你可以先看看我的这篇文章 “<a href="https://coolshell.cn/articles/7992.html" target="_blank" rel="noopener">C++ 的坑真的多吗？</a>” ，有个基本认识。下面推荐几本 C++ 的书。</p>
<ul>
<li>《<a href="https://book.douban.com/subject/25708312/" target="_blank" rel="noopener">C++ Primer 中文版</a>》，这本书是久负盛名的 C++ 经典教程。书是有点厚，前面 1/3 讲 C 语言，后面讲 C++。C++ 的知识点实在是太多了，而且又有点晦涩。但是你主要就看几个点，一个是面向对象的多态，一个是模板和重载操作符，以及一些 STL 的东西。看看 C++ 是怎么玩泛型和函数式编程的。</li>
<li>如果你想继续研究，你需要看另外两本更为经典的书《<a href="https://book.douban.com/subject/5387403/" target="_blank" rel="noopener">Effective C++</a>》和《<a href="https://book.douban.com/subject/5908727/" target="_blank" rel="noopener">More Effective C++</a>》。 这两本书不厚，但是我读了 10 多年，每过一段时间再读一下，就会发现有更多的收获。这两本书的内容会随着你经历的丰富而变得丰富，这也是对我影响最大的两本书，其中影响最大的不是书中的那些 C++ 的东西，而是作者的思维方式和不断求真的精神，这真是太赞了。</li>
<li>学习 C/C++ 都是需要好好了解一下编译器到底干了什么事的。就像 Java 需要了解 JVM 一样，所以，这里还有一本非常非常难啃的书你可以挑战一下《<a href="https://book.douban.com/subject/10427315/" target="_blank" rel="noopener">深度探索 C++ 对象模型
</a>》。这本书是非常之经典的，看完后，C++ 对你来说就再也没有什么秘密可言。我以前写过的《<a href="https://coolshell.cn/articles/12165.html" target="_blank" rel="noopener">C++ 虚函数表解析</a>》，还有《<a href="https://coolshell.cn/articles/12176.html" target="_blank" rel="noopener">C++ 对象内存布局</a>》属于这个范畴。</li>
<li>还有 C++ 的作者 Bjarne Stroustrup 写的 <a href="http://www.stroustrup.com/bs_faq.html" target="_blank" rel="noopener">C++ FAQ</a> （<a href="http://www.stroustrup.com/bsfaqcn.html" target="_blank" rel="noopener">中文版</a>），也是非常值得一读的。</li>
</ul>
<h2 id="学习-Go-语言"><a href="#学习-Go-语言" class="headerlink" title="学习 Go 语言"></a>学习 Go 语言</h2><p>C 语言太原始了，C++ 太复杂了，Go 语言是不二之选。有了 C/C++ 的功底，学习 Go 语言非常简单。</p>
<p>首推 <a href="https://gobyexample.com/" target="_blank" rel="noopener">Go by Example</a> 作为你的入门教程。然后，<a href="https://go101.org/article/101.html" target="_blank" rel="noopener">Go 101</a> 也是一个很不错的在线电子书。如果你想看纸书的话，<a href="https://book.douban.com/subject/26337545/" target="_blank" rel="noopener">The Go Programming Language</a> 一书在豆瓣上有 9.2 分，但是国内没有卖的。（当然，我以前也写过两篇入门的供你参考 “<a href="https://coolshell.cn/articles/8460.html" target="_blank" rel="noopener">GO 语言简介（上）- 语法</a>” 和 “<a href="https://coolshell.cn/articles/8489.html" target="_blank" rel="noopener">GO 语言简介（下）- 特性</a>”）。</p>
<p>另外，Go 语言官方的 <a href="https://golang.org/doc/effective_go.html" target="_blank" rel="noopener">Effective Go</a> 是必读的，这篇文章告诉你如何更好地使用 Go 语言，以及 Go 语言中的一些原理。</p>
<p>Go 语言最突出之处是并发编程，Unix 老牌黑客罗勃·派克（Rob Pike）在 Google I/O 上的两个分享，可以让你学习到一些并发编程的模式。</p>
<ul>
<li>Go Concurrency Patterns（ <a href="https://talks.golang.org/2012/concurrency.slide" target="_blank" rel="noopener">幻灯片</a>和<a href="https://www.youtube.com/watch?v=f6kdp27TYZs" target="_blank" rel="noopener">演讲视频</a>）。</li>
<li>Advanced Go Concurrency Patterns（<a href="https://talks.golang.org/2013/advconc.slide" target="_blank" rel="noopener">幻灯片</a>、<a href="https://youtu.be/QDDwwePbDtw" target="_blank" rel="noopener">演讲视频</a>）。</li>
</ul>
<p>然后，Go 在 GitHub 的 wiki 上有好多不错的学习资源，你可以从中学习到多。比如：</p>
<ul>
<li><a href="https://github.com/golang/go/wiki/Articles" target="_blank" rel="noopener">Go 精华文章列表</a>。</li>
<li><a href="https://github.com/golang/go/wiki/Blogs" target="_blank" rel="noopener">Go 相关博客列表</a>。</li>
<li><a href="https://github.com/golang/go/wiki/GoTalks" target="_blank" rel="noopener">Go Talks</a>。</li>
</ul>
<p>此外，还有个内容丰富的 Go 资源列表 <a href="https://github.com/avelino/awesome-go" target="_blank" rel="noopener">Awesome Go</a>，推荐看看。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>在编程语言方面，推荐学习 C、C++、Java 和 Go 四门语言，并分别阐释了推荐的原因。</p>
<ul>
<li>我认为，C 语言是必须学习的语言，因为这个世界上绝大多数编程语言都是 C-like 的语言，也是在不同的方面来解决 C 语言的各种问题。</li>
<li>而 C++ 虽然复杂难学，但它几乎是目前世界上范式最多的语言了，其做得最好的范式就是 “ 泛型编程 “，这在静态语言中，是绝对地划时代的一个事。尤其要看看 C++ 是如何解决 C 语言中的各种问题的。</li>
<li>Java 是综合能力最强的语言。其实我是先学了 Java，然后又去学了 C++，之后去学了 C 语言的。C -&gt; C++ -&gt; Java 整条线融汇贯通，这对我未来的技术成长有非常大的帮助。</li>
<li>在文章最末，我推荐了 Go 语言，并给出了相关的学习资料。</li>
</ul>
<p>一个合格的程序员应该掌握几门语言。一方面，这会让你对不同的语言进行比较，让你有更多的思考。另一方面，这也是一种学习能力的培养，会让你对于未来的新技术学习得更快。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

        
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/06/%E7%B3%BB%E7%BB%9F%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Yuanchen">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Yuanchen's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/06/%E7%B3%BB%E7%BB%9F%E7%9F%A5%E8%AF%86/" class="post-title-link" itemprop="url">计算机系统知识</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-06 16:50:40 / Modified: 17:55:01" itemprop="dateCreated datePublished" datetime="2020-02-06T16:50:40-05:00">2020-02-06</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>首先推荐的是翻译版图书《<a href="https://book.douban.com/subject/5333562/" target="_blank" rel="noopener">深入理解计算机系统</a>》，原书名为《Computer Systems A Programmer’s Perspective》。不过，这本书叫做《程序员所需要了解的计算机知识》更为合适。</p>
<p>本书的最大优点是为程序员描述计算机系统的实现细节，帮助其在大脑中构造一个层次型的计算机系统。从最底层的数据在内存中的表示到流水线指令的构成，到虚拟存储器，到编译系统，到动态加载库，到最后的用户态应用。通过掌握程序是如何映射到系统上，以及程序是如何执行的，你能够更好地理解程序的行为为什么是这样的，以及效率低下是如何造成的。</p>
<p><strong>再强调一下，这本书是程序员必读的一本书！</strong></p>
<p>然后就是美国计算机科学家 <a href="https://zh.wikipedia.org/wiki/理查德·史蒂文斯" target="_blank" rel="noopener">理查德·史蒂文斯（Richard Stevens）</a> 的三套巨经典无比的书。（理查德·史蒂文斯于 1999 年 9 月 1 日离世，终年 48 岁。死因不详，有人说是滑雪意外，有人说是攀岩意外，有人说是滑翔机意外。总之，家人没有透露。大师的 <a href="http://www.kohala.com/start/" target="_blank" rel="noopener">个人主页</a> 今天还可以访问。）</p>
<ul>
<li>《<a href="https://book.douban.com/subject/1788421/" target="_blank" rel="noopener">Unix 高级环境编程</a>》。</li>
<li>《Unix 网络编程》 <a href="https://book.douban.com/subject/1500149/" target="_blank" rel="noopener">第 1 卷 套接口 API</a> 、<a href="https://book.douban.com/subject/4118577/" target="_blank" rel="noopener">第 2 卷 进程间通信</a> 。</li>
<li>《<a href="https://book.douban.com/subject/1088054/" target="_blank" rel="noopener">TCP/IP 详解 卷 I 协议</a>》。</li>
</ul>
<p>这几书的地位我就不多说了，你可以自己看相关的书评。但是，这三本书可能都不容易读，一方面是比较厚，另一方面是知识的密度太大了，所以，读起来有点枯燥和乏味。但是，这没办法，你得忍住。</p>
<p>这里要重点说一下《TCP/IP 详解》这本书，是一本很奇怪的书。这本书迄今至少被 <a href="http://portal.acm.org/citation.cfm?id=161724" target="_blank" rel="noopener">近五百篇学术论文引用过</a> 。这本写给工程师看的书居然被各种学院派的论文来引用，也是很神奇的一件事了。而且，虽然理查德·史蒂文斯不是 TCP 的发明人，但是这本书中把这个协议深入浅出地讲出来，还画了几百张时序图，也是令人叹为观止了。</p>
<p>如果你觉得上面这几本经典书比较难啃，你可以试试下面这些通俗易懂的（当然，如果读得懂上面那三本的，下面的这些也就不需要读了）。</p>
<ul>
<li>《<a href="https://book.douban.com/subject/4141733/" target="_blank" rel="noopener">Linux C 编程一站式学习</a>》。</li>
<li>《<a href="https://book.douban.com/subject/25911735/" target="_blank" rel="noopener">TCP/IP 网络编程</a>》。</li>
<li>《<a href="https://book.douban.com/subject/24737674/" target="_blank" rel="noopener">图解 TCP/IP</a>》，这本书其实并不是只讲了 TCP/IP，应该是叫《计算机网络》才对，主要是给想快速入门的人看的。</li>
<li>《<a href="http://www.tcpipguide.com/free/index.htm" target="_blank" rel="noopener">The TCP/IP Guide</a>》，这本书在豆瓣上的评分 9.2，这里给的链接是这本书的 HTML 英文免费版的，里面的图画得很精彩。</li>
</ul>
<p>另外，学习网络协议不单只是看书，最好用个抓包工具看看这些网络包是什么样的。所以，这里推荐一本书《<a href="https://book.douban.com/subject/21691692/" target="_blank" rel="noopener">Wireshark 数据包分析实战</a>》。在这本书中，作者结合一些简单易懂的实际网络案例，图文并茂地演示使用 Wireshark 进行数据包分析的技术方法，可以让我们更好地了解和学习网络协议。当然，也拥有了一定的黑客的技能。</p>
<p>看完《Unix 高级环境编程》后，你可以趁热打铁看看《<a href="https://book.douban.com/subject/25809330/" target="_blank" rel="noopener">Linux/Unix 系统编程手册</a>》或是罗伯特·拉姆（Robert Love）的 <a href="http://igm.univ-mlv.fr/~yahya/progsys/linux.pdf" target="_blank" rel="noopener">Linux System Programming 英文电子版</a> 。其中文翻译版<a href="https://book.douban.com/subject/25828773/" target="_blank" rel="noopener">Linux 系统编程</a> 也值得一读，虽然和《Unix 高级环境编程》很像，不过其主要突出的是 Linux 的一些关键技术和相关的系统调用。</p>
<p>关于 TCP 的东西，你还可以看看下面这一系列的文章。</p>
<ul>
<li><a href="http://www.saminiir.com/lets-code-tcp-ip-stack-1-ethernet-arp/" target="_blank" rel="noopener">Let’s code a TCP/IP stack, 1: Ethernet &amp; ARP</a></li>
<li><a href="http://www.saminiir.com/lets-code-tcp-ip-stack-2-ipv4-icmpv4/" target="_blank" rel="noopener">Let’s code a TCP/IP stack, 2: IPv4 &amp; ICMPv4</a></li>
<li><a href="http://www.saminiir.com/lets-code-tcp-ip-stack-3-tcp-handshake/" target="_blank" rel="noopener">Let’s code a TCP/IP stack, 3: TCP Basics &amp; Handshake</a></li>
<li><a href="http://www.saminiir.com/lets-code-tcp-ip-stack-4-tcp-data-flow-socket-api/" target="_blank" rel="noopener">Let’s code a TCP/IP stack, 4: TCP Data Flow &amp; Socket API</a></li>
<li><a href="http://www.saminiir.com/lets-code-tcp-ip-stack-5-tcp-retransmission/" target="_blank" rel="noopener">Let’s code a TCP/IP stack, 5: TCP Retransmission</a></li>
</ul>
<p><strong>对于系统知识，主要有以下一些学习要点。</strong></p>
<ul>
<li>用这些系统知识操作一下文件系统，实现一个可以拷贝目录树的小程序。</li>
<li>用 fork / wait / waitpid 写一个多进程的程序，用 pthread 写一个多线程带同步或互斥的程序。比如，多进程购票的程序。</li>
<li>用 signal / kill / raise / alarm / pause / sigprocmask 实现一个多进程间的信号量通信的程序。</li>
<li>学会使用 gcc 和 gdb 来编程和调试程序（参看《<strong>用 gdb 调试程序</strong>》<a href="https://blog.csdn.net/haoel/article/details/2879" target="_blank" rel="noopener">一</a>、<a href="https://blog.csdn.net/haoel/article/details/2880" target="_blank" rel="noopener">二</a>、<a href="https://blog.csdn.net/haoel/article/details/2881" target="_blank" rel="noopener">三</a>、<a href="https://blog.csdn.net/haoel/article/details/2882" target="_blank" rel="noopener">四</a>、<a href="https://blog.csdn.net/haoel/article/details/2883" target="_blank" rel="noopener">五</a>、<a href="https://blog.csdn.net/haoel/article/details/2884" target="_blank" rel="noopener">六</a>、<a href="https://blog.csdn.net/haoel/article/details/2885" target="_blank" rel="noopener">七</a>）。</li>
<li>学会使用 makefile 来编译程序（参看《<strong>跟我一起写 makefile</strong>》<a href="https://blog.csdn.net/haoel/article/details/2886" target="_blank" rel="noopener">一</a>、<a href="https://blog.csdn.net/haoel/article/details/2887" target="_blank" rel="noopener">二</a>、<a href="https://blog.csdn.net/haoel/article/details/2888" target="_blank" rel="noopener">三</a>、<a href="https://blog.csdn.net/haoel/article/details/2889" target="_blank" rel="noopener">四</a>、<a href="https://blog.csdn.net/haoel/article/details/2890" target="_blank" rel="noopener">五</a>、<a href="https://blog.csdn.net/haoel/article/details/2891" target="_blank" rel="noopener">六</a>、<a href="https://blog.csdn.net/haoel/article/details/2892" target="_blank" rel="noopener">七</a>、<a href="https://blog.csdn.net/haoel/article/details/2893" target="_blank" rel="noopener">八</a>、<a href="https://blog.csdn.net/haoel/article/details/2894" target="_blank" rel="noopener">九</a>、<a href="https://blog.csdn.net/haoel/article/details/2895" target="_blank" rel="noopener">十</a>、<a href="https://blog.csdn.net/haoel/article/details/2896" target="_blank" rel="noopener">十一</a>、<a href="https://blog.csdn.net/haoel/article/details/2897" target="_blank" rel="noopener">十二</a>、<a href="https://blog.csdn.net/haoel/article/details/2898" target="_blank" rel="noopener">十三</a>、<a href="https://blog.csdn.net/haoel/article/details/2899" target="_blank" rel="noopener">十四</a>）。</li>
<li>Socket 的进程间通信。用 C 语言写一个 1 对 1 的聊天小程序，或是一个简单的 HTTP 服务器。</li>
</ul>
<h1 id="C10K-问题"><a href="#C10K-问题" class="headerlink" title="C10K 问题"></a>C10K 问题</h1><p>然后，当你读完《Unix 网络编程》后，千万要去读一下 “<a href="http://www.kegel.com/c10k.html" target="_blank" rel="noopener">C10K Problem</a> （<a href="https://www.oschina.net/translate/c10k" target="_blank" rel="noopener">中文翻译版</a>）”。提出这个问题的人叫丹·凯格尔（Dan Kegel），目前工作在美国 Google 公司。</p>
<p>他从 1978 年起开始接触计算机编程，是 Winetricks 的作者，也是 Wine 1.0 的管理员，同时也是 Crosstool（ 一个让 gcc/glibc 编译器更易用的工具套件）的作者。还是 Java JSR 51 规范的提交者并参与编写了 Java 平台的 NIO 和文件锁，同时参与了 RFC 5128 标准中有关 NAT 穿越（P2P 打洞）技术的描述和定义。</p>
<p>C10K 问题本质上是操作系统处理大并发请求的问题。对于 Web 时代的操作系统而言，对于客户端过来的大量的并发请求，需要创建相应的服务进程或线程。这些进程或线程多了，导致数据拷贝频繁（缓存 I/O、内核将数据拷贝到用户进程空间、阻塞）， 进程 / 线程上下文切换消耗大，从而导致资源被耗尽而崩溃。这就是 C10K 问题的本质。</p>
<p>了解这个问题，并了解操作系统是如何通过多路复用的技术来解决这个问题的，有助于你了解各种 I/O 和异步模型，这对于你未来的编程和架构能力是相当重要的。</p>
<p>另外，现在，整个世界都在解决 C10M 问题，推荐看看 <a href="http://highscalability.com/blog/2013/5/13/the-secret-to-10-million-concurrent-connections-the-kernel-i.html" target="_blank" rel="noopener">The Secret To 10 Million Concurrent Connections -The Kernel Is The Problem, Not The Solution</a> 一文。</p>
<h1 id="实践项目"><a href="#实践项目" class="headerlink" title="实践项目"></a>实践项目</h1><p>学习完了编程语言、理论学科和系统知识三部分内容，下面就来做几个实践项目，小试牛刀一下。实现语言可以用 C、C++ 或 Java。</p>
<p>实现一个 telnet 版本的聊天服务器，主要有以下需求。</p>
<ul>
<li>每个客户端可以用使用<code>telnet ip:port</code>的方式连接到服务器上。</li>
<li>新连接需要用用户名和密码登录，如果没有，则需要注册一个。</li>
<li>然后可以选择一个聊天室加入聊天。</li>
<li>管理员有权创建或删除聊天室，普通人员只有加入、退出、查询聊天室的权力。</li>
<li>聊天室需要有人数限制，每个人发出来的话，其它所有的人都要能看得到。</li>
</ul>
<p>实现一个简单的 HTTP 服务器，主要有以下需求。</p>
<ul>
<li>解释浏览器传来的 HTTP 协议，只需要处理 URL path。</li>
<li>然后把所代理的目录列出来。</li>
<li>在浏览器上可以浏览目录里的文件和下级目录。</li>
<li>如果点击文件，则把文件打开传给浏览器（浏览器能够自动显示图片、PDF，或 HTML、CSS、JavaScript 以及文本文件）。</li>
<li>如果点击子目录，则进入到子目录中，并把子目录中的文件列出来。</li>
</ul>
<p>实现一个生产者 / 消费者消息队列服务，主要有以下需求。</p>
<ul>
<li>消息队列采用一个 Ring-buffer 的数据结构。</li>
<li>可以有多个 topic 供生产者写入消息及消费者取出消息。</li>
<li>需要支持多个生产者并发写。</li>
<li>需要支持多个消费者消费消息（只要有一个消费者成功处理消息就可以删除消息）。</li>
<li>消息队列要做到不丢数据（要把消息持久化下来）。</li>
<li>能做到性能很高。</li>
</ul>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>学习完了专业编程方面最为重要的三部分内容：编程语言、理论学科和系统知识，针对这些内容做个小结。如果想看完推荐的那些书和知识，并能理解和掌握，估计怎么也得需要 4-5 年的时间。嗯，是的，就是一个计算机科学系科班出身的程序员需要学习的一些东西。这其中，最重要的是下面这几点。</p>
<p><strong>编程语言</strong>。以工业级的 C、C++、Java 这三门语言为主，这三门语言才是真正算得上工业级的编程语言，因为有工业级的标准化组织在控制着这几门语言，而且也有工业级的企业应用。尤其是 Java，还衍生出了大量的企业级架构上的开源生态。你至少需要掌握 C 语言和 Java 语言，这对你以后面对各式各样的编程语言是非常重要的。</p>
<p>此外，还推荐学习 Go 语言，它已成为云计算领域事实上的标准语言，尤其是在 Docker、Kubernetes 等项目中。而且，Go 语言在国内外一些知名公司中有了一定的应用和实践，并且其生态圈也越来越好。</p>
<p><strong>算法和数据结构</strong>。这个太重要了，尤其是最基础的算法和数据结构，这是任何一个称职的程序员都需要学习和掌握的。你必需要掌握。</p>
<p><strong>计算机的相关系统</strong>。你至少要掌握三个系统的基础知识，一个是操作系统，一个是网络系统，还有一个是数据库系统。它们分别代表着计算机基础构架的三大件——计算、存储、网络。</p>
<p>如果你能够走到这里，把前面的那些知识都了解了（不用精通，因为精通是需要时间和实践来慢慢锤炼出来的，所以，你也不用着急），那么你已经是一个非常非常合格的程序员了，而且你的潜力和可能性是非常非常高的。</p>
<p>如果经历过这些比较枯燥的理论知识，而且你还能有热情和成就感，那么我要恭喜你了。因为你已经超过了绝大多数人，而且还是排在上游的比较抢手的程序员了。我相信你至少可以找到年薪 50 万以上的工作了。</p>
<p>但是，你还需要很多的经验或是一些实践，以及一些大系统大项目的实际动手的经验。没关系，我们后面会有教你怎么实操的方法和攻略。</p>
<p>但是，往后面走，你需要开始需要术业有专攻了。下面给一些建议的方向。</p>
<ul>
<li><strong>底层方向</strong>：操作系统、文件系统、数据库、网络……</li>
<li><strong>架构方向</strong>：分布式系统架构、微服务、DevOps、Cloud Native……</li>
<li><strong>数据方向</strong>：大数据、机器学习、人工智能……</li>
<li><strong>前端方向</strong>：你对用户体验或是交互更感兴趣，那么你走前端的路吧。</li>
<li><strong>其它方向</strong>：比如，安全开发、运维开发、嵌入式开发……</li>
</ul>
<p>这些方向你要仔细选择，因为一旦选好，就要勇往直前地走下去，当然，你要回头转别的方向也没什么问题，因为你有前面的这些基础知识在身，所以，不用害怕。只是不同的方向上会有不同的经验积累，经验积累是看书看不来的，这个是转方向的成本。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/7/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><span class="page-number current">8</span><a class="page-number" href="/page/9/">9</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/9/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->
      <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=26924746&auto=1&height=66"></iframe>
      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Yuanchen</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">123</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">55</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yuanchen</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.0
  </div>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

</body>
</html>
