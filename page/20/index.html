<!DOCTYPE html>


<html lang="en">
	

		<head>
			<meta charset="utf-8" />
			 
			<meta name="keywords" content="life,think,work,blog,code" />
			 
			<meta name="description" content="a place holder" />
			
			<meta
				name="viewport"
				content="width=device-width, initial-scale=1, maximum-scale=1"
			/>
			<meta
				name="google-site-verification"
				content="Xe5wkkWgdmMwA81kCWOHLlJSlYSRE47NKPlVzl8ynK8"
			/>
			<title> Blog</title>
  <meta name="generator" content="hexo-theme-ayer">
			
			<link rel="shortcut icon" href="/favicon.ico" />
			 
<link rel="stylesheet" href="/dist/main.css">

			<link
				rel="stylesheet"
				href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
			/>
			
<link rel="stylesheet" href="/css/custom.css">
 
			<script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
			 
 

		<link rel="alternate" href="/atom.xml" title="Blog" type="application/atom+xml">
</head>
	</html>
</html>


<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      
<section class="cover">
	<div class="cover-frame">
		<div class="bg-box">
			<img src="/images/cover3.jpg" alt="image frame" />
		</div>
		<div class="cover-inner text-center text-white">
			<h1><a href="/">Blog</a></h1>
			<div id="subtitle-box">
				
				<span id="subtitle"></span>
				
			</div>
			<div>
				
			</div>
		</div>
	</div>
	<div class="cover-learn-more">
		<a href="javascript:void(0)" class="anchor"
			><i class="ri-arrow-down-line"></i
		></a>
	</div>
</section>
 
<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

<script>
	try {
	  var typed = new Typed("#subtitle", {
	    strings: ['It doesn&#39;t work...... why?', 'It works...... why?', 'I used to have a life... But Now I&#39;m a programmer.'],
	    startDelay: 100,
	    typeSpeed: 50,
	    loop: false,
	    backSpeed: 20,
	    showCursor: true
	  });
	} catch (err) {
	  console.log(err)
	}
</script>


<div id="main">
  <section class="outer">
  
  
  <article class="articles">
    
    
    
    
    <article
  id="post-如何超过大多数人"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/15/%E5%A6%82%E4%BD%95%E8%B6%85%E8%BF%87%E5%A4%A7%E5%A4%9A%E6%95%B0%E4%BA%BA/"
    >如何超过大多数人</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/02/15/%E5%A6%82%E4%BD%95%E8%B6%85%E8%BF%87%E5%A4%A7%E5%A4%9A%E6%95%B0%E4%BA%BA/" class="article-date">
  <time datetime="2020-02-15T14:38:10.000Z" itemprop="datePublished">2020-02-15</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h4 id="相关技巧和最佳实践"><a href="#相关技巧和最佳实践" class="headerlink" title="相关技巧和最佳实践"></a>相关技巧和最佳实践</h4><p>要超过别人其实还是比较简单的，尤其在今天的中国，更是简单。因为，你只看看中国的互联网，你就会发现，他们基本上全部都是在消费大众，让大众变得更为地愚蠢和傻瓜。<strong>所以，在今天的中国，你基本上不用做什么，只需要不使用中国互联网，你就很自然地超过大多数人了</strong>。当然，如果你还想跟他们彻底拉开，甩他们几个身位，把别人打到底层，下面的这些“技巧”你要多多了解一下。</p>
<p>在信息获取上，你要不断地向大众鼓吹下面的这些事：</p>
<ul>
<li>让大家都用百度搜索引擎查找信息，订阅微信公众号或是到知乎上学习知识……要做到这一步，你就需要把“百度一下”挂在嘴边，然后要经常在群或朋友圈中转发微信公众号的文章，并且转发知乎里的各种“如何看待……”这样的文章，让他们爱上八卦，爱上转发，爱上碎片。</li>
<li>让大家到微博或是知识星球上粉一些大咖，密切关注他们的言论和动向……是的，告诉大家，大咖的任何想法一言一行都可以在微博、朋友圈或是知识星球上获得，让大家相信，你的成长和大咖的见闻和闲扯非常有关系，你跟牛人在一个圈子里你也会变牛。</li>
<li>把今日头条和抖音这样的APP推荐给大家……你只需要让你有朋友成功地安装这两个APP，他们就会花大量的时间在上面，而不能自拔，要让他们安装其实还是很容易的，你要不信你就装一个试玩一会看看（嘿嘿嘿）。</li>
<li>让大家热爱八卦，八卦并不一定是明星的八卦，还可以是你身边的人，比如，公司的同事，自己的同学，职场见闻，社会热点，争议话题，……这些东西总有一些东西会让人心态有很多微妙的变化，甚至花大量的时间去搜索和阅读大量的观点，以及花大量时间与人辩论争论，这个过程会让人上瘾，让人欲罢不能，然而这些事却和自己没有半毛钱关系。你要做的事就是转发其中一些SB或是很极端的观点，造成大家的一睦讨论后，就早早离场……</li>
<li>利用爱国主义，让大家觉得不用学英文，不要出国，不要翻墙，咱们已经是强国了……这点其实还是很容易做到的，因为学习是比较逆人性的，所以，只要你鼓吹那些英文无用论，出国活得更惨，国家和民族都变得很强大，就算自己过得很底层，也有大国人民的感觉。</li>
</ul>
<p>然后，在知识学习和技能训练上，让他们不得要领并产生幻觉</p>
<ul>
<li>让他们混淆认识和知识，以为开阔认知就是学习，让他们有学习和成长的幻觉……</li>
<li>培养他们要学会使用碎片时间学习。等他们习惯利用碎片时间吃快餐后，他们就会失去精读一本书的耐性……</li>
<li>不断地给他们各种各样“有价值的学习资料”，让他们抓不住重点，成为一个微信公众号或电子书“收藏家”……</li>
<li>让他们看一些枯燥无味的基础知识和硬核知识，这样让他们只会用“死记硬背”的方式来学习，甚至直接让他们失去信心，直接放弃……</li>
<li>玩具手枪是易用的，重武器是难以操控的，多给他们一些玩具，这样他们就会对玩具玩地得心应手，觉得玩玩具就是自己的专业……</li>
<li>让他们喜欢直接得到答案的工作和学习方式，成为一个伸手党，从此学习再也不思考……</li>
<li>告诉他们东西做出来就好了，不要追求做漂亮，做优雅，这样他们就会慢慢地变成劳动密集型……</li>
<li>让他们觉得自己已经很努力了，剩下的就是运气，并说服他们去‘及时行乐’，然后再也找不到高阶和高效率学习的感觉……</li>
<li>让他们觉得“读完书”、“读过书”就行了，不需要对书中的东西进行思考，进行总结，或是实践，只要囫囵吞枣尽快读完就等同于学好了……</li>
</ul>
<p>最后，在认知和格局上，彻底打垮他们，让他们变成韭菜。</p>
<ul>
<li>让他们不要看到大的形势，只看到眼前的一亩三分地，做好一个井底之蛙。其实这很简单，比如，你不要让他们看到整个计算机互联网技术改变人类社会的趋势，你要多让他看到，从事这一行业的人有多苦逼，然后再说一下其它行业或职业有多好……</li>
<li>宣扬一夜暴富以及快速挣钱的案例，最好让他们进入“赌博类”或是“传销类”的地方，比如：股市、数字货币……要让他们相信各种财富神话，相信他们就是那个幸运儿，他们也可以成为巴菲特，可以成为马云……</li>
<li>告诉他们，一些看上去很难的事都是有捷径的，比如：21天就能学会机器学习，用区块链就能颠覆以及重构整个世界等等……</li>
<li>多跟他们讲一些小人物的励志的故事，这样让他们相信，不需要学习高级知识，不需要掌握高级技能，只需要用低等的知识和低级的技能，再加上持续不断拼命重复现有的工作，终有一天就会成功……</li>
<li>多让他们跟别人比较，人比人不会气死人，但是会让人变得浮躁，变得心急，变得焦虑，当一个人没有办法控制自己的情绪，没有办法让自己静下心来，人会失去耐性和坚持，开始好大喜欢功，开始装逼，开始歪门邪道剑走偏锋……</li>
<li>让他们到体制内的一些非常稳定的地方工作，这样他们拥有不思进取、怕承担责任、害怕犯错、喜欢偷懒、得过且过的素质……</li>
<li>让他们到体制外的那些喜欢拼命喜欢加班的地方工作，告诉他们爱拼才会赢，努力加班是一种福报，青春就是用来拼的，让他们喜欢上使蛮力的感觉……</li>
<li>告诉他们你的行业太累太辛苦，干不到30岁。让他们早点转行，不要耽误人生和青春……</li>
<li>当他们要做决定的时候，一定要让他们更多的关注自己会失去的东西，而不是会得到的东西。培养他们患得患失心态，让他们认识不到事物真正的价值，失去判断能力……（比如：让他们觉得跟对人拍领导的马屁忠于公司比自我的成长更有价值）</li>
<li>告诉他们，你现有的技能和知识不用更新，就能过好一辈子，新出来的东西没有生命力的……这样他们就会像我们再也不学习的父辈一样很快就会被时代所抛弃……</li>
<li>每个人都喜欢在一些自己做不到的事上找理由，这种能力不教就会，比如，事情太多没有时间，因为工作上没有用到，等等，你要做的就是帮他们为他们做不到的事找各种非常合理的理由，比如：没事的，一切都是最好的安排；你得不到的那个事没什么意思；你没有面好主要原因是那个面试官问的问题都是可以上网查得到的知识，而不没有问到你真正的能力上；这些东西学了不用很快会忘了，等有了环境再学也不迟……</li>
</ul>
<p><strong>最后友情提示一下，上述的这些“最佳实践”你要小心，是所谓，贩毒的人从来不吸毒，开赌场的人从来不赌博！所以，你要小心别自己也掉进去了！这就是“欲练神功，必先自宫”的道理。</strong></p>
<h4 id="相关原理和思维模型"><a href="#相关原理和思维模型" class="headerlink" title="相关原理和思维模型"></a>相关原理和思维模型</h4><p>一般来说，超过别人一般来说就是两个维度：</p>
<ol>
<li><strong>在认知、知识和技能上</strong>。这是一个人赖以立足社会的能力（参看《<a target="_blank" rel="noopener" href="https://coolshell.cn/articles/4235.html">程序员的荒谬之言还是至理名言？</a>》和《<a target="_blank" rel="noopener" href="https://coolshell.cn/articles/2250.html">21天教你学会C++</a>》）</li>
<li><strong>在领导力上</strong>。所谓领导力就是你跑在别人前面，你得要有比别人更好的能力更高的标准（参看《<a target="_blank" rel="noopener" href="https://coolshell.cn/articles/17583.html">技术人员发展之路</a>》）</li>
</ol>
<p>首先，我们要明白，人的技能是从认识开始，然后通过学校、培训或是书本把“零碎的认知”转换成“系统的知识”，而有要把知识转换成技能，就需要训练和实践，这样才能完成从：认识 -&gt; 知识 -&gt; 技能 的转换。这个转换过程是需要耗费很多时间和精力的，而且其中还需要有强大的学习能力和动手能力，这条路径上有很多的“关卡”，每道关卡都会过滤掉一大部分人。比如：对于一些比较枯燥的硬核知识来说，90%的人基本上就倒下来，不是因为他们没有智商，而是他们没有耐心。</p>
<h5 id="认知"><a href="#认知" class="headerlink" title="认知"></a>认知</h5><p>要在认知上超过别人，就要在下面几个方面上做足功夫：</p>
<p>1）<strong>信息渠道</strong>。试想如果别人的信息源没有你的好，那么，这些看不见信息源的人，只能接触得到二手信息甚至三手信息，只能获得被别人解读过的信息，这些信息被三传两递后必定会有错误和失真，甚至会被传递信息的中间人hack其中的信息（也就是“中间人攻击”），而这些找不出信息源的人，只能“被人喂养”，于是，他们最终会被困在信息的底层，永世不得翻身。（比如：学习C语言，放着原作者K&amp;R的不用，硬要用错误百出谭浩强的书，能有什么好呢？）</p>
<p>2）<strong>信息质量</strong>。信息质量主要表现在两个方面，一个是信息中的燥音，另一个是信息中的质量等级，我们都知道，在大数据处理中有一句名言，叫 garbage in garbage out，你天天看的都是垃圾，你的思想和认识也只有垃圾。所以，如果你的信息质量并不好的话，你的认知也不会好，而且你还要花大量的时间来进行有价值信息的挖掘和处理。</p>
<p>3）<strong>信息密度</strong>。优质的信息，密度一般都很大，因为这种信息会逼着你去干这么几件事，a）搜索并学习其关联的知识，b）沉思和反省，c）亲手去推理、验证和实践……一般来说，经验性的文章会比知识性的文章会更有这样的功效。比如，类似于像 Effiective C++/Java，设计模式，Unix编程艺术，算法导论等等这样的书就是属于这种密度很大的书，而像<a target="_blank" rel="noopener" href="https://medium.com/netflix-techblog">Netflix的官方blog</a>和<a target="_blank" rel="noopener" href="https://www.allthingsdistributed.com/">AWS CTO的blog</a>等等地方也会经常有一些这样的文章。</p>
<h5 id="知识"><a href="#知识" class="headerlink" title="知识"></a>知识</h5><p>要在知识上超过别人，你就需要在下面几个方面上做足功夫：</p>
<p>1）<strong>知识树（图）</strong>。任何知识，只在点上学习不够的，需要在面上学习，这叫系统地学习，这需要我们去总结并归纳知识树或知识图，一个知识面会有多个知识板块组成，一个板块又有各种知识点，一个知识点会导出另外的知识点，各种知识点又会交叉和依赖起来，学习就是要系统地学习整个知识树（图）。而我们都知道，<strong>对于一棵树来说，“根基”是非常重要的，所以，学好基础知识也是非常重要的，对于一个陌生的地方，有一份地图是非常重要的，没有地图的你只会乱窜，只会迷路、练路、走冤枉路！</strong></p>
<p>2）<strong>知识缘由</strong>。任何知识都是有缘由的，了解一个知识的来龙去脉和前世今生，会让你对这个知识有非常强的掌握，而不再只是靠记忆去学习。靠记忆去学习是一件非常糟糕的事。而对于一些操作性的知识（不需要了解由来的），我把其叫操作知识，就像一些函数库一样，这样的知识只要学会查文档就好了。<strong>能够知其然，知其所以然的人自然会比识知识到表皮的人段位要高很多。</strong></p>
<p>3）<strong>方法套路</strong>。学习不是为了找到答案，而是找到方法。就像数学一样，你学的是方法，是解题思路，是套路，会用方程式解题的和不会用方程式解题的在解题效率上不可比较，而在微积分面前，其它的解题方法都变成了渣渣。<strong>你可以看到，掌握高级方法的人比别人的优势有多大，学习的目的就是为了掌握更为高级的方法和解题思路</strong>。</p>
<h5 id="技能"><a href="#技能" class="headerlink" title="技能"></a>技能</h5><p>要在技能上超过别人，你就需要在下面几个方面做足功夫：</p>
<p>1）<strong>精益求精</strong>。如果你想拥有专业的技能，你要做不仅仅是拼命地重复一遍又一遍的训练，而是在每一次重复训练时你都要找到更好的方法，总结经验，让新的一遍能够更好，更漂亮，更有效率，否则，用相同的方法重复，那你只不过在搬砖罢了。</p>
<p>2）<strong>让自己犯错</strong>。犯错是有利于成长的，这是因为出错会让人反思，反思更好的方法，反思更完美的方案，总结教训，寻求更好更完美的过程，是技能升级的最好的方式。尤其是当你在出错后，被人鄙视，被人嘲笑后，你会有更大的动力提升自己，这样的动力才是进步的源动力。当然，千万不要同一个错误重复地犯！</p>
<p>3）<strong>找高手切磋</strong>。下过棋，打个球的人都知道，你要想提升自己的技艺，你必需找高手切磋，在和高手切磋的过程中你会感受到高手的技能和方法，有时候你会情不自禁地哇地一下，我靠，还可以这么玩！</p>
<h5 id="领导力"><a href="#领导力" class="headerlink" title="领导力"></a>领导力</h5><p>最后一个是领导力，要有领导力或是影响力这个事并不容易，这跟你的野心有多大，好胜心有多强 ，你愿意付出多少很有关系，因为一个人的领导力跟他的标准很有关系，因为有领导力的人的标准比绝大多数人都要高。</p>
<p>1）<strong>识别自己的特长和天赋</strong>。首先，每个人DNA都可能或多或少都会有一些比大多数人NB的东西（当然，也可能没有），如果你有了，那么在你过去的人生中就一定会表现出来了，就是那种大家遇到这个事会来请教你的寻求你帮助的现象。那种，别人要非常努力，而且毫不费劲的事。一旦你有了这样的特长或天赋，那你就要大力地扩大你的领先优势，千万不要进到那些会限制你优势的地方。你是一条鱼，你就一定要把别人拉到水里来玩，绝对不要去陆地上跟别人拼，不断地在自己的特长和天赋上扩大自己的领先优势，彻底一骑绝尘。</p>
<p>2）<strong>识别自己的兴趣和事业</strong>。没有天赋也没有问题，还有兴趣点，都说兴趣是最好的老师，当年，Linus就是在学校里对minx着迷了，于是整出个Linux来，这就是兴趣驱动出的东西，一般来说，兴趣驱动的事总是会比那些被动驱动的更好。但是，这里我想说明一下什么叫“真∙兴趣”，真正的兴趣不是那种三天热度的东西，而是那种，你愿意为之付出一辈子的事，是那种无论有多大困难有多难受你都要死磕的事，这才是“真∙兴趣”，这也就是你的“野心”和“好胜心”所在，其实上升到了你的事业。相信我，绝大多数人只有职业而没有事业的。</p>
<p>3）<strong>建立高级的习惯和方法</strong>。没有天赋没有野心，也还是可以跟别人拼习惯拼方法的，只要你有一些比较好的习惯和方法，那么你一样可以超过大多数人。对此，在习惯上你要做到比较大多数人更自律，更有计划性，更有目标性，比如，每年学习一门新的语言或技术，并可以参与相关的顶级开源项目，每个月训练一个类算法，掌握一种算法，每周阅读一篇英文论文，并把阅读笔记整理出来……自律的是非常可怕的。除此之外，你还需要在方法上超过别人，你需要满世界的找各种高级的方法，其中包括，思考的方法，学习的方法、时间管理的方法、沟通的方法这类软实力的，还有，解决问题的方法（trouble shooting 和 problem solving），设计的方法，工程的方法，代码的方法等等硬实力的，一开始照猫画虎，时间长了就可能会自己发明或推导新的方法。</p>
<p>4）<strong>勤奋努力执着坚持</strong>。如果上面三件事你都没有也没有能力，那还有最后一件事了，那就是勤奋努力了，就是所谓的“一万小时定律”了（参看《<a target="_blank" rel="noopener" href="https://coolshell.cn/articles/2250.html">21天教你学会C++</a>》中的十年学编程一节），我见过很多不聪明的人，悟性也不够（比如我就是一个），别人学一个东西，一个月就好了，而我需要1年甚至更长，但是很多东西都是死的，只要肯花时间就有一天你会搞懂的，耐不住我坚持十年二十年，聪明的人发明个飞机飞过去了，笨一点的人愚公移山也过得去，因为更多的人是懒人，我不用拼过聪明人，我只用拼过那些懒人就好了。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Chinese/" rel="tag">Chinese</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-git工作流"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/14/git%E5%B7%A5%E4%BD%9C%E6%B5%81/"
    >git工作流</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/02/14/git%E5%B7%A5%E4%BD%9C%E6%B5%81/" class="article-date">
  <time datetime="2020-02-15T01:58:00.000Z" itemprop="datePublished">2020-02-14</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="../images/0011a6d80f3f9d9cc36bcef5ad1d7f77.jpg" alt="0011a6d80f3f9d9cc36bcef5ad1d7f77"></p>
<p>与传统的代码版本管理工具相比，Git 有很多的优势，因而越来越成为程序员喜欢的版本管理工具。Git 这个代码版本管理工具最大的优势有以下几个。</p>
<ul>
<li>Git 是一个分布式的版本管理工具，而且可以是单机版的，所以，你在没有网络的时候同样可以提交（commit）代码。对于我们来说，这意味着在出差途中或是没有网络的环境中依然可以工作写代码。</li>
</ul>
<p>这是不是听起来有点不对？一方面，以后再也不能以将没有网络作为不工作的借口了。另一方面，没有网络意味着没有 Google 和 StackOverflow，光有个本地的 Git 我也一样不能写代码啊……</p>
<p>（哈哈。好吧，这已经超出了 Git 这个技术的范畴了，这里就不讨论了。）</p>
<ul>
<li>Git 从一个分支向另一个分支合并代码的时候，会把要合并的分支上的所有提交一个一个应用到被合并的分支上，合并后也能看得到整个代码的变更记录。而其他的版本管理工具则不能。</li>
<li>Git 切换分支的时候通常很快。不像其他版本管理器，每个分支一份拷贝。</li>
<li>Git 有很多非常有用的命令，让你可以很方便地工作。</li>
</ul>
<p>比如<code>git stash</code>命令，可以把当前没有完成的事先暂存一下，然后去忙别的事。<code>git cherry-pick</code>命令可以让你有选择地合并提交。<code>git add -p</code>可以让你挑选改动提交，<code>git grep $regexp $(git rev-list --all)</code>可以用来在所有的提交中找代码。因为都是本地操作，所以你会觉得飞快。</p>
<p>除此之外，由 Git 衍生出来的 GitHub/GitLab 可以帮你很好地管理编程工作，比如 wiki、fork、pull request、issue……集成了与编程相关的工作，让人觉得这不是一个冷冰冰的工具，而真正和我们的日常工作发生了很好的交互。</p>
<p>GitHub/GitLab 这样工具的出现，让我们的工作可以呈现在一个工作平台上，并以此来规范整个团队的工作，这才正是 Git 这个版本管理工具成功的原因。</p>
<p>今天，我们不讲 Git 是怎么用的，因为互联网上有太多的文章和书了。而且，如果你还不会用 Git 的话，那么你已经严重落后于这个时代了。在这篇文章中讲一下 Git 的协同工作流，因为很多团队在使用 Git 时，并没有用好。</p>
<p>注意，因为 Git 是一个分布式的代码管理器，所以，是分布式就会出现数据不一致的情况，因此，需要一个协同工作流来让工作变得高效，并可以有效地让代码具有更好的一致性。</p>
<p>说到一致性，就是每个人手里的开发代码，还有测试和生产线上的代码，要有一个比较好的一致性的管理和协同方法。这就是 Git 协同工作流所需要解决的问题。</p>
<p>GitFlow 工作流太过复杂，GitFlow 工作流是一个好的工作流。如果你的团队在用这种工作流开发软件，你的感觉一定是糟透了。</p>
<p>所以，这篇文章会对比一些比较主流的协同工作流，然后，再抨击一下 GitFlow 工作流。</p>
<h1 id="中心式协同工作流"><a href="#中心式协同工作流" class="headerlink" title="中心式协同工作流"></a>中心式协同工作流</h1><p>首先，先说明一下，Git 是可以像 SVN 这样的中心工作流一样工作的。很多程序员都是在采用这样的工作方式。</p>
<p>这个过程一般是下面这个样子的。</p>
<ol>
<li>从服务器上做<code>git pull origin master</code>把代码同步下来。</li>
<li>改完后，<code>git commit</code>到本地仓库中。</li>
<li>然后<code>git push origin master</code>到远程仓库中，这样其他同学就可以得到你的代码了。</li>
</ol>
<p>如果在第 3 步发现 push 失败，因为别人已经提交了，那么你需要先把服务器上的代码给 pull 下来，为了避免有 merge 动作，你可以使用 <code>git pull --rebase</code> 。这样就可以把服务器上的提交直接合并到你的代码中，对此，Git 的操作是这样的。</p>
<ol>
<li>先把你本地提交的代码放到一边。</li>
<li>然后把服务器上的改动下载下来。</li>
<li>然后在本地把你之前的改动再重新一个一个地做 commit，直到全部成功。</li>
</ol>
<p>如下图所示。Git 会把 Origin/Master 的远程分支下载下来（紫色的），然后把本地的 Master 分支上的改动一个一个地提交上去（蓝色的）。</p>
<p><img src="../images/5974a4026acca1000cd21772c4c52a6b.png" alt="img"></p>
<p>如果有冲突，那么你要先解决冲突，然后做 <code>git rebase --continue</code> 。如下图所示，git 在做 pull –rebase 时，会一个一个地应用（apply）本地提交的代码，如果有冲突就会停下来，等你解决冲突。</p>
<p><img src="../images/75b3fea18fa91b837f4f3ae6db6ab6e7.png" alt="img"></p>
<h1 id="功能分支协同工作流"><a href="#功能分支协同工作流" class="headerlink" title="功能分支协同工作流"></a>功能分支协同工作流</h1><p>上面的那种方式有一个问题，就是大家都在一个主干上开发程序，对于小团队或是小项目你可以这么干，但是对比较大的项目或是人比较多的团队，这么干就会有很多问题。</p>
<p>最大的问题就是代码可能干扰太严重。尤其是，我们想安安静静地开发一个功能时，我们想把各个功能的代码变动隔离开来，同时各个功能又会有多个开发人员在开发。</p>
<p>这时，我们不想让各个功能的开发人员都在 Master 分支上共享他们的代码。我们想要的协同方式是这样的：同时开发一个功能的开发人员可以分享各自的代码，但是不会把代码分享给开发其他功能的开发人员，直到整个功能开发完毕后，才会分享给其他的开发人员（也就是进入主干分支）。</p>
<p>因此，我们引入“功能分支”。这个协同工作流的开发过程如下。</p>
<ol>
<li>首先使用 <code>git checkout -b new-feature</code> 创建 “new-feature”分支。</li>
<li>然后共同开发这个功能的程序员就在这个分支上工作，进行 add、commit 等操作。</li>
<li>然后通过 <code>git push -u origin new-feature</code> 把分支代码 push 到服务器上。</li>
<li>其他程序员可以通过<code>git pull --rebase</code>来拿到最新的这个分支的代码。</li>
<li>最后通过 Pull Request 的方式做完 Code Review 后合并到 Master 分支上。</li>
</ol>
<p><img src="../images/455b921b2d178c87fe66714910301aec.png" alt="img"></p>
<p>就像上面这个图显示的一样，紫色的分支就是功能分支，合并后就会像上面这个样子。</p>
<p>其实，这种开发也是以服务器为中心的开发，还不是 Git 分布式开发，它只不过是用分支来完成代码改动的隔离。</p>
<p>另外，为什么会叫“功能分支”，而不是“项目分支”？因为 Git 的最佳实践希望大家在开发的过程中，快速提交，快速合并，快速完成。这样可以少很多冲突的事，所以叫功能分支。</p>
<p>传统的项目分支开得太久，时间越长就越合不回去。这种玩法其实就是让我们把一个大项目切分成若干个小项目来执行（最好是一个小功能一个项目）。这样才是互联网式的快速迭代式的开发流程。</p>
<h1 id="GitFlow-协同工作流"><a href="#GitFlow-协同工作流" class="headerlink" title="GitFlow 协同工作流"></a>GitFlow 协同工作流</h1><p>在真实的生产过程中，前面的协同工作流还是不能满足工作的要求。这主要因为生产过程是比较复杂的，软件生产中会有各式各样的问题，并要面对不同的环境。我们要在不停地开发新代码的同时，维护线上的代码，于是，就有了下面这些需求。</p>
<ol>
<li>希望有一个分支是非常干净的，上面是可以发布的代码，上面的改动永远都是可以发布到生产环境中的。这个分支上不能有中间开发过程中不可以上生产线的代码提交。</li>
<li>希望当代码达到可以上线的状态时，也就是在 alpha/beta release 时，在测试和交付的过程中，依然可以开发下一个版本的代码。</li>
<li>最后，对于已经发布的代码，也会有一些 Bug-fix 的改动，不会将正在开发的代码提交到生产线上去。</li>
</ol>
<p>你看，面对这些需求，前面的那些协同方式就都不行了。因为我们不仅是要在整个团队中共享代码，我们要的更是管理好不同环境下的代码不互相干扰。说得技术一点儿就是，要管理好代码与环境的一致性。</p>
<p>为了解决这些问题，GitFlow 协同工作流就出来了。</p>
<p>GitFlow 协同工作流是由 Vincent Driessen 于 2010 年在 A successful Git branching model 这篇文章介绍给世人的。</p>
<p>这个协同工作流的核心思想如下图所示。</p>
<p><img src="../images/9cf4c9bc17bf11aa07d47f61d2137fca.png" alt="img"></p>
<p>整个代码库中一共有五种分支。</p>
<ul>
<li>Master 分支。也就是主干分支，用作发布环境，上面的每一次提交都是可以发布的。</li>
<li>Feature 分支。也就是功能分支，用于开发功能，其对应的是开发环境。</li>
<li>Developer 分支。是开发分支，一旦功能开发完成，就向 Developer 分支合并，合并完成后，删除功能分支。这个分支对应的是集成测试环境。</li>
<li>Release 分支。当 Developer 分支测试达到可以发布状态时，开出一个 Release 分支来，然后做发布前的准备工作。这个分支对应的是预发环境。之所以需要这个 Release 分支，是我们的开发可以继续向前，不会因为要发布而被 block 住而不能提交。</li>
</ul>
<p>一旦 Release 分支上的代码达到可以上线的状态，那么需要把 Release 分支向 Master 分支和 Developer 分支同时合并，以保证代码的一致性。然后再把 Release 分支删除掉。</p>
<ul>
<li>Hotfix 分支。是用于处理生产线上代码的 Bug-fix，每个线上代码的 Bug-fix 都需要开一个 Hotfix 分支，完成后，向 Developer 分支和 Master 分支上合并。合并完成后，删除 Hotfix 分支。</li>
</ul>
<p>这就是整个 GitFlow 协同工作流的工作过程。我们可以看到：</p>
<ol>
<li>需要长期维护 Master 和 Developer 两个分支。</li>
<li>这其中的方式还是有一定复杂度的，尤其是 Release 和 Hotfix 分支需要同时向两个分支作合并。所以，如果没有一个好的工具来支撑的话，这会因为我们可能会忘了做一些操作而导致代码不一致。</li>
<li>GitFlow 协同虽然工作流比较重。但是它几乎可以应对所有公司的各种开发流程，包括瀑布模型，或是快速迭代模型。</li>
</ol>
<h1 id="GitHub-GitLab-协同工作流"><a href="#GitHub-GitLab-协同工作流" class="headerlink" title="GitHub/GitLab 协同工作流"></a>GitHub/GitLab 协同工作流</h1><h2 id="GitFlow-的问题"><a href="#GitFlow-的问题" class="headerlink" title="GitFlow 的问题"></a>GitFlow 的问题</h2><p>对于 GitFlow 来说，虽然可以解决问题，但是也有很多问题。在 GitFlow 流行了一段时间后，圈内出现了一些不同的声音。参看下面两篇吐槽文章。</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://endoflineblog.com/gitflow-considered-harmful">GitFlow considered harmful</a></li>
<li><a target="_blank" rel="noopener" href="http://luci.criosweb.ro/a-real-life-git-workflow-why-git-flow-does-not-work-for-us/">Why git flow does not work for us</a></li>
</ul>
<p>其中有个问题就是因为分支太多，所以会出现 git log 混乱的局面。具体来说，主要是 git-flow 使用<code>git merge --no-ff</code>来合并分支，在 git-flow 这样多个分支的环境下会让你的分支管理的 log 变得很难看。如下所示，左边是使用–no-ff 参数在多个分支下的问题。</p>
<p><img src="../images/13a78e9d493ba2737c3d6b8431be47b8.png" alt="img"></p>
<p>所谓–no-ff 参数的意思是——no fast forward 的意思。也就是说，合并的方法不要把这个分支的提交以前置合并的方式，而是留下一个 merge 的提交。这是把双刃剑，我们希望我们的–no-ff 能像右边那样，而不是像左边那样。</p>
<p>对此的建议是：只有 feature 合并到 developer 分支时，使用–no-ff 参数，其他的合并都不使用–no-ff 参数来做合并。</p>
<p>另外，还有一个问题就是，在开发得足够快的时候，你会觉得同时维护 Master 和 Developer 两个分支是一件很无聊的事，因为这两个分支在大多数情况下都是一样的。包括 Release 分支，你会觉得创建的这些分支太无聊。</p>
<p>而你的整个开发过程也会因为这么复杂的管理变得非常复杂。尤其当你想回滚某些人的提交时，你就会发现这事似乎有点儿不好干了。而且在工作过程中，你会来来回回地切换工作的分支，有时候一不小心没有切换，就提交到了不正确的分支上，你还要回滚和重新提交，等等。</p>
<p>GitLab 一开始是 GitFlow 的坚定支持者，后来因为这些吐槽，以及 Hacker News 和 Reddit 上大量的讨论，GitLab 也开始不玩了。他们写了<a target="_blank" rel="noopener" href="https://about.gitlab.com/2014/09/29/gitlab-flow/">一篇 blog</a>来创造了一个新的 Workflow——GitLab Flow，这个 GitLab Flow 是基于 GitHub Flow 来做的（参看：<a target="_blank" rel="noopener" href="http://scottchacon.com/2011/08/31/github-flow.html"> GitHub Flow</a> ）。</p>
<h2 id="GitHub-Flow"><a href="#GitHub-Flow" class="headerlink" title="GitHub Flow"></a>GitHub Flow</h2><p>所谓 GitHub Flow，其实也叫 Forking flow，也就是 GitHub 上的那个开发方式。</p>
<ol>
<li>每个开发人员都把“官方库”的代码 fork 到自己的代码仓库中。</li>
<li>然后，开发人员在自己的代码仓库中做开发，想干啥干啥。</li>
<li>因此，开发人员的代码库中，需要配两个远程仓库，一个是自己的库，一个是官方库（用户的库用于提交代码改动，官方库用于同步代码）。</li>
<li>然后在本地建“功能分支”，在这个分支上做代码开发。</li>
<li>这个功能分支被 push 到开发人员自己的代码仓库中。</li>
<li>然后，向“官方库”发起 pull request，并做 Code Review。</li>
<li>一旦通过，就向官方库进行合并。</li>
</ol>
<p>这就是 GitHub 的工作流程。</p>
<p>如果你有“官方库”的权限，那么就可以直接在“官方库”中建功能分支开发，然后提交 pull request。通过 Code Review 后，合并进 Master 分支，而 Master 一旦有代码被合并就可以马上 release。</p>
<p>这是一种非常 Geek 的玩法。这需要一个自动化的 CI/CD 工具做辅助。是的，CI/CD 应该是开发中的标配了。</p>
<h2 id="GitLab-Flow"><a href="#GitLab-Flow" class="headerlink" title="GitLab Flow"></a>GitLab Flow</h2><p>然而，GitHub Flow 这种玩法依然会有好多问题，因为其虽然变得很简单，但是没有把我们的代码和我们的运行环境给联系在一起。所以，GitLab 提出了几个优化点。</p>
<p>其中一个是引入环境分支，如下图所示，其包含了预发布（Pre-Production）和生产（Production）分支。</p>
<p><img src="../images/c9cf817612cc9d474cd253d26344e184.png" alt="img"></p>
<p>而有些时候，我们还会有不同版本的发布，所以，还需要有各种 release 的分支。如下图所示。Master 分支是一个 roadmap 分支，然后，一旦稳定了就建稳定版的分支，如 2.3.stable 分支和 2.4.stable 分支，其中可以 cherry-pick master 分支上的一些改动过去。</p>
<p><img src="../images/ed94b250461ca2bf6d7faa2d0aaa1a96.png" alt="img"></p>
<p>这样也就解决了两个问题：</p>
<ul>
<li>环境和代码分支对应的问题；</li>
<li>版本和代码分支对应的问题。</li>
</ul>
<p>老实说，对于互联网公司来说，环境和代码分支对应这个事，只要有个比较好的 CI/CD 生产线，这种环境分支应该也是没有必要的。而对于版本和代码分支的问题，我觉得这应该是有意义的，但是，最好不要维护太多的版本，版本应该是短暂的，等新的版本发布时，老的版本就应该删除掉了。</p>
<h1 id="协同工作流的本质"><a href="#协同工作流的本质" class="headerlink" title="协同工作流的本质"></a>协同工作流的本质</h1><p>对于上面这些各式各样的工作流的比较和思考，虽然，我个人非常喜欢 GitHub Flow，在必要的时候使用上 GitLab 中的版本或环境分支。不过，我们现实生活中，还是有一些开发工作不是以功能为主要，而是以项目为主的。也就是说，项目的改动量可能比较大，时间和周期可能也比较长。</p>
<p>我在想，是否有一种工作流，可以面对我们现实工作中的各种情况。但是，我想这个世界太复杂了，应该不存在一种一招鲜吃遍天的放之四海皆准的银弹方案。所以，我们还要根据自己的实际情况来挑选适合我们的协同工作的方式。</p>
<p>而代码的协同工作流属于 SCM（Software Configuration Management）的范畴，要挑选好适合自己的方式，我们需要知道软件工程配置管理的本质。根据这么多年来我在各个公司的经历，有互联网的，有金融的，有项目的，有快速迭代的等，我认为团队协同工作的本质不外乎这么几个事儿。</p>
<ol>
<li>不同的团队能够尽大可能地并行开发。</li>
<li>不同软件版本和代码的一致性。</li>
<li>不同环境和代码的一致性。</li>
<li>代码总是会在稳定和不稳定间交替。我们希望生产线上的代码总是能对应到稳定的代码上来。</li>
</ol>
<p>基本上述的四个事儿，上述的工作流大都是在以建立不同的分支，来做到开发并行、代码和环境版本一致，以及稳定的代码。</p>
<p>要选择适合自己的协同工作流，我们就不得不谈一下软件开发的工作模式。</p>
<p>首先，我们知道软件开发的趋势一定是下面这个样子的。</p>
<ul>
<li><strong>以微服务或是 SOA 为架构的方式</strong>。一个大型软件会被拆分成若干个服务，那么，我们的代码应该也会跟着服务拆解成若干个代码仓库。这样一来，我们的每个代码仓库都会变小，于是我们的协同工作流程就会变简单。</li>
</ul>
<p>对于每个服务的代码仓库，我们的开发和迭代速度也会变得很快，开发团队也会跟服务一样被拆分成多个小团队。这样一来， Gitflow 这种协同工作流程就非常重了，而 GitHub 这种方式或是功能分支这种方式会更适合我们的开发。</p>
<ul>
<li><strong>以 DevOps 为主的开发流程</strong>。DevOps 关注于 CI/CD，需要我们有自动化的集成测试和持续部署的工具。这样一来，我们的代码发布速度就会大大加快，每一次提交都能很快地被完整地集成测试，并很快地发布到生产线上。</li>
</ul>
<p>于是，就可以使用更简单的协同工作流程，不需要维护多个版本，也不需要关注不同的运行环境，只需要一套代码，就可以了。GitHub Flow 或是功能分支这种方式也更适应这种开发。</p>
<p>如果将软件开发升级并简化到 SOA 服务化以及 DevOps 上来，那么协同工作流就会变得非常简单。所以，协同工作流的本质，并不是怎么玩好代码仓库的分支策略，而是玩好我们的软件架构和软件开发流程。</p>
<p>当然，服务化和 DevOps 是每个开发团队需要去努力的目标，但就算是这样，也有某些情况需要用重的协同工作的模式。比如，整个公司在做一个大的升级项目，这其中会对代码做一个大的调整（很有可能是一次重大的重构）。</p>
<p>这个时候，可能还有一些并行的开发需要做，如一些小功能的优化，一些线上 Bug 的处理，可能还需要在生产线上做新旧两个版本的 A/B 测试。在这样的情况下，可能会或多或少地使用 GitFlow 协同工作流。</p>
<p>但是，这样的方式不会是常态，是特殊时期，不可能隔三差五地对系统做架构或是对代码做大规模的重构。所以，在大多数情况下，还是应该选择一个比较轻量的协同工作流，而在特殊时期特例特办。</p>
<p>最后，用一句话来结束这篇文章——<strong>与其花时间在 Git 协同工作流上，还不如把时间花在调整软件架构和自动化软件生产和运维流程上来，这才是真正简化协同工作流程的根本</strong>。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Chinese/" rel="tag">Chinese</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Git/" rel="tag">Git</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-System-Design-Questions"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/13/System-Design-Questions/"
    >System_Design_Questions</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/02/13/System-Design-Questions/" class="article-date">
  <time datetime="2020-02-13T14:27:37.000Z" itemprop="datePublished">2020-02-13</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>System design questions are an important part of programming job interviews, and if you want to do well, you must prepare this topic. In the past, when I shared my list of <a target="_blank" rel="noopener" href="http://www.java67.com/2018/05/top-75-programming-interview-questions-answers.html">programming interview questions</a>, I shared a couple of system design questions, but my readers kept asking me for more questions, as it is a hard topic to master and more and more practice is needed.</p>
<p>I had my own list of questions that I collected from various interviews with friends and colleagues, but I needed more questions for this article and my own preparation. Then, I stumbled upon the <a target="_blank" rel="noopener" href="https://www.educative.io/collection/5668639101419520/5649050225344512?affiliate_id=5073518643380224">Grokking System Design Interview</a> course.</p>
<p>It’s an excellent resource because it not only asks you a lot of system design questions but also provides the knowledge and tools you need to solve these problems. I have yet to find a similar or better resource than this one on system design, one of the important topics for <a target="_blank" rel="noopener" href="http://javarevisited.blogspot.sg/2012/06/20-design-pattern-and-software-design.html#axzz55mVSPFfH">software engineering interviews</a>.</p>
<p>In other words, this course teaches you to step by step how to proceed with designing a real-world system like Facebook, Twitter, Uber, etc.</p>
<p>When you combine this course with this list of questions, you have the best material to prepare for your system design interview. You can also first try all these questions by yourself before joining the course or looking at my solutions for some of the questions.</p>
<h2 id="System-Design-Interview-Questions-for-Programmers"><a href="#System-Design-Interview-Questions-for-Programmers" class="headerlink" title="System Design Interview Questions for Programmers"></a>System Design Interview Questions for Programmers</h2><p>Without any further ado, here is the list of some of the most popular system design or object-oriented analysis and design questions to crack any programming job interview.</p>
<h3 id="1-How-to-design-a-vending-machine-in-Java-solution"><a href="#1-How-to-design-a-vending-machine-in-Java-solution" class="headerlink" title="1. How to design a vending machine in Java? (solution)"></a><strong>1. How to design a vending machine in Java?</strong> (<a target="_blank" rel="noopener" href="http://javarevisited.blogspot.sg/2016/06/design-vending-machine-in-java.html#axzz4sZVwtCgs">solution</a>)</h3><p>You need to write code to implement a vending machine that has a bunch of products, e.g. chocolates, candy, cold drinks, and accepts different coins, e.g. nickles, dimes, quarters, cent, etc. Make sure you insert a coin, get a product back, and get your change back. Also, write the unit test to demonstrate that these common use cases work. If you get stuck, you can read my two-part series (<a target="_blank" rel="noopener" href="http://javarevisited.blogspot.sg/2016/06/design-vending-machine-in-java.html#axzz4sZVwtCgs">Part 1</a> and <a target="_blank" rel="noopener" href="http://javarevisited.blogspot.sg/2016/06/java-object-oriented-analysis-and-design-vending-machine-part-2.html">Part 2</a>) about solving these classical system design questions.</p>
<h3 id="2-How-to-design-a-URL-shortening-service-like-goo-gl"><a href="#2-How-to-design-a-URL-shortening-service-like-goo-gl" class="headerlink" title="2. How to design a URL shortening service like goo.gl?"></a><strong>2. How to design a URL shortening service like goo.gl?</strong></h3><p>This one is another common system design question. If you are given a (typically) long URL, how would you design a service that would generate a shorter and unique alias for it? If you are not familiar with the URL shortener service, have a look at some of the popular ones like goo.gl from <a target="_blank" rel="noopener" href="https://javarevisited.blogspot.com/2012/01/google-interview-questions-answers-top.html">Google</a> and bit.ly, which is used by Twitter.</p>
<p>Make sure to provide a database schema and rationale behind some design decisions, e.g. how long you keep the data, how to get stats and analytics, etc. If you get stuck, you can follow the solution given on <a target="_blank" rel="noopener" href="https://www.educative.io/collection/5668639101419520/5649050225344512?affiliate_id=5073518643380224">System Design Interviews: Grokking the System Design Interview</a>.</p>
<h3 id="3-How-to-design-a-limit-order-book"><a href="#3-How-to-design-a-limit-order-book" class="headerlink" title="3. How to design a limit order book?"></a><strong>3. How to design a limit order book?</strong></h3><p>A limit order book is used in stock exchanges to match a buy order with a sell order based on price and time priority. How would you go about that? Which <a target="_blank" rel="noopener" href="https://hackernoon.com/10-data-structure-algorithms-and-programming-courses-to-crack-any-coding-interview-e1c50b30b927">data structure</a> you will use? Remember: the speed of matching is key, as well as the reliability. If you need a refresher on data structures, then you can check out a data structure and algorithm Java course, and if you feel stuck, you can check out my solution <a target="_blank" rel="noopener" href="https://javarevisited.blogspot.sg/2017/03/2-practical-data-structure-algorithm-interview-questions-java.html">here</a>.</p>
<h3 id="4-How-to-design-a-traffic-control-system"><a href="#4-How-to-design-a-traffic-control-system" class="headerlink" title="4. How to design a traffic control system?"></a><strong>4. How to design a traffic control system?</strong></h3><p>A classical system design question from old age is still popular. Make sure you know how to transition from one state to another like RED to GREEN and from GREEN to ORANGE to RED, etc. One tip is you can use the <a target="_blank" rel="noopener" href="http://javarevisited.blogspot.sg/2014/04/difference-between-state-and-strategy-design-pattern-java.html">state design pattern</a> because there is a clear transition of state from one to other. For example, you cannot go from RED to GREEN before ORANGE or so on.</p>
<p>You can also use the <a target="_blank" rel="noopener" href="http://www.java67.com/2018/07/java-enum-tutorial-10-things-java-devs.html">Enum</a> to represent the state and implement the pattern, just as we did with the <a target="_blank" rel="noopener" href="https://javarevisited.blogspot.com/2014/11/strategy-design-pattern-in-java-using-Enum-Example.html">strategy design pattern</a> in my last article.</p>
<p>If you are not familiar with these essential patterns, then I suggest you to first take a look at this <strong><a target="_blank" rel="noopener" href="https://pluralsight.pxf.io/c/1193463/424552/7490?u=https%3A%2F%2Fwww.pluralsight.com%2Fcourses%2Fpatterns-library">Design Pattern Library </a></strong>course on Pluarslight, one of the better resources to learn this pattern and get insight on how and when to use them in the real world.</p>
<h3 id="5-How-to-design-a-website-like-Pastebin"><a href="#5-How-to-design-a-website-like-Pastebin" class="headerlink" title="5. How to design a website like Pastebin?"></a><strong>5. How to design a website like Pastebin?</strong></h3><p>Pastebin allows you to paste text or code and then share a link to that code anywhere you want. It’s not an online code editor but you can use this to store any kind of text.</p>
<h3 id="6-How-would-you-create-your-own-Instagram"><a href="#6-How-would-you-create-your-own-Instagram" class="headerlink" title="6. How would you create your own Instagram?"></a><strong>6. How would you create your own Instagram?</strong></h3><p>Instagram is a photo sharing application that provides some custom filters to enhance your photo quality.</p>
<h3 id="7-How-to-design-a-global-file-sharing-and-storage-apps-like-Google-Drive-or-Dropbox"><a href="#7-How-to-design-a-global-file-sharing-and-storage-apps-like-Google-Drive-or-Dropbox" class="headerlink" title="7. How to design a global file sharing and storage apps like Google Drive or Dropbox?"></a><strong>7. How to design a global file sharing and storage apps like Google Drive or Dropbox?</strong></h3><p>These are used to store and share files, photos, and other media. How do you go about designing things like allowing users to upload/view/search/share files or photos, track permissions for file sharing, and allow multiple users to edit the same document?</p>
<h3 id="8-How-to-design-a-chat-application-like-WhatsApp-or-Facebook-Messenger"><a href="#8-How-to-design-a-chat-application-like-WhatsApp-or-Facebook-Messenger" class="headerlink" title="8. How to design a chat application like WhatsApp or Facebook Messenger?"></a><strong>8. How to design a chat application like WhatsApp or Facebook Messenger?</strong></h3><p>You have surely used WhatsApp and Facebook, right? No? If not, let me tell you that a chat application allows you to send messages to your friend. It’s a point-to-point connection. You keep a friends list, view their status, and chat with them.</p>
<p>In WhatsApp, you can also connect groups, but that is for advanced and experienced developers. At a minimum, you should provide a design to keep the friends list and send and receive messages from them.</p>
<h3 id="9-How-to-design-a-Twitter-Clone"><a href="#9-How-to-design-a-Twitter-Clone" class="headerlink" title="9. How to design a Twitter Clone?"></a><strong>9. How to design a Twitter Clone?</strong></h3><p>Twitter is a popular messaging service that lets you broadcast messages to all of your followers. You post a tweet and your followers see those messages. Additionally, they can like or retweet your post. Make sure you implement common features like followers, hashtags, tweets, deletes, etc. If you do not feel like going anywhere and are stuck, you can follow the solution on <a target="_blank" rel="noopener" href="https://www.educative.io/collection/5668639101419520/5649050225344512?affiliate_id=5073518643380224">System Design Interviews: Grokking the System Design Interview</a>.</p>
<h3 id="10-How-to-design-a-global-video-streaming-service-e-g-YouTube-or-Netflix"><a href="#10-How-to-design-a-global-video-streaming-service-e-g-YouTube-or-Netflix" class="headerlink" title="10. How to design a global video streaming service, e.g. YouTube or Netflix?"></a><strong>10. How to design a global video streaming service, e.g. YouTube or Netflix?</strong></h3><p>While designing a video streaming service like NetFlix or YouTube, the key concept to remember is to smooth streaming and buffering and functioning over low bandwidth connection and how you manage those challenges.</p>
<h3 id="11-How-to-design-an-ATM-machine"><a href="#11-How-to-design-an-ATM-machine" class="headerlink" title="11. How to design an ATM machine?"></a><strong>11. How to design an ATM machine?</strong></h3><p>An ATM machine allows a user to deposit and withdraw cash. It also allows a user to check their balance. How do you design such a system? What are your main challenges?</p>
<h3 id="12-How-to-design-an-API-Rate-Limiter"><a href="#12-How-to-design-an-API-Rate-Limiter" class="headerlink" title="12. How to design an API Rate Limiter?"></a><strong>12. How to design an API Rate Limiter?</strong></h3><p>This is a challenging one. You may want to find more information about it, starting with a Google search.</p>
<h3 id="13-How-to-design-Twitter-Search"><a href="#13-How-to-design-Twitter-Search" class="headerlink" title="13. How to design Twitter Search?"></a><strong>13. How to design Twitter Search?</strong></h3><p>To be honest, Twitter search is poor — unless you enter the exact title of your tweet, you won’t be able to find anything. So, this one should be easy to implement<strong>.</strong></p>
<h3 id="14-How-to-design-a-web-crawler-like-Google"><a href="#14-How-to-design-a-web-crawler-like-Google" class="headerlink" title="14. How to design a web crawler like Google?"></a><strong>14. How to design a web crawler like Google?</strong></h3><p>A web crawler goes to a website and crawls all links and indexes, like Google, so that they can later appear in a search result. A crawler can also be used for searching a particular file in a set of directories. But be sure to ask yourself about some of the major challenges.</p>
<h3 id="15-How-to-design-Facebook’s-Newsfeed-What-kind-of-algorithm-will-you-use"><a href="#15-How-to-design-Facebook’s-Newsfeed-What-kind-of-algorithm-will-you-use" class="headerlink" title="15. How to design Facebook’s Newsfeed? What kind of algorithm will you use?"></a><strong>15. How to design Facebook’s Newsfeed? What kind of algorithm will you use?</strong></h3><p>The newsfeed is an important part of Facebook that allows a user to see what’s happening around the world, including friends, families, pages he has liked, a group he has followed, and of course, the Facebook ads.</p>
<p>The job of the newsfeed algorithm is to show messages that are most important to the user and which can generate high engagement. Obviously, messages from friends and family should take priority. If you feel not going anywhere and stuck, you can follow the solution on<strong><a target="_blank" rel="noopener" href="https://www.educative.io/collection/5668639101419520/5649050225344512?affiliate_id=5073518643380224">System Design Interviews: Grokking the System Design Interview</a>.</strong></p>
<h3 id="16-How-to-design-Yelp-or-Nearby-Friends"><a href="#16-How-to-design-Yelp-or-Nearby-Friends" class="headerlink" title="16. How to design Yelp or Nearby Friends?"></a><strong>16. How to design Yelp or Nearby Friends?</strong></h3><p>Have you used Yelp? If not first use it to understand how it works before you can design a similar system. For those who don’t know Yelp is a local-search service powered by crowd-sourced review forum</p>
<h3 id="17-How-to-design-a-global-ride-hailing-service-e-g-Uber-Grab-or-Ola-backend"><a href="#17-How-to-design-a-global-ride-hailing-service-e-g-Uber-Grab-or-Ola-backend" class="headerlink" title="17. How to design a global ride-hailing service e.g. Uber, Grab, or Ola backend?"></a><strong>17. How to design a global ride-hailing service e.g. Uber, Grab, or Ola backend?</strong></h3><p>Uber and Ola are two of the most popular ride-hailing services, it brings both drivers and passengers together. How do you go about designing to allow a passenger to see nearby taxis and book them?</p>
<h3 id="18-How-to-design-BookMyShow"><a href="#18-How-to-design-BookMyShow" class="headerlink" title="18. How to design BookMyShow?"></a><strong>18. How to design BookMyShow?</strong></h3><p>A website that allows you to book cinema and event tickets.</p>
<h3 id="19-How-to-design-a-social-network-message-board-service-sites-like-Quora-Reddit-or-HackerNews"><a href="#19-How-to-design-a-social-network-message-board-service-sites-like-Quora-Reddit-or-HackerNews" class="headerlink" title="19. How to design a social network + message board service sites like Quora, Reddit, or HackerNews?"></a><strong>19. How to design a social network + message board service sites like Quora, Reddit, or HackerNews?</strong></h3><p>Reddit, Quora, and HackerNews are some of the most popular social network sites where users can post questions or share links. Other users can answer questions or comment on the shared links.</p>
<h3 id="20-How-do-you-design-an-application-like-Airbnb"><a href="#20-How-do-you-design-an-application-like-Airbnb" class="headerlink" title="20. How do you design an application like Airbnb?"></a><strong>20. How do you design an application like Airbnb?</strong></h3><p>This allows users to upload rooms for rent and other users to rent them. Some features are only available to admins, publishers, and subscribers.</p>
<p>That’s all for now on some of the <strong>frequently asked system design interview questions for Java programmers</strong>. These questions are not just useful for Java programmers but also for <a target="_blank" rel="noopener" href="https://javarevisited.blogspot.com/2018/12/10-free-python-courses-for-programmers.html">Python</a>, <a target="_blank" rel="noopener" href="https://hackernoon.com/top-5-free-c-courses-to-learn-programming-in-2019-d27352277da0">C++</a>, and <a target="_blank" rel="noopener" href="http://www.java67.com/2018/02/5-free-ruby-and-rails-courses-to-learn-online.html">Ruby</a> programmers. These questions are actually independent of programming languages and test your software design and architecture skills.</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Interview/" rel="tag">Interview</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-分布式系统技术栈"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/12/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A0%88/"
    >分布式系统技术栈</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/02/12/%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E6%8A%80%E6%9C%AF%E6%A0%88/" class="article-date">
  <time datetime="2020-02-13T04:57:31.000Z" itemprop="datePublished">2020-02-12</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>构建分布式系统的目的是增加系统容量，提高系统的可用性，转换成技术方面，也就是完成下面两件事。</p>
<ul>
<li><strong>大流量处理</strong>。通过集群技术把大规模并发请求的负载分散到不同的机器上。</li>
<li><strong>关键业务保护</strong>。提高后台服务的可用性，把故障隔离起来阻止多米诺骨牌效应（雪崩效应）。如果流量过大，需要对业务降级，以保护关键业务流转。</li>
</ul>
<p>说白了就是干两件事。一是提高整体架构的吞吐量，服务更多的并发和流量，二是为了提高系统的稳定性，让系统的可用性更高。</p>
<h1 id="提高架构的性能"><a href="#提高架构的性能" class="headerlink" title="提高架构的性能"></a>提高架构的性能</h1><p>咱们先来看看，提高系统性能的常用技术。</p>
<ul>
<li><p><strong>缓存系统</strong>。加入缓存系统，可以有效地提高系统的访问能力。从前端的浏览器，到网络，再到后端的服务，底层的数据库、文件系统、硬盘和 CPU，全都有缓存，这是提高快速访问能力最有效的手段。对于分布式系统下的缓存系统，需要的是一个缓存集群。这其中需要一个 Proxy 来做缓存的分片和路由。</p>
</li>
<li><p><strong>负载均衡系统</strong>，是做水平扩展的关键技术。其可以用多台机器来共同分担一部分流量请求。</p>
</li>
<li><p><strong>异步调用</strong>。异步系统主要通过消息队列来对请求做排队处理，这样可以把前端的请求的峰值给“削平”了，而后端通过自己能够处理的速度来处理请求。这样可以增加系统的吞吐量，但是实时性就差很多了。同时，还会引入消息丢失的问题，所以要对消息做持久化，这会造成“有状态”的结点，从而增加了服务调度的难度。</p>
</li>
<li><p><strong>数据分区和数据镜像</strong>。<strong>数据分区</strong>是把数据按一定的方式分成多个区（比如通过地理位置），不同的数据区来分担不同区的流量。这需要一个数据路由的中间件，会导致跨库的 Join 和跨库的事务非常复杂。而<strong>数据镜像</strong>是把一个数据库镜像成多份一样的数据，这样就不需要数据路由的中间件了。你可以在任意结点上进行读写，内部会自行同步数据。然而，数据镜像中最大的问题就是数据的一致性问题。</p>
</li>
</ul>
<p>对于一般公司来说，在初期，会使用读写分离的数据镜像方式，而后期会采用分库分表的方式。</p>
<h1 id="提高架构的稳定性"><a href="#提高架构的稳定性" class="headerlink" title="提高架构的稳定性"></a>提高架构的稳定性</h1><p>接下来，咱们来看看提高系统系统稳定性的一些常用技术。</p>
<ul>
<li><strong>服务拆分</strong>，主要有两个目的：一是为了隔离故障，二是为了重用服务模块。但服务拆分完之后，会引入服务调用间的依赖问题。</li>
<li><strong>服务冗余</strong>，是为了去除单点故障，并可以支持服务的弹性伸缩，以及故障迁移。然而，对于一些有状态的服务来说，冗余这些有状态的服务带来了更高的复杂性。其中一个是弹性伸缩时，需要考虑数据的复制或是重新分片，迁移的时候还要迁移数据到其它机器上。</li>
<li><strong>限流降级</strong>。当系统实在扛不住压力时，只能通过限流或者功能降级的方式来停掉一部分服务，或是拒绝一部分用户，以确保整个架构不会挂掉。这些技术属于保护措施。</li>
<li><strong>高可用架构</strong>，通常来说是从冗余架构的角度来保障可用性。比如，多租户隔离，灾备多活，或是数据可以在其中复制保持一致性的集群。总之，就是为了不出单点故障。</li>
<li><strong>高可用运维</strong>，指的是 DevOps 中的 CI（持续集成）/CD（持续部署）。一个良好的运维应该是一条很流畅的软件发布管线，其中做了足够的自动化测试，还可以做相应的灰度发布，以及对线上系统的自动化控制。这样，可以做到“计划内”或是“非计划内”的宕机事件的时长最短。</li>
</ul>
<p>上述这些技术非常有技术含量，而且需要投入大量的时间和精力。</p>
<h1 id="分布式系统的关键技术"><a href="#分布式系统的关键技术" class="headerlink" title="分布式系统的关键技术"></a>分布式系统的关键技术</h1><p>而通过上面的分析，我们可以看到，引入分布式系统，会引入一堆技术问题，需要从以下几个方面来解决。</p>
<ul>
<li><strong>服务治理</strong>。服务拆分、服务调用、服务发现，服务依赖，服务的关键度定义……服务治理的最大意义是需要把服务间的依赖关系、服务调用链，以及关键的服务给梳理出来，并对这些服务进行性能和可用性方面的管理。</li>
<li><strong>架构软件管理</strong>。服务之间有依赖，而且有兼容性问题，所以，整体服务所形成的架构需要有架构版本管理、整体架构的生命周期管理，以及对服务的编排、聚合、事务处理等服务调度功能。</li>
<li><strong>DevOps</strong>。分布式系统可以更为快速地更新服务，但是对于服务的测试和部署都会是挑战。所以，还需要 DevOps 的全流程，其中包括环境构建、持续集成、持续部署等。</li>
<li><strong>自动化运维</strong>。有了 DevOps 后，我们就可以对服务进行自动伸缩、故障迁移、配置管理、状态管理等一系列的自动化运维技术了。</li>
<li><strong>资源调度管理</strong>。应用层的自动化运维需要基础层的调度支持，也就是云计算 IaaS 层的计算、存储、网络等资源调度、隔离和管理。</li>
<li><strong>整体架构监控</strong>。如果没有一个好的监控系统，那么自动化运维和资源调度管理只可能成为一个泡影，因为监控系统是你的眼睛。没有眼睛，没有数据，就无法进行高效的运维。所以说，监控是非常重要的部分。这里的监控需要对三层系统（应用层、中间件层、基础层）进行监控。</li>
<li><strong>流量控制</strong>。最后是我们的流量控制，负载均衡、服务路由、熔断、降级、限流等和流量相关的调度都会在这里，包括灰度发布之类的功能也在这里。</li>
</ul>
<p>此时，你会发现，要做好这么多的技术，或是要具备这么多的能力，简直就是一个门槛，是一个成本巨高无比的技术栈，看着就都头晕。要实现出来得投入多少人力、物力和时间啊。是的，这就是分布式系统中最大的坑。</p>
<p>不过，我们应该庆幸自己生活在了一个非常不错的年代。今天有一个技术叫——Docker，通过 Docker 以及其衍生出来的 Kubernetes 之类的软件或解决方案，大大地降低了做上面很多事情的门槛。Docker 把软件和其运行的环境打成一个包，然后比较轻量级地启动和运行。在运行过程中，因为软件变成了服务可能会改变现有的环境。但是没关系，当你重新启动一个 Docker 的时候，环境又会变成初始化状态。</p>
<p>这样一来，我们就可以利用 Docker 的这个特性来把软件在不同的机器上进行部署、调度和管理。如果没有 Docker 或是 Kubernetes，那么你可以认为我们还活在“原始时代”。现在你知道为什么 Docker 这样的容器化虚拟化技术是未来了吧。因为分布式系统已经是完全不可逆转的技术趋势了。</p>
<p>但是，上面还有很多的技术是 Docker 及其周边技术没有解决的，所以，依然还有很多事情要做。那么，如果是一个一个地去做这些技术的话，就像是我们在撑开一张网里面一个一个的网眼，本质上这是使蛮力的做法。我们希望可以找到系统的“纲”，一把就能张开整张网。那么，这个纲在哪里呢？</p>
<h1 id="分布式系统的“纲”"><a href="#分布式系统的“纲”" class="headerlink" title="分布式系统的“纲”"></a>分布式系统的“纲”</h1><p>总结一下上面讲述的内容，你不难发现，分布式系统有五个关键技术，它们是：</p>
<ul>
<li>全栈系统监控；</li>
<li>服务 / 资源调度；</li>
<li>流量调度；</li>
<li>状态 / 数据调度；</li>
<li>开发和运维的自动化。</li>
</ul>
<p>而最后一项——开发和运维的自动化，是需要把前四项都做到了，才有可能实现的。所以，最为关键是下面这四项技术，即应用整体监控、资源和服务调度、状态和数据调度及流量调度，它们是构建分布式系统最最核心的东西。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>回顾一下今天的要点内容。首先，我总结了分布式系统需要干的两件事：一是提高整体架构的吞吐量，服务更多的并发和流量，二是为了提高系统的稳定性，让系统的可用性更高。然后分别从这两个方面阐释，需要通过哪些技术来实现，并梳理出其中的技术难点及可能会带来的问题。最后，欢迎你分享一下你在解决系统的性能和可用性方面使用到的方法和技巧。</p>
<p>虽然 Docker 及其衍生出来的 Kubernetes 等软件或解决方案，能极大地降低很多事儿的门槛。但它们没有解决的问题还有很多，需要掌握分布式系统的五大关键技术，从根本上解决问题。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-分布式架构难点"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/11/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E9%9A%BE%E7%82%B9/"
    >分布式架构难点</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/02/11/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E9%9A%BE%E7%82%B9/" class="article-date">
  <time datetime="2020-02-12T02:07:13.000Z" itemprop="datePublished">2020-02-11</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>对分布式服务化架构实践最早的应该是亚马逊。因为早在 2002 年的时候，亚马逊 CEO 杰夫·贝索斯（Jeff Bezos）就向全公司颁布了下面的这几条架构规定.</p>
<ol>
<li>所有团队的程序模块都要通过 Service Interface 方式将其数据与功能开放出来。</li>
<li>团队间程序模块的信息通信，都要通过这些接口。</li>
<li>除此之外没有其它的通信方式。其他形式一概不允许：不能直接链结别的程序（把其他团队的程序当做动态链接库来链接），不能直接读取其他团队的数据库，不能使用共享内存模式，不能使用别人模块的后门，等等。唯一允许的通信方式是调用 Service Interface。</li>
<li>任何技术都可以使用。比如：HTTP、CORBA、Pub/Sub、自定义的网络协议等。</li>
<li>所有的 Service Interface，毫无例外，都必须从骨子里到表面上设计成能对外界开放的。也就是说，团队必须做好规划与设计，以便未来把接口开放给全世界的程序员，没有任何例外。</li>
<li>不这样做的人会被炒鱿鱼。</li>
</ol>
<p>这应该就是 AWS（Amazon Web Service）出现的基因吧。当然，前面说过，采用分布式系统架构后会出现很多的问题。比如：</p>
<ul>
<li>一个线上故障的工单会在不同的服务和不同的团队中转过来转过去的。</li>
<li>每个团队都可能成为一个潜在的 DDoS 攻击者，除非每个服务都要做好配额和限流。</li>
<li>监控和查错变得更为复杂。除非有非常强大的监控手段。</li>
<li>服务发现和服务治理也变得非常复杂。</li>
</ul>
<p>为了克服这些问题，亚马逊这么多年的实践让其可以运维和管理极其复杂的分布式服务架构。主要有以下几点。</p>
<ol>
<li><strong>分布式服务的架构需要分布式的团队架构</strong>。在亚马逊，一个服务由一个小团队（Two Pizza Team 不超过 16 个人，两张 Pizza 可以喂饱的团队）负责，从前端负责到数据，从需求分析负责到上线运维。这是良性的分工策略——按职责分工，而不是按技能分工。</li>
<li><strong>分布式服务查错不容易</strong>。一旦出现比较严重的故障，需要整体查错。出现一个 S2 的故障，就可以看到每个团队的人都会上线。在工单系统里能看到，在故障发生的一开始，大家都在签到并自查自己的系统。如果没问题，也要在线待命（standby），等问题解决。</li>
<li><strong>没有专职的测试人员，也没有专职的运维人员，开发人员做所有的事情</strong>。开发人员做所有事情的好处是——吃自己的狗粮（Eat Your Own Dog Food） 最微观的实践。自己写的代码自己维护自己养，会让开发人员明白，写代码容易维护代码复杂。这样，开发人员在接需求、做设计、写代码、做工具时都会考虑到软件的长期维护性。</li>
<li><strong>运维优先，崇尚简化和自动化</strong>。为了能够运维如此复杂的系统，亚马逊内部在运维上下了非常大的功夫。现在人们所说的 DevOps 这个事，亚马逊在 10 多年前就做到了。亚马逊最为强大的就是运维，拼命地对系统进行简化和自动化，让亚马逊做到了可以轻松运维拥有上千万台虚机的 AWS 云平台。</li>
<li><strong>内部服务和外部服务一致</strong>。无论是从安全方面，还是接口设计方面，无论是从运维方面，还是故障处理的流程方面，亚马逊的内部系统都和外部系统一样对待。这样做的好处是，内部系统的服务随时都可以开放出来。而且，从第一天开始，服务提供方就有对外服务的能力。可以想像，以这样的标准运作的团队其能力会是什么样的。</li>
</ol>
<p>我们再来看一下分布式系统在技术上需要注意的问题。</p>
<h1 id="问题一：异构系统的不标准问题"><a href="#问题一：异构系统的不标准问题" class="headerlink" title="问题一：异构系统的不标准问题"></a>问题一：异构系统的不标准问题</h1><p>这主要表现在：</p>
<ul>
<li>软件和应用不标准。</li>
<li>通讯协议不标准。</li>
<li>数据格式不标准。</li>
<li>开发和运维的过程和方法不标准。</li>
</ul>
<p>不同的软件，不同的语言会出现不同的兼容性和不同的开发、测试、运维标准。不同的标准会让我们用不同的方式来开发和运维，引起架构复杂度的提升。比如：有的软件修改配置要改它的.conf 文件，而有的则是调用管理 API 接口。</p>
<p>在通讯方面，不同的软件用不同的协议，就算是相同的网络协议里也会出现不同的数据格式。还有，不同的团队因为用不同的技术，会有不同的开发和运维方式。这些不同的东西，会让我们的整个分布式系统架构变得异常复杂。所以，分布式系统架构需要有相应的规范。</p>
<p>比如，我看到，很多服务的 API 出错不返回 HTTP 的错误状态码，而是返回个正常的状态码 200，然后在 HTTP Body 里的 JSON 字符串中写着个：error，bla bla error message。这简直就是一种反人类的做法。我实在不明白为什么会有众多这样的设计。这让监控怎么做啊？现在，你应该使用 Swagger 的规范了。</p>
<p>再比如，我看到很多公司的软件配置管理里就是一个 key-value 的东西，这样的东西灵活到可以很容易地被滥用。不规范的配置命名，不规范的值，甚至在配置中直接嵌入前端展示内容……</p>
<p>一个好的配置管理，应该分成三层：底层和操作系统相关，中间层和中间件相关，最上面和业务应用相关。于是底层和中间层是不能让用户灵活修改的，而是只让用户选择。比如：操作系统的相关配置应该形成模板来让人选择，而不是让人乱配置的。只有配置系统形成了规范，我们才 hold 得住众多的系统。</p>
<p>再比如：数据通讯协议。通常来说，作为一个协议，一定要有协议头和协议体。协议头定义了最基本的协议数据，而协议体才是真正的业务数据。对于协议头，我们需要非常规范地让每一个使用这个协议的团队都使用一套标准的方式来定义，这样我们才容易对请求进行监控、调度和管理。</p>
<h1 id="问题二：系统架构中的服务依赖性问题"><a href="#问题二：系统架构中的服务依赖性问题" class="headerlink" title="问题二：系统架构中的服务依赖性问题"></a>问题二：系统架构中的服务依赖性问题</h1><p>对于传统的单体应用，一台机器挂了，整个软件就挂掉了。但是你千万不要以为在分布式的架构下不会发生这样的事。分布式架构下，服务是会有依赖的，于是一个服务依赖链上，某个服务挂掉了，会导致出现“多米诺骨牌”效应，会倒一片。</p>
<p>所以，在分布式系统中，服务的依赖也会带来一些问题。</p>
<ul>
<li>如果非关键业务被关键业务所依赖，会导致非关键业务变成一个关键业务。</li>
<li>服务依赖链中，出现“木桶短板效应”——整个 SLA 由最差的那个服务所决定。</li>
</ul>
<p>这是服务治理的内容了。服务治理不但需要我们定义出服务的关键程度，还需要我们定义或是描述出关键业务或服务调用的主要路径。没有这个事情，我们将无法运维或是管理整个系统。</p>
<p>这里需要注意的是，很多分布式架构在应用层上做到了业务隔离，然而，在数据库结点上并没有。如果一个非关键业务把数据库拖死，那么会导致全站不可用。所以，数据库方面也需要做相应的隔离。也就是说，最好一个业务线用一套自己的数据库。这就是亚马逊服务器的实践——系统间不能读取对方的数据库，只通过服务接口耦合。这也是微服务的要求。我们不但要拆分服务，还要为每个服务拆分相应的数据库。</p>
<h1 id="问题三：故障发生的概率更大"><a href="#问题三：故障发生的概率更大" class="headerlink" title="问题三：故障发生的概率更大"></a>问题三：故障发生的概率更大</h1><p>在分布式系统中，因为使用的机器和服务会非常多，所以，故障发生的频率会比传统的单体应用更大。只不过，单体应用的故障影响面很大，而分布式系统中，虽然故障的影响面可以被隔离，但是因为机器和服务多，出故障的频率也会多。另一方面，因为管理复杂，而且没人知道整个架构中有什么，所以非常容易犯错误。</p>
<p>你会发现，对分布式系统架构的运维，简直就是一场噩梦。我们会慢慢地明白下面这些道理。</p>
<ul>
<li>出现故障不可怕，故障恢复时间过长才可怕。</li>
<li>出现故障不可怕，故障影响面过大才可怕。</li>
</ul>
<p>运维团队在分布式系统下会非常忙，忙到每时每刻都要处理大大小小的故障。我看到，很多大公司，都在自己的系统里拼命地添加各种监控指标，有的能够添加出几万个监控指标。我觉得这完全是在“使蛮力”。一方面，信息太多等于没有信息，另一方面，SLA 要求我们定义出“Key Metrics”，也就是所谓的关键指标。然而，他们却没有。这其实是一种思维上的懒惰。</p>
<p>但是，上述的都是在“救火阶段”而不是“防火阶段”。所谓“防火胜于救火”，我们还要考虑如何防火，这需要我们在设计或运维系统时都要为这些故障考虑，即所谓 Design for Failure。在设计时就要考虑如何减轻故障。如果无法避免，也要使用自动化的方式恢复故障，减少故障影响面。</p>
<p>因为当机器和服务数量越来越多时，你会发现，人类的缺陷就成为了瓶颈。这个缺陷就是人类无法对复杂的事情做到事无巨细的管理，只有机器自动化才能帮助人类。 也就是，人管代码，代码管机器，人不管机器！</p>
<h1 id="问题四：多层架构的运维复杂度更大"><a href="#问题四：多层架构的运维复杂度更大" class="headerlink" title="问题四：多层架构的运维复杂度更大"></a>问题四：多层架构的运维复杂度更大</h1><p>通常来说，我们可以把系统分成四层：基础层、平台层、应用层和接入层。</p>
<ul>
<li>基础层就是我们的机器、网络和存储设备等。</li>
<li>平台层就是我们的中间件层，Tomcat、MySQL、Redis、Kafka 之类的软件。</li>
<li>应用层就是我们的业务软件，比如，各种功能的服务。</li>
<li>接入层就是接入用户请求的网关、负载均衡或是 CDN、DNS 这样的东西。</li>
</ul>
<p>对于这四层，我们需要知道：</p>
<ul>
<li>任何一层的问题都会导致整体的问题；</li>
<li>没有统一的视图和管理，导致运维被割裂开来，造成更大的复杂度。</li>
</ul>
<p>很多公司都是按技能分工是，把技术团队分为产品开发、中间件开发、业务运维、系统运维等子团队。这样的分工导致各管一摊，很多事情完全连不在一起。整个系统会像 “多米诺骨牌”一样，一个环节出现问题，就会倒下去一大片。因为没有一个统一的运维视图，不知道一个服务调用是如何经过每一个服务和资源，也就导致我们在出现故障时要花大量的时间在沟通和定位问题上。</p>
<p>从接入层到负载均衡，再到服务层，再到操作系统底层，设置的 KeepAlive 的参数完全不一致，导致用户发现，软件运行的行为和文档中定义的完全不一样。工程师查错的过程简直就是一场恶梦，以为找到了一个，结果还有一个，来来回回花了大量的时间才把所有 KeepAlive 的参数设置成一致的，浪费了太多的时间。</p>
<p><strong>分工不是问题，问题是分工后的协作是否统一和规范</strong>。这点，一定要重视。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>好了，我们来总结一下今天分享的主要内容。首先，我以亚马逊为例，讲述了它是如何做分布式服务架构的，遇到了哪些问题，以及是如何解决的。我认为，亚马逊在分布式服务系统方面的这些实践和经验积累，是 AWS 出现的基因。随后分享了在分布式系统中需要注意的几个问题，同时给出了应对方案。</p>
<p>我认为，构建分布式服务需要从组织，到软件工程，再到技术上的一次大的改造，需要比较长的时间来磨合和改进，并不断地总结教训和成功经验。下篇文章中，我们讲述分布式系统的技术栈。希望对你有帮助。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Chinese/" rel="tag">Chinese</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Distributed-System/" rel="tag">Distributed System</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-分布式架构经典资料"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/10/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E7%BB%8F%E5%85%B8%E8%B5%84%E6%96%99/"
    >分布式架构经典资料</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/02/10/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E7%BB%8F%E5%85%B8%E8%B5%84%E6%96%99/" class="article-date">
  <time datetime="2020-02-10T23:18:40.000Z" itemprop="datePublished">2020-02-10</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="基础理论"><a href="#基础理论" class="headerlink" title="基础理论"></a>基础理论</h1><ul>
<li>CAP 定理</li>
<li>Fallacies of Distributed Computing</li>
</ul>
<h1 id="经典资料"><a href="#经典资料" class="headerlink" title="经典资料"></a>经典资料</h1><ul>
<li>Distributed systems theory for the distributed systems engineer</li>
<li>FLP Impossibility Result</li>
<li>An introduction to distributed systems</li>
<li>Distributed Systems for fun and profit</li>
<li>Distributed Systems: Principles and Paradigms</li>
<li>Scalable Web Architecture and Distributed Systems</li>
<li>Principles of Distributed Systems</li>
<li>Making reliable distributed systems in the presence of software errors</li>
<li>Designing Data Intensive Applications</li>
</ul>
<h1 id="基础理论-1"><a href="#基础理论-1" class="headerlink" title="基础理论"></a>基础理论</h1><h2 id="CAP-定理"><a href="#CAP-定理" class="headerlink" title="CAP 定理"></a><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/CAP_theorem">CAP 定理</a></h2><p>CAP 定理是分布式系统设计中最基础，也是最为关键的理论。它指出，分布式数据存储不可能同时满足以下三个条件。</p>
<ul>
<li><strong>一致性（Consistency）</strong>：每次读取要么获得最近写入的数据，要么获得一个错误。</li>
<li><strong>可用性（Availability）</strong>：每次请求都能获得一个（非错误）响应，但不保证返回的是最新写入的数据。</li>
<li><strong>分区容忍（Partition tolerance）</strong>：尽管任意数量的消息被节点间的网络丢失（或延迟），系统仍继续运行。</li>
</ul>
<p>也就是说，CAP 定理表明，在存在网络分区的情况下，一致性和可用性必须二选一。而在没有发生网络故障时，即分布式系统正常运行时，一致性和可用性是可以同时被满足的。这里需要注意的是，CAP 定理中的一致性与 ACID 数据库事务中的一致性截然不同。</p>
<p>掌握 CAP 定理，尤其是能够正确理解 C、A、P 的含义，对于系统架构来说非常重要。因为对于分布式系统来说，网络故障在所难免，如何在出现网络故障的时候，维持系统按照正常的行为逻辑运行就显得尤为重要。你可以结合实际的业务场景和具体需求，来进行权衡。</p>
<p>例如，对于大多数互联网应用来说（如门户网站），因为机器数量庞大，部署节点分散，网络故障是常态，可用性是必须要保证的，所以只有舍弃一致性来保证服务的 AP。而对于银行等，需要确保一致性的场景，通常会权衡 CA 和 CP 模型，CA 模型网络故障时完全不可用，CP 模型具备部分可用性。</p>
<ul>
<li>CA (consistency + availability)，这样的系统关注一致性和可用性，它需要非常严格的全体一致的协议，比如“两阶段提交”（2PC）。CA 系统不能容忍网络错误或节点错误，一旦出现这样的问题，整个系统就会拒绝写请求，因为它并不知道对面的那个结点是否挂掉了，还是只是网络问题。唯一安全的做法就是把自己变成只读的。</li>
<li>CP (consistency + partition tolerance)，这样的系统关注一致性和分区容忍性。它关注的是系统里大多数人的一致性协议，比如：Paxos 算法 (Quorum 类的算法)。这样的系统只需要保证大多数结点数据一致，而少数的结点会在没有同步到最新版本的数据时变成不可用的状态。这样能够提供一部分的可用性。</li>
<li>AP (availability + partition tolerance)，这样的系统关心可用性和分区容忍性。因此，这样的系统不能达成一致性，需要给出数据冲突，给出数据冲突就需要维护数据版本。Dynamo 就是这样的系统。</li>
</ul>
<p>然而，还是有一些人会错误地理解 CAP 定理，甚至误用。Cloudera 工程博客中，<a target="_blank" rel="noopener" href="http://blog.cloudera.com/blog/2010/04/cap-confusion-problems-with-partition-tolerance/">CAP Confusion: Problems with ‘partition tolerance’</a>一文中对此有详细的阐述。</p>
<p>推荐谷歌的<a target="_blank" rel="noopener" href="http://www.youtube.com/watch?v=srOgpXECblk">Transaction Across DataCenter 视频</a>。</p>
<h2 id="Fallacies-of-Distributed-Computing"><a href="#Fallacies-of-Distributed-Computing" class="headerlink" title="Fallacies of Distributed Computing"></a><a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">Fallacies of Distributed Computing</a></h2><p>本文是英文维基百科上的一篇文章。它是 Sun 公司的<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/L_Peter_Deutsch">劳伦斯·彼得·多伊奇（Laurence Peter Deutsch）</a>等人于 1994~1997 年提出的，讲的是刚刚进入分布式计算领域的程序员常会有的一系列错误假设。</p>
<p>多伊奇于 1946 年出生在美国波士顿。他创办了阿拉丁企业（Aladdin Enterprises），并在该公司编写出了著名的 Ghostscript 开源软件，于 1988 年首次发布。</p>
<p>他在学生时代就和艾伦·凯（Alan Kay）等比他年长的人一起开发了 Smalltalk，并且他的开发成果激发了后来 Java 语言 JIT 编译技术的创造灵感。他后来在 Sun 公司工作并成为 Sun 的公司院士。在 1994 年，他成为了 ACM 院士。</p>
<p>基本上，每个人刚开始建立一个分布式系统时，都做了以下 8 条假定。随着时间的推移，每一条都会被证明是错误的，也都会导致严重的问题，以及痛苦的学习体验。</p>
<ol>
<li>网络是稳定的。</li>
<li>网络传输的延迟是零。</li>
<li>网络的带宽是无穷大。</li>
<li>网络是安全的。</li>
<li>网络的拓扑不会改变。</li>
<li>只有一个系统管理员。</li>
<li>传输数据的成本为零。</li>
<li>整个网络是同构的。</li>
</ol>
<p>阿尔农·罗特姆 - 盖尔 - 奥兹（Arnon Rotem-Gal-Oz）写了一篇长文<a target="_blank" rel="noopener" href="http://www.rgoarchitects.com/Files/fallacies.pdf">Fallacies of Distributed Computing Explained</a>来解释这些点。</p>
<p>由于他写这篇文章的时候已经是 2006 年了，所以从中能看到这 8 条常见错误被提出十多年后还有什么样的影响：一是，为什么当今的分布式软件系统也需要避免这些设计错误；二是，在当今的软硬件环境里，这些错误意味着什么。比如，文中在谈“延迟为零”假设时，还谈到了 AJAX，而这是 2005 年开始流行的技术。</p>
<p>而<a target="_blank" rel="noopener" href="http://blog.fogcreek.com/eight-fallacies-of-distributed-computing-tech-talk/">加勒思·威尔逊（Gareth Wilson）的文章</a>则用日常生活中的例子，对这些点做了更为通俗的解释。</p>
<p>这 8 个需要避免的错误不仅对于中间件和底层系统开发者及架构师是重要的知识，而且对于网络应用程序开发者也同样重要。分布式系统的其他部分，如容错、备份、分片、微服务等也许可以对应用程序开发者部分透明，但这 8 点则是应用程序开发者也必须知道的。</p>
<p><strong>为什么我们要深刻地认识这 8 个错误？是因为，这要我们清楚地认识到——在分布式系统中错误是不可能避免的，我们能做的不是避免错误，而是要把错误的处理当成功能写在代码中。</strong></p>
<h1 id="经典资料-1"><a href="#经典资料-1" class="headerlink" title="经典资料"></a>经典资料</h1><h2 id="Distributed-systems-theory-for-the-distributed-systems-engineer"><a href="#Distributed-systems-theory-for-the-distributed-systems-engineer" class="headerlink" title="Distributed systems theory for the distributed systems engineer"></a><a target="_blank" rel="noopener" href="http://the-paper-trail.org/blog/distributed-systems-theory-for-the-distributed-systems-engineer/">Distributed systems theory for the distributed systems engineer</a></h2><p>本文作者认为，推荐大量的理论论文是学习分布式系统理论的错误方法，除非这是你的博士课程。因为论文通常难度大又很复杂，需要认真学习，而且需要理解这些研究成果产生的时代背景，才能真正的领悟到其中的精妙之处。</p>
<p>在本文中，作者给出了他整理的分布式工程师必须要掌握的知识列表，并直言掌握这些足够设计出新的分布式系统。首先，作者推荐了 4 份阅读材料，它们共同概括了构建分布式系统的难点，以及所有工程师必须克服的技术难题。</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://book.mixu.net/distsys/">Distributed Systems for Fun and Profit</a>，这是一本小书，涵盖了分布式系统中的关键问题，包括时间的作用和不同的复制策略。后文中对这本书有较详细的介绍。</li>
<li><a target="_blank" rel="noopener" href="https://www.somethingsimilar.com/2013/01/14/notes-on-distributed-systems-for-young-bloods/">Notes on distributed systems for young bloods</a>，这篇文章中没有理论，是一份适合新手阅读的分布式系统实践笔记。</li>
<li><a target="_blank" rel="noopener" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.41.7628">A Note on Distributed Systems</a>，这是一篇经典的论文，讲述了为什么在分布式系统中，远程交互不能像本地对象那样进行。</li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">The fallacies of distributed computing</a>，每个分布式系统新手都会做的 8 个错误假设，并探讨了其会带来的影响。上文中专门对这篇文章做了介绍。</li>
</ul>
<p>随后，分享了几个关键点。</p>
<ul>
<li><strong>失败和时间（Failure and Time）</strong>。分布式系统工程师面临的很多困难都可以归咎于两个根本原因：1. 进程可能会失败；2. 没有好方法表明进程失败。这就涉及到如何设置系统时钟，以及进程间的通讯机制，在没有任何共享时钟的情况下，如何确定一个事件发生在另一个事件之前。</li>
</ul>
<p>可以参考 Lamport 时钟和 Vector 时钟，还可以看看<a target="_blank" rel="noopener" href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo 论文</a>。</p>
<ul>
<li><strong>容错的压力（The basic tension of fault tolerance）</strong>。能在不降级的情况下容错的系统一定要像没有错误发生的那样运行。这就意味着，系统的某些部分必须冗余地工作，从而在性能和资源消耗两方面带来成本。</li>
</ul>
<p>最终一致性以及其他技术方案在以系统行为弱保证为代价，来试图避免这种系统压力。阅读<a target="_blank" rel="noopener" href="http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf">Dynamo 论文</a>和帕特·赫尔兰（Pat Helland）的经典论文<a target="_blank" rel="noopener" href="http://www.cloudtran.com/pdfs/LifeBeyondDistTRX.pdf">Life Beyond Transactions</a>能获很得大启发。</p>
<ul>
<li><strong>基本原语（Basic primitives）</strong>。在分布式系统中几乎没有一致认同的基本构建模块，但目前在越来越多地在出现。比如 Leader 选举，可以参考<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Bully_algorithm">Bully 算法</a>；分布式状态机复制，可以参考<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/State_machine_replication">维基百科</a>和<a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/publication/how-to-build-a-highly-available-system-using-consensus/?from=http%3A%2F%2Fresearch.microsoft.com%2Fen-us%2Fum%2Fpeople%2Fblampson%2F58-consensus%2Facrobat.pdf">Lampson 的论文</a>，后者更权威，只是有些枯燥。</li>
<li><strong>基本结论（Fundamental Results）</strong>。某些事实是需要吸收理解的，有几点：如果进程之间可能丢失某些消息，那么不可能在实现一致性存储的同时响应所有的请求，这就是 CAP 定理；一致性不可能同时满足以下条件：a. 总是正确，b. 在异步系统中只要有一台机器发生故障，系统总是能终止运行——停止失败（FLP 不可能性）；一般而言，消息交互少于两轮都不可能达成共识（Consensus）。</li>
<li><strong>真实系统（Real systems）</strong>。学习分布式系统架构最重要的是，结合一些真实系统的描述，反复思考和点评其背后的设计决策。如谷歌的 GFS、Spanner、Chubby、BigTable、Dapper 等，以及 Dryad、Cassandra 和 Ceph 等非谷歌系统。</li>
</ul>
<h2 id="FLP-Impossibility-Result"><a href="#FLP-Impossibility-Result" class="headerlink" title="FLP Impossibility Result"></a><a target="_blank" rel="noopener" href="https://groups.csail.mit.edu/tds/papers/Lynch/jacm85.pdf">FLP Impossibility Result</a></h2><p>FLP 不可能性的名称起源于它的三位作者，Fischer、Lynch 和 Paterson。它是关于理论上能做出的功能最强的共识算法会受到怎样的限制的讨论。</p>
<p>所谓共识问题，就是让网络上的分布式处理者最后都对同一个结果值达成共识。该解决方案对错误有恢复能力，处理者一旦崩溃以后，就不再参与计算。在同步环境下，每个操作步骤的时间和网络通信的延迟都是有限的，要解决共识问题是可能的，方式是：等待一个完整的步长来检测某个处理者是否已失败。如果没有收到回复，那就假定它已经崩溃。</p>
<p>共识问题有几个变种，它们在“强度”方面有所不同——通常，一个更“强”问题的解决方案同时也能解决比该问题更“弱”的问题。共识问题的一个较强的形式如下。</p>
<p>给出一个处理者的集合，其中每一个处理者都有一个初始值：</p>
<ul>
<li>所有无错误的进程（处理过程）最终都将决定一个值；</li>
<li>所有会做决定的无错误进程决定的都将是同一个值；</li>
<li>最终被决定的值必须被至少一个进程提出过。</li>
</ul>
<p>这三个特性分别被称为“终止”、“一致同意”和“有效性”。任何一个具备这三点特性的算法都被认为是解决了共识问题。</p>
<p>FLP 不可能性则讨论了异步模型下的情况，主要结论有两条。</p>
<ol>
<li>在异步模型下不存在一个完全正确的共识算法。不仅上述较“强”形式的共识算法不可能实现，FLP 还证明了比它弱一些的、只需要有一些无错误的进程做决定就足够的共识算法也是不可能实现的。</li>
<li>在异步模型下存在一个部分正确的共识算法，前提是所有无错误的进程都总能做出一个决定，此外没有进程会在它的执行过程中死亡，并且初始情况下超过半数进程都是存活状态。</li>
</ol>
<p>FLP 的结论是，在异步模型中，仅一个处理者可能崩溃的情况下，就已经没有分布式算法能解决共识问题。这是该问题的理论上界。其背后的原因在于，异步模型下对于一个处理者完成工作然后再回复消息所需的时间并没有上界。因此，无法判断出一个处理者到底是崩溃了，还是在用较长的时间来回复，或者是网络有很大的延迟。</p>
<p>FLP 不可能性对我们还有别的启发。一是网络延迟很重要，网络不能长时间处于拥塞状态，否则共识算法将可能因为网络延迟过长而导致超时失败。二是计算时间也很重要。对于需要计算共识的处理过程（进程），如分布式数据库提交，需要在短时间里就计算出能否提交的结果，那就要保证计算结点资源充分，特别是内存容量、磁盘空闲时间和 CPU 时间方面要足够，并在软件层面确保计算不超时。</p>
<p>另一个问题是，像 Paxos 这样的共识算法为什么可行？实际上它并不属于 FLP 不可能性证明中所说的“完全正确”的算法。它的正确性会受超时值的影响。但这并不妨碍它在实践中有效，因为我们可以通过避免网络拥塞等手段来保证超时值是合适的。</p>
<h2 id="An-introduction-to-distributed-systems"><a href="#An-introduction-to-distributed-systems" class="headerlink" title="An introduction to distributed systems"></a><a target="_blank" rel="noopener" href="https://github.com/aphyr/distsys-class">An introduction to distributed systems</a></h2><p>它是<a target="_blank" rel="noopener" href="https://github.com/aphyr/distsys-class#review-1">分布式系统基础课</a>的课程提纲，也是一份很棒的分布式系统介绍，几乎涵盖了所有知识点，并辅以简洁并切中要害的说明文字，非常适合初学者提纲挈领地了解知识全貌，快速与现有知识结合，形成知识体系。此外，还可以把它作为分布式系统的知识图谱，根据其中列出的知识点一一搜索，你能学会所有的东西。</p>
<h2 id="Distributed-Systems-for-fun-and-profit"><a href="#Distributed-Systems-for-fun-and-profit" class="headerlink" title="Distributed Systems for fun and profit"></a><a target="_blank" rel="noopener" href="http://book.mixu.net/distsys/single-page.html">Distributed Systems for fun and profit</a></h2><p>这是一本免费的电子书。作者撰写此书的目的是希望以一种更易于理解的方式，讲述以亚马逊的 Dynamo、谷歌的 BigTable 和 MapReduce 等为代表的分布式系统背后的核心思想。</p>
<p>因而，书中着力撰写分布式系统中的关键概念，以便让读者能够快速了解最为核心的知识，并且进行了足够详实的讲述，方便读者体会和理解，又不至于陷入细节。</p>
<p>全书分为五章，讲述了扩展性、可用性、性能和容错等基础知识，FLP 不可能性和 CAP 定理，探讨了大量的一致性模型；讨论了时间和顺序，及时钟的各种用法。随后，探讨了复制问题，如何防止差异，以及如何接受差异。此外，每章末尾都给出了针对本章内容的扩展阅读资源列表，这些资料是对本书内容的很好补充。</p>
<h2 id="Distributed-Systems-Principles-and-Paradigms-http-barbie-uta-edu-jli-Resources-MapReduce-amp-Hadoop-Distributed-Systems-Principles-and-Paradigms-pdf"><a href="#Distributed-Systems-Principles-and-Paradigms-http-barbie-uta-edu-jli-Resources-MapReduce-amp-Hadoop-Distributed-Systems-Principles-and-Paradigms-pdf" class="headerlink" title="[Distributed Systems: Principles and Paradigms](http://barbie.uta.edu/~jli/Resources/MapReduce&amp;Hadoop/Distributed Systems Principles and Paradigms.pdf)"></a>[Distributed Systems: Principles and Paradigms](<a target="_blank" rel="noopener" href="http://barbie.uta.edu/~jli/Resources/MapReduce&amp;Hadoop/Distributed">http://barbie.uta.edu/~jli/Resources/MapReduce&amp;Hadoop/Distributed</a> Systems Principles and Paradigms.pdf)</h2><p>本书是由计算机科学家安德鲁·斯图尔特·塔能鲍姆（Andrew S. Tanenbaum）和其同事马丁·范·斯蒂恩（Martin van Steen）合力撰写的，是分布式系统方面的经典教材。</p>
<p>语言简洁，内容通俗易懂，介绍了分布式系统的七大核心原理，并给出了大量的例子；系统讲述了分布式系统的概念和技术，包括通信、进程、命名、同步化、一致性和复制、容错以及安全等；讨论了分布式应用的开发方法（即范型）。</p>
<p>但本书不是一本指导“如何做”的手册，仅适合系统性地学习基础知识，了解编写分布式系统的基本原则和逻辑。中文翻译版为<a target="_blank" rel="noopener" href="https://item.jd.com/10079452.html">《分布式系统原理与范型》（第二版）</a>。</p>
<h2 id="Scalable-Web-Architecture-and-Distributed-Systems"><a href="#Scalable-Web-Architecture-and-Distributed-Systems" class="headerlink" title="Scalable Web Architecture and Distributed Systems"></a><a target="_blank" rel="noopener" href="http://www.aosabook.org/en/distsys.html">Scalable Web Architecture and Distributed Systems</a></h2><p>这是一本免费的在线小册子，其中文翻译版为<a target="_blank" rel="noopener" href="http://nettee.github.io/posts/2016/Scalable-Web-Architecture-and-Distributed-Systems/">可扩展的 Web 架构和分布式系统</a>。</p>
<p>本书主要针对面向的互联网（公网）的分布式系统，但其中的原理或许也可以应用于其他分布式系统的设计中。作者的观点是，通过了解大型网站的分布式架构原理，小型网站的构建也能从中受益。本书从大型互联网系统的常见特性，如高可用、高性能、高可靠、易管理等出发，引出了一个类似于 Flickr 的典型的大型图片网站的例子。</p>
<p>首先，从程序模块化易组合的角度出发，引出了面向服务架构（SOA）的概念。同时，引申出写入和读取两者的性能问题，及对此二者如何调度的考量——在当今的软硬件架构上，写入几乎总是比读取更慢，包括软件层面引起的写入慢（如数据库的一致性要求和 B 树的修改）和硬件层面引起的写入慢（如 SSD）。</p>
<p>网络提供商提供的下载带宽也通常比上传带宽更大。读取往往可以异步操作，还可以做 gzip 压缩。写入则往往需要保持连接直到数据上传完成。因此，往往我们会想把服务做成读写分离的形式。然后通过一个 Flickr 的例子，介绍了他们的服务器分片式集群做法。</p>
<p>接下来讲了冗余。数据的冗余异地备份（如 master-slave）、服务的多版本冗余、避免单点故障等。</p>
<p>随后，在冗余的基础上，讲了多分区扩容，亦即横向扩容。横向扩容是在单机容量无法满足需求的情况下不得不做的设计。但横向扩容会带来一个问题，即数据的局域性会变差。本来数据可以存在于同一台服务器上，但现在数据不得不存在于不同服务器上，潜在地降低了系统的性能（主要是可能延长响应时间）。另一个问题是多份数据的不一致性。</p>
<p>之后，本书开始深入讲解数据访问层面的设计。首先抛出一个大型数据（TB 级以上）的存储问题。如果内存都无法缓存该数据量，性能将大幅下降，那么就需要缓存数据。数据可以缓存在每个节点上。</p>
<p>但如果为所有节点使用负载均衡，那么分配到每个节点的请求将十分随机，大大降低缓存命中率，从而导致低效的缓存。接下来考虑全局缓存的设计。再接下来考虑分布式缓存的设计。进一步，介绍了 Memcached，以及 Facebook 的缓存设计方案。</p>
<p>代理服务器则可以用于把多个重复请求合并成一个，对于公网上的公共服务来说，这样做可以大大减少对数据层访问的次数。Squid 和 Varnish 是两个可用于生产的代理服务软件。</p>
<p>当知道所需要读取的数据的元信息时，比如知道一张图片的 URL，或者知道一个要全文搜索的单词时，索引就可以帮助找到那几台存有该信息的服务器，并从它们那里获取数据。文中扩展性地讨论了本话题。</p>
<p>接下来谈负载均衡器，以及一些典型的负载均衡拓扑。然后讨论了对于用户会话数据如何处理。比如，对于电子商务网站，用户的购物车在没有下单之前都必须保持有效。</p>
<p>一种办法是让用户会话与服务器产生关联，但这样做会较难实现自动故障转移，如何做好是个问题。另外，何时该使用负载均衡是个问题。有时节点数量少的情况下，只要使用轮换式 DNS 即可。负载均衡也会让在线性能问题的检测变得更麻烦。</p>
<p>对于写入的负载，可以用队列的方式来减少对服务器的压力，保证服务器的效率。消息队列的开源实现有很多，如 RabbitMQ、ActiveMQ、BeanstalkD，但有些队列方案也使用了如 Zookeeper，甚至是像 Redis 这样的存储服务。</p>
<p>本书主要讲述了高性能互联网分布式服务的架构方案，并介绍了许多实用的工具。作者指出这是一个令人兴奋的设计领域，虽然只讲了一些皮毛，但这一领域不仅现在有很多创新，将来也会越来越多。</p>
<h2 id="Principles-of-Distributed-Systems"><a href="#Principles-of-Distributed-Systems" class="headerlink" title="Principles of Distributed Systems"></a><a target="_blank" rel="noopener" href="http://dcg.ethz.ch/lectures/podc_allstars/lecture/podc.pdf">Principles of Distributed Systems</a></h2><p>本书是苏黎世联邦理工学院的教材。它讲述了多种分布式系统中会用到的算法。虽然分布式系统的不同场景会用到不同算法，但并不表示这些算法都会被用到。不过，对于学生来说，掌握了算法设计的精髓也就能举一反三地设计出解决其他问题的算法，从而得到分布式系统架构设计中所需的算法。</p>
<p>本书覆盖的算法有：</p>
<ul>
<li>顶点涂色算法（可用于解决互相冲突的任务分配问题）</li>
<li>分布式的树算法（广播算法、会聚算法、广度优先搜索树算法、最小生成树算法）</li>
<li>容错以及 Paxos（Paxos 是最经典的共识算法之一）</li>
<li>拜占庭协议（节点可能没有完全宕机，而是输出错误的信息）</li>
<li>全互联网络（服务器两两互联的情况下算法的复杂度）</li>
<li>多核计算的工程实践（事务性存储、资源争用管理）</li>
<li>主导集（又一个用随机化算法打破对称性的例子；这些算法可以用于路由器建立路由）</li>
<li>……</li>
</ul>
<p>这些算法对你迈向更高级更广阔的技术领域真的相当有帮助的。</p>
<h2 id="Making-reliable-distributed-systems-in-the-presence-of-software-errors"><a href="#Making-reliable-distributed-systems-in-the-presence-of-software-errors" class="headerlink" title="Making reliable distributed systems in the presence of software errors"></a><a target="_blank" rel="noopener" href="https://github.com/theanalyst/awesome-distributed-systems/blob/master/README.md">Making reliable distributed systems in the presence of software errors</a></h2><p>这本书的书名直译过来是在有软件错误的情况下，构建可靠的分布式系统，Erlang 之父乔·阿姆斯特朗（Joe Armstrong）的力作。书中撰写的内容是从 1981 年开始的一个研究项目的成果，这个项目是寻找更好的电信应用编程方式。</p>
<p>当时的电信应用都是大型程序，虽然经过了仔细的测试，但投入使用时程序中仍会存在大量的错误。作者及其同事假设这些程序中确实有错误，然后想法设法在这些错误存在的情况下构建可靠的系统。他们测试了所有的编程语言，没有一门语言拥有电信行业所需要的所有特性，所以促使一门全新的编程语言 Erlang 的开发，以及随之出现的构建健壮系统（OTP）的设计方法论和库集。</p>
<p>书中抽象了电信应用的所有需求，定义了问题域，讲述了系统构建思路——模拟现实，简单通用，并给出了指导规范。阿姆斯特朗认为，在存在软件错误的情况下，构建可靠系统的核心问题可以通过编程语言或者编程语言的标准库来解决。所以本书有很大的篇幅来介绍 Erlang，以及如何运用其构建具有容错能力的电信应用。</p>
<p>虽然书中的内容是以构建 20 世纪 80 年代的电信系统为背景，但是这种大规模分布式的系统开发思路，以及对系统容错能力的核心需求，与互联网时代的分布式系统架构思路出奇一致。书中对问题的抽象、总结，以及解决问题的思路和方案，有深刻的洞察和清晰的阐释，所以此书对现在的项目开发和架构有极强的指导和借鉴意义。</p>
<h2 id="Designing-Data-Intensive-Applications"><a href="#Designing-Data-Intensive-Applications" class="headerlink" title="Designing Data Intensive Applications"></a><a target="_blank" rel="noopener" href="https://www.amazon.com/Designing-Data-Intensive-Applications-Reliable-Maintainable/dp/1449373321">Designing Data Intensive Applications</a></h2><p>这是一本非常好的书。我们知道，在分布式的世界里，数据结点的扩展是一件非常麻烦的事。而这本书则深入浅出地用很多工程案例讲解了如何让数据结点做扩展。</p>
<p>作者马丁·科勒普曼（Martin Kleppmann）在分布式数据系统领域有着很深的功底，并在这本书中完整地梳理各类纷繁复杂设计背后的技术逻辑，不同架构之间的妥协与超越，很值得开发人员与架构设计者阅读。</p>
<p>这本书深入到 B-Tree、SSTables、LSM 这类数据存储结构中，并且从外部的视角来审视这些数据结构对 NoSQL 和关系型数据库所产生的影响。它可以让你很清楚地了解到真正世界的大数据架构中的数据分区、数据复制的一些坑，并提供了很好的解决方案。</p>
<p><strong>最赞的是，作者将各种各样的技术的本质非常好地关联在一起，帮你触类旁通</strong>。而且抽丝剥茧，循循善诱，从“提出问题”，到“解决问题”，到“解决方案”，再到“优化方案”和“对比不同的方案”，一点一点地把非常晦涩的技术和知识展开。</p>
<p>本书的引用相当多，每章后面都有几百个 Reference。通过这些 Reference，你可以看到更为广阔更为精彩的世界。</p>
<p>这本书是 2017 年 3 月份出版的，目前还没有中译版，不过英文也不难读。非常推荐。这里有<a target="_blank" rel="noopener" href="http://www.antonfagerberg.com/files/intensive.pdf">这本书的 PPT</a>，你可从这个 PPT 中管中窥豹一下。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>在今天的文章中，给出了一些分布式系统的基础理论知识和几本很不错的图书和资料，需要慢慢消化吸收。也许你看到这么庞大的书单和资料列表有点望而却步，但是我真的希望你能够花点时间来看看这些资料。相信你看完这些资料后，一定能上一个新的台阶。再加上一些在工程项目中的实践，我保证你，一定能达到大多数人难以企及的技术境界。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Chinese/" rel="tag">Chinese</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Distributed-System/" rel="tag">Distributed System</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-Learn-to-Code-by-Competitive-Programming"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/09/Learn-to-Code-by-Competitive-Programming/"
    >Learn_to_Code_by_Competitive_Programming</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/02/09/Learn-to-Code-by-Competitive-Programming/" class="article-date">
  <time datetime="2020-02-10T03:20:51.000Z" itemprop="datePublished">2020-02-09</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>How do I Learn to Code? This is probably the most nagging question at the back of your mind, once you have decided that you want to learn programming. Like learning anything else, there is no standard process for learning to code. Of course there are guidelines, there are courses, there are ideologies and there are set traditions, but there is no one single correct way.</p>
<p>One school of thought which is very popular and fairly simple to begin with is <a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Competitive_programming">Competitive Programming</a>. Getting started with it is quite easy and if one devotes sufficient amount of time and effort, you can develop a very strong grasp of programming logic in relatively short amount of time.</p>
<hr>
<p>Here are some steps to get started and be good at it.</p>
<ul>
<li>Get comfortable writing code in either of one of these languages <strong>C, C++ or Java</strong>. Why only C, C++ or Java? Because these are the standard languages allowed in any programming competition.</li>
<li>If you are already good at C, it is suggested to <strong>learn C++</strong>. It is the most popular language among competitive programmers because of its speed and an excellent library in the form of STL (Standard Template Library).</li>
<li>Pick an online judge. Recommended ones are <a target="_blank" rel="noopener" href="http://community.topcoder.com/tc"><strong>Topcoder</strong></a> and <a target="_blank" rel="noopener" href="http://codeforces.com/"><strong>Codeforces</strong></a>. These sites have high quality of problems and also allow you to see other’s code post contest completion. These also categorize problems based on the topic. Some other popular judges include <a target="_blank" rel="noopener" href="http://www.spoj.com/">SPOJ</a>, <a target="_blank" rel="noopener" href="http://codechef.com/">CodeChef</a> (powered by SPOJ) and <a target="_blank" rel="noopener" href="http://www.hackerearth.com/">HackerEarth</a>.</li>
<li>To begin with, <strong>start with simple problems</strong> that typically require transforming English to code and does not require any knowledge on algorithms. Solving <a target="_blank" rel="noopener" href="http://community.topcoder.com/tc?module=ProblemArchive&sr=&er=&sc=&sd=&class=&cat=&div1l=&div2l=1&mind1s=&mind2s=&maxd1s=&maxd2s=&wr=">Div 2 250</a> (Division 2, 250 points) in Topcoder or Div 2 Problem A in Codeforces is a good start.</li>
<li>At the early stages of programming one tends to write long pieces of code, which is actually not required. Try to keep codes <strong>short and simple</strong>.</li>
<li><strong>Practice</strong> these problems until you become comfortable that you can submit it for 240 odd points on any day.</li>
<li>Start implementing basic(or standard) algorithms. It is suggested to read them from <a target="_blank" rel="noopener" href="http://community.topcoder.com/tc?module=Static&d1=tutorials&d2=alg_index">Topcoder tutorials</a> or <a target="_blank" rel="noopener" href="http://www.amazon.com/Introduction-Algorithms-Thomas-H-Cormen/dp/0262033844">Introduction to algorithms</a>.</li>
</ul>
<hr>
<p>Some basic concepts that you should learn are</p>
<ol>
<li>Graph algorithms: Breadth first search(BFS), Depth first search(DFS), Strongly connected components(SCC), Dijkstra, Floyd-Warshall, Minimum spanning tree(MST), Topological sort.</li>
<li>Dynamic programming: Standard dynamic programming problems such as Rod Cutting, Knapsack, Matrix chain multiplication etc.</li>
<li>Number theory: Modular arithmetic, Fermat’s theorem, Chinese remainder theorem(CRT), Euclidian method for GCD, Logarithmic Exponentiation, Sieve of Eratosthenes, Euler’s totient function.</li>
<li>Greedy: Standard problems such as Activity selection.</li>
<li>Search techniques: Binary search, Ternary search and Meet in the middle.</li>
<li>Data structures (Basic): Stacks, Queues, Trees and Heaps.</li>
<li>Data structures (Advanced): Trie, Segment trees, Fenwick tree or Binary indexed tree(BIT), Disjoint data structures.</li>
<li>Strings: Knuth Morris Pratt(KMP), Z algorithm, Suffix arrays/Suffix trees. These are bit advanced algorithms.</li>
<li>Computational geometry: Graham-Scan for convex hull, Line sweep.</li>
<li>Game theory: Basic principles of Nim game, Grundy numbers, Sprague-Grundy theorem.</li>
</ol>
<p>The list is not complete but these are the ones that you encounter very frequently in the contests. There are other algorithms but are required very rarely in the contests.</p>
<p>You can find description and implementation of standard algorithms <a target="_blank" rel="noopener" href="http://e-maxx.ru/algo/">here</a>.</p>
<ul>
<li>Once you have sufficient knowledge of popular algorithms, you can start solving the medium level problems. That is Div 2 all problems in Topcoder and Codeforces. It is advisable not to go for Div 1 500 at this point.</li>
<li>Learning to code is all about practicing. <strong>Participate regularly</strong> in the programming contests. Solve the ones that you cannot solve in the contest, after the contest. Apart from Topcoder and Codeforces you can also look at <a target="_blank" rel="noopener" href="http://www.hackerearth.com/challenges/">HackerEarth Challenges</a> or <a target="_blank" rel="noopener" href="http://www.codechef.com/contests">Codechef contests</a>.</li>
<li><strong>Read the codes</strong> of high rated programmers. Compare your solution with them. You can observe that it is simple and shorter than your solution. Analyse how they have approached and improve your implementation skills.</li>
<li><strong>Read the editorials</strong> after the contest. You can learn how to solve the problems that you were not able to solve in the contest and learn alternative ways to solve the problems which you could solve.</li>
<li>Always <strong>practice the problems that you could solve in the contest</strong>. Suppose if you are able to solve Div 2 250 and 500 in the contest but not Div 2 1000 then practice as many Div 2 1000 problems as as you can.</li>
<li><strong>Do not spend too much time</strong> if you are not getting the solution or are stuck somewhere.</li>
<li>After you feel that you have spent enough time, look at the editorials. Understand the algorithm and code it. Do not look at the actual solution before you have attempted to write the code on your own.</li>
<li>Programming is a very practical and hands on skill. You have to continuously do it to be good at it. It’s not enough to solve the problem theoretically, <strong>you have to code it and get the solution accepted</strong>. Knowing which algorithm/logic to use and implementing it are two different things. It takes both to be good at programming.</li>
<li>Programming learning phase is going to take a lot of time and the key is <strong>practicing regularly</strong>. It takes some time before you can attempt Div 1 500 and other tough problems. Do not give up on reading the editorials and implementing them, even if it takes many hours/days. Remember everything requires practice to master it.</li>
</ul>
<p>It takes considerable amount of time before you get good at it. You have to keep yourself motivated throughout. Forming a team and practicing is a good choice. <strong>Not giving up is the key here</strong>.</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Programming/" rel="tag">Programming</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-识别表象和本质的方法"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/09/%E8%AF%86%E5%88%AB%E8%A1%A8%E8%B1%A1%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9A%84%E6%96%B9%E6%B3%95/"
    >识别表象和本质的方法</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/02/09/%E8%AF%86%E5%88%AB%E8%A1%A8%E8%B1%A1%E5%92%8C%E6%9C%AC%E8%B4%A8%E7%9A%84%E6%96%B9%E6%B3%95/" class="article-date">
  <time datetime="2020-02-09T15:32:13.000Z" itemprop="datePublished">2020-02-09</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="关于兴趣和投入"><a href="#关于兴趣和投入" class="headerlink" title="关于兴趣和投入"></a>关于兴趣和投入</h1><p>兴趣是学习的助燃剂。对一件事有兴趣是是否愿意对这件事投入更多的前提条件。因此，找到自己的兴趣点的确是非常关键的。不过，我们也能看到下面几点。</p>
<ul>
<li><p><strong>一方面，兴趣是需要保持的</strong>。有的人有的事就是三分钟的兴趣。刚开始兴趣十足，然而时间一长，兴趣因为各种原因不能保持，就会很快地“移情别恋”了。所以，不能持久的兴趣，或是一时兴起的兴趣，都无法让人投入下去。</p>
</li>
<li><p><strong>另一方面，兴趣其实也是可以培养出来的</strong>。有些人对计算机软件毫无兴趣，反而对物理世界里的很多东西非常有兴趣，比如无线电、原子能，或是飞行器之类的。但阴差阳错，最终考了个计算机软件专业，然后发现，自己越来越有兴趣，于是就到了今天。</p>
</li>
</ul>
<p>一个可以持久的兴趣，或是可以培养出来的兴趣，后面都有一个比较本质的东西，那就是你在做这个件事时的一种正反馈，其实就是成就感。也就是说，<strong>兴趣只是开始，而能让人不断投入时间和精力的则是正反馈，是成就感</strong>。</p>
<p>带娃的父母可能对此比较好理解。比如，小孩 3 岁的时候，买了一桶积木给她。她一开始只喜欢把积木胡乱堆，没玩一会就对这种抽象的玩具失去了兴趣，去玩别的更形象的玩具去了。于是，我就搭了一个小城堡给她看，她看完后兴趣就来了，也想自己搭一个。但是，不一会儿，她就受挫了，因为没有掌握好物体在构建时的平衡和支点的方法，所以搭出来的东西会倒。</p>
<p>有时倒了之后，她会从中有一点点的学习总结，但更多的时候总结不出来。于是，就上前帮她做调整，她很快就学会了，并且每一次都比上一次搭得更好……如此反复，最终，小孩玩积木上花的时间大大超过了其它的玩具，直到她无法从中得到成就感。</p>
<p>很显然，把孩子从“天性喜欢破坏的兴趣点”上拉到了“喜欢创造的兴趣点”上。因为创造能带来更多的成就感，不是吗？</p>
<p>所以，你对一件事的兴趣只是一种表象，而内在更多的是你做这件事的成就感是否可以持续。<strong>你需要找到让自己能够更有成就感的事情，兴趣总是可以培养的</strong>。</p>
<h1 id="关于学习和工作"><a href="#关于学习和工作" class="headerlink" title="关于学习和工作"></a>关于学习和工作</h1><p>学习一门语言或者一项技术是否只有找到了相应的工作才学得好。</p>
<p>学好一项技术和是否找到与之相匹配的工作有关联，但它们之间并不是强关联的，因为我们每个人的成长和学习有很多时候是在还没有参加工作的时候。但之所以，我们都觉得通过工作才让我们学习和成长得更快，主要有这些原因。</p>
<ul>
<li>工作中能为我们带来相应的场景和实际的问题，而不是空泛的学习。带着问题去学习，带着场景去解决问题，的确是一种很高效的学习方式。</li>
<li>在工作当中，有同事和高手帮助。和他们的交互和讨论，可以让你更快地学习和成长。</li>
</ul>
<p><strong>本质上来说，并不是只有找到了相应的工作我们才可以学好一项技术，而是，我们在通过解决实际问题，在和他人讨论，获得高手帮助的环境下，能更快更有效率地学习和成长。</strong></p>
<p>有时候，在工作中你反而学不到东西，那是因为你找的这个工作能够提供的场景不够丰富，需要解决的实际问题太过简单，以及你的同事对你的帮助不大。这时，这个工作反而限制了你的学习和成长。</p>
<p>所以，两点。</p>
<ul>
<li>找工作不只是找用这个技术的工作，更是要找场景，找实际问题，找团队。这些才是本质。一项技术很多公司都在用，然而，只有进入到有更多的场景、有挑战性的问题、有靠谱团队的公司，才对学习和成长更有帮助。</li>
<li>不要完全把自己的学习寄希望于找一份工作，才会学得好。在一些开源社区内，有助于学习的场景会更多，要解决的实际问题也更多，同时你能接触到的牛人也更多。特别是一些有大量公司和几万、几十万甚至上百万的开发人员在贡献代码的项目，可以让人成长很快。</li>
</ul>
<p><strong>总之，找到学习的方法，提升自己对新事物学习的能力，才是真正靠谱的。</strong></p>
<h1 id="关于技术和价值"><a href="#关于技术和价值" class="headerlink" title="关于技术和价值"></a>关于技术和价值</h1><p>后面，我们聊到了什么样的技术会是属于未来的技术，以及应该把时间花在什么样的技术上。一个问题：“你觉得，让人登月探索宇宙的技术价值大，还是造高铁的技术价值大？或者是科学种田的技术价值大？……”</p>
<p>是的，对于这个问题，从不同的角度上看，就会得到不同的结论。似乎，我们无法说明白哪项技术创造的价值更大，因为完全没法比较。</p>
<p>于是我又说了一个例子，在第一次工业革命的时候，也就是蒸汽机时代，除了蒸汽机之外还有其它一些技术含量更高的技术，比如化学、冶金、水泥、玻璃……但是，这么一个不起眼的技术引发了人类社会的变革。也许，那个时候，在技术圈中，很多技术专家还鄙视蒸汽机的技术含量太低呢。</p>
<p>我并不是想说高大上的技术无用，我想说的是，技术无贵贱，很多伟大的事就是通过一些不起眼的技术造就的。所以，我们应该关注的是：</p>
<ul>
<li>要用技术解决什么样的问题，场景非常重要；</li>
<li>如何降低技术的学习成本，提高易用性，从而可以让技术更为普及。</li>
</ul>
<p>另外。假设，我们今天没有电，忽然，有人说他发明了电。这个世界上的很多人都会觉得“电”这个东西没什么用，而只有等到“电灯”的发明，人们才明白发明“电”是多么牛。</p>
<p>所以，对于一些“基础技术”来说，通常会在某段时间内被人类社会低估。就像国内前几年低估“云计算”技术一样。基础技术就像是创新的引擎，其不断地成熟和完善会导致更上层的技术不断地衍生，越滚越大。</p>
<p>而在一个基础技术被广泛应用的过程中，如何规模化也会成为一个关键技术。这就好像发电厂一样，没有发电厂，电力就无法做到规模化。记得汽车发明的时候，要组装一个汽车的时间成本、人力成本、物力成本都非常高，所以完全无法做到规模化，而通过模块化分工、自动化生产等技术手段才释放了产能，从而普及。</p>
<p>所以，一项有价值的技术，并不在于这项技术是否有技术含量，而是在于：</p>
<ul>
<li>能否低成本高效率地解决实际问题；</li>
<li>是不是众多产品的基础技术；</li>
<li>是不是可以支持规模化的技术。</li>
</ul>
<p>对于搞计算机软件的人来说，也可以找到相对应的技术点。比如：</p>
<ul>
<li>低成本高效率地解决实际问题的技术，一定是自动化的技术。软件天生就是用来完成重复劳动的，天生就是用来做自动化的。而未来的 AI 和 IoT 也是在拼命数字化和自动化还没有自动化的领域。</li>
<li>基础技术总是枯燥和有价值的。数学、算法、网络、存储等基础技术吃得越透，就越容易服务上层的各种衍生技术或产品。</li>
<li>支持规模化的技术也是很有价值的。在软件行业中，也就是 PaaS 的相关技术。</li>
</ul>
<p>当然，我的意思并不是别的技术都没有价值了。重申一下，<strong>技术无贵贱。我只是想说，能规模化低成本高效率解决实际问题的技术及其基础技术，就算是很 low，也是很有价值的。</strong></p>
<h1 id="关于趋势和未来"><a href="#关于趋势和未来" class="headerlink" title="关于趋势和未来"></a>关于趋势和未来</h1><p>似乎有些规律也是有迹可寻的。</p>
<p><strong>这个世界的技术趋势和未来其实是被人控制的</strong>。就是被那些有权有势有钱的公司或国家来控制的。当然，他们控制的不是长期的未来，但短期的未来（3-5 年）一定是他们控制着的。</p>
<p>也就是说，技术的未来要去哪，主要是看这个世界的投入会到哪。基本上就是这个世界上的有钱有势的人把财富投到哪个领域，也就是这个世界的大公司或大国们的规划。一旦他们把大量的金钱投到某个领域，这个领域就会得到发展，那么发展之后，这个领域也就成为未来了。只要是有一堆公司在往一个方向上不间断地投资或者花钱，这个方向不想成为未来似乎都不可能。</p>
<p>听上去多少有点儿令人沮丧，但世界就是如此简单粗暴运作着的。</p>
<p>所以，对于在这个世界里排不上号的人来说，只能默默地跟随着这些大公司所引领的趋势和未来。对一些缺钱缺人的创业公司，唯一能够做的，也许只是两条路，一是用更为低的成本来提供和大公司相应的技术，另一条路是在细分垂直市场上做得比大公司更专更精。等着自己有一天长大后，也能加入第一梯队从而“引领”未来。</p>
<h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>在我们的生活和工作中，总是会有很多人混淆一些看似有联系，实则却关系不大的词和概念，分辨不清事物的表象和本质。</p>
<p>兴趣和投入。表面上，兴趣是决定一件事儿能否做持久的关键因素。而反观我们自己和他人的经历不难发现，兴趣充演的角色通常是敲门砖，它引发我们关注到某事某物。而真正能让我们坚持下去的，实际上是做一件事之后从中收获到的正反馈，也就是成就感。</p>
<p>同样，人们也经常搞错学习和工作之间的关系。多数人都会认为，在工作中学习和成长速度快。而细细观察下来，就会发现，工作不过是提供了一个能够解决实际问题，能跟人讨论，有高手帮助的环境。所以说，让我们成长的并不是工作本身，而是有利于学习的环境。也就是说，如果我们想学习，除了可以选择有助于学习的工作机会，开源社区提供的环境同样有助于我们的学习和提高，那里高手更多，实际问题不少。</p>
<p>还有，技术和价值。人们通常认为技术含量高的技术其价值会更高，而历史上无数的事实却告诉我们，能规模化、低成本、高效率地解决实际问题的技术及其基础技术，才发挥出了更为深远的影响，甚至其价值更是颠覆性的，难以估量。</p>
<p>趋势和未来也是被误解得很深的一对“孪生兄弟”。虽然大家通常会认为有什么样的技术趋势，必然带来什么样的未来。殊不知，所谓的趋势和未来，其实都是可以由人为控制的，特别是哪些有钱有势的人和公司。也就是，社会的资金和资源流向什么领域，这个领域势必会得到成长和发展，会逐渐形成趋势，进而成为未来。我们遵循这样的规律，就能很容易地判断出未来的，最起码是近几年的，技术流向了。</p>
<p>再如，加班和产出，努力和成功，速度和效率……加班等于高产出吗？显然不是。很努力就一定会成功吗？当然不是。速度快就是效率高吗？更加不是。可以枚举的还有很多，如干得多就等于干得好吗？等等。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Chinese/" rel="tag">Chinese</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-分布式架构入门"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/08/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%85%A5%E9%97%A8/"
    >分布式架构入门</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/02/08/%E5%88%86%E5%B8%83%E5%BC%8F%E6%9E%B6%E6%9E%84%E5%85%A5%E9%97%A8/" class="article-date">
  <time datetime="2020-02-08T19:56:10.000Z" itemprop="datePublished">2020-02-08</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>分布式系统涵盖的面非常广，如下：</p>
<ul>
<li><strong>服务调度</strong>，涉及服务发现、配置管理、弹性伸缩、故障恢复等。</li>
<li><strong>资源调度</strong>，涉及对底层资源的调度使用，如计算资源、网络资源和存储资源等。</li>
<li><strong>流量调度</strong>，涉及路由、负载均衡、流控、熔断等。</li>
<li><strong>数据调度</strong>，涉及数据复本、数据一致性、分布式事务、分库、分表等。</li>
<li><strong>容错处理</strong>，涉及隔离、幂等、重试、业务补偿、异步、降级等。</li>
<li><strong>自动化运维</strong>，涉及持续集成、持续部署、全栈监控、调用链跟踪等。</li>
</ul>
<p>所有这些形成了分布式架构的整体复杂度，也造就了分布式系统中的很多很多论文、图书以及很多很多的项目。要学好分布式系统及其架构，我们需要大量的时间和实践才能真正掌握这些技术。</p>
<p>这里有几点需要你注意一下。</p>
<ul>
<li><strong>分布式系统之所以复杂，就是因为其太容易也太经常出错了</strong>。这意味着，<strong>你要把处理错误的代码当成正常功能的代码来处理</strong>。</li>
<li><strong>开发一个健壮的分布式系统的成本是单体系统的几百倍甚至几万倍</strong>。这意味着，<strong>我们要自己开发一个，需要能力很强的开发人员</strong>。</li>
<li><strong>非常健壮的开源的分布式系统并不多，或者说基本没有</strong>。这意味着，<strong>如果你要用开源的，那么你需要 hold 得住其源码</strong>。</li>
<li><strong>管理或是协调多个服务或机器是非常难的</strong>。这意味着，<strong>我们要去读很多很多的分布式系统的论文</strong>。</li>
<li><strong>在分布式环境下，出了问题是很难 debug 的</strong>。这意味着，<strong>我们需要非常好的监控和跟踪系统，还需要经常做演练和测试</strong>。</li>
<li><strong>在分布式环境下，你需要更科学地分析和统计</strong>。这意味着，<strong>我们要用 P90 这样的统计指标，而不是平均值，我们还需要做容量计划和评估</strong>。</li>
<li><strong>在分布式环境下，需要应用服务化</strong>。这意味着，<strong>我们需要一个服务开发框架，比如 SOA 或微服务</strong>。</li>
<li><strong>在分布式环境下，故障不可怕，可怕的是影响面过大，时间过长</strong>。这意味着，<strong>我们需要花时间来开发我们的自动化运维平台</strong>。</li>
</ul>
<p>总之，在分布式环境下，一切都变得非常复杂。要进入这个领域，你需要有足够多的耐性和足够强的心态来接受各式各样的失败。当拥有丰富的实践和经验后，你才会有所建树。这并不是一日之功，你可能要在这个领域花费数年甚至数十年的时间。</p>
<h2 id="分布式架构入门"><a href="#分布式架构入门" class="headerlink" title="分布式架构入门"></a>分布式架构入门</h2><p>学习如何设计可扩展的架构将会有助于你成为一个更好的工程师。系统设计是一个很宽泛的话题。在互联网上，关于架构设计原则的资源也是多如牛毛。所以，你需要知道一些基本概念，对此，这里你先读一下下面两篇文章，都非常不错。</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.aosabook.org/en/distsys.html">Scalable Web Architecture and Distributed Systems</a> ，这篇文章会给你一个大概的分布式架构是怎么来解决系统扩展性问题的粗略方法。</li>
<li><a target="_blank" rel="noopener" href="http://www.slideshare.net/jboner/scalability-availability-stability-patterns">Scalability, Availability &amp; Stability Patterns</a> ，这个 PPT 能在扩展性、可用性、稳定性等方面给你一个非常大的架构设计视野和思想，可以让你感受一下大概的全景图。</li>
</ul>
<p>然后，强烈推荐 GitHub 上的一篇文档 - <a target="_blank" rel="noopener" href="https://github.com/donnemartin/system-design-primer">System Design Primer</a> ，这个仓库主要组织收集分布式系统的一些与扩展性相关的资源，它可以帮助你学习如何构建可扩展的架构。</p>
<p>目前这个仓库收集到了好些系统架构和设计的基本方法。其中包括：CAP 理论、一致性模型、可用性模式、DNS、CDN、负载均衡、反向代理、应用层的微服务和服务发现、关系型数据库和 NoSQL、缓存、异步通讯、安全等。</p>
<p>上面这几篇文章基本足够可以让你入门了，因为其中基本涵盖了所有与系统架构相关的技术。这些技术，足够这世上 90% 以上的公司用了，只有超级巨型的公司才有可能使用更高层次的技术。</p>
<h2 id="分布式理论"><a href="#分布式理论" class="headerlink" title="分布式理论"></a>分布式理论</h2><p>下面学习一下分布式方面的理论知识。</p>
<p>首先，你需要看一下 <a target="_blank" rel="noopener" href="https://github.com/aphyr/distsys-class">An introduction to distributed systems</a>。 这只是某个教学课程的提纲，几乎涵盖了分布式系统方面的所有知识点，而且辅以简洁并切中要害的说明文字，非常适合初学者提纲挈领地了解知识全貌，快速与现有知识结合，形成知识体系。这也是一个分布式系统的知识图谱，可以让你看到分布式系统的整体全貌。你可以根据这个知识图 Google 下去，然后你会学会所有的东西。</p>
<p>然后，你需要了解一下拜占庭将军问题（<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Byzantine_fault_tolerance">Byzantine Generals Problem</a>）。这个问题是莱斯利·兰波特（Leslie Lamport）于 1982 年提出用来解释一致性问题的一个虚构模型（<a target="_blank" rel="noopener" href="https://www.microsoft.com/en-us/research/uploads/prod/2016/12/The-Byzantine-Generals-Problem.pdf">论文地址</a>）。拜占庭是古代东罗马帝国的首都，由于地域宽广，守卫边境的多个将军（系统中的多个节点）需要通过信使来传递消息，达成某些一致的决定。但由于将军中可能存在叛徒（系统中节点出错），这些叛徒将努力向不同的将军发送不同的消息，试图会干扰一致性的达成。拜占庭问题即为在此情况下，如何让忠诚的将军们能达成行动的一致。</p>
<p>对于拜占庭问题来说，假如节点总数为 <code>N</code>，叛变将军数为 <code>F</code>，则当 <code>N &gt;= 3F + 1</code> 时，问题才有解，即拜占庭容错（Byzantine Fault Tolerant，BFT）算法。拜占庭容错算法解决的是，网络通信可靠但节点可能故障情况下一致性该如何达成的问题。</p>
<p>最早由卡斯特罗（Castro）和利斯科夫（Liskov）在 1999 年提出的实用拜占庭容错（Practical Byzantine Fault Tolerant，PBFT）算法，是第一个得到广泛应用的 BFT 算法。只要系统中有 2/3 的节点是正常工作的，则可以保证一致性。PBFT 算法包括三个阶段来达成共识：预准备（Pre-Prepare）、准备（Prepare）和提交（Commit）。</p>
<p>这里有几篇和这个问题相关的文章，推荐阅读。</p>
<ul>
<li><a target="_blank" rel="noopener" href="http://www.drdobbs.com/cpp/the-byzantine-generals-problem/206904396">Dr.Dobb’s - The Byzantine Generals Problem</a></li>
<li><a target="_blank" rel="noopener" href="http://blog.jameslarisch.com/the-byzantine-generals-problem">The Byzantine Generals Problem</a></li>
<li><a target="_blank" rel="noopener" href="http://pmg.csail.mit.edu/papers/osdi99.pdf">Practicle Byzantine Fault Tolerance</a></li>
</ul>
<p>拜占庭容错系统研究中有三个重要理论：CAP、FLP 和 DLS。</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/CAP_theorem">CAP 定理</a>，CAP 理论相信你应该听说过不下 N 次了。CAP 定理是分布式系统设计中最基础也是最为关键的理论。CAP 定理指出，分布式数据存储不可能同时满足以下三个条件：一致性（Consistency）、可用性（Availability）和 分区容忍（Partition tolerance）。 “在网络发生阻断（partition）时，你只能选择数据的一致性（consistency）或可用性（availability），无法两者兼得”。</p>
<p>论点比较直观：如果网络因阻断而分隔为二，在其中一边我送出一笔交易：“将我的十元给 A”；在另一半我送出另一笔交易：” 将我的十元给 B “。此时系统要不是，a）无可用性，即这两笔交易至少会有一笔交易不会被接受；要不就是，b）无一致性，一半看到的是 A 多了十元而另一半则看到 B 多了十元。要注意的是，CAP 理论和扩展性（scalability）是无关的，在分片（sharded）或非分片的系统皆适用。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://the-paper-trail.org/blog/a-brief-tour-of-flp-impossibility/">FLP impossibility</a>- 在异步环境中，如果节点间的网络延迟没有上限，只要有一个恶意的节点存在，就没有算法能在有限的时间内达成共识。但值得注意的是， <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Las_Vegas_algorithm">“Las Vegas” algorithms</a>（这个算法又叫撞大运算法，其保证结果正确，只是在运算时所用资源上进行赌博，一个简单的例子是随机快速排序，它的 pivot 是随机选的，但排序结果永远一致）在每一轮皆有一定机率达成共识，随着时间增加，机率会越趋近于 1。而这也是许多成功的共识算法会采用的解决问题的办法。</p>
</li>
<li><p>容错的上限 - 由 <a target="_blank" rel="noopener" href="http://groups.csail.mit.edu/tds/papers/Lynch/jacm88.pdf">DLS 论文</a> ，我们可以得到以下结论。</p>
<ul>
<li>在部分同步（partially synchronous）的网络环境中（即网络延迟有一定的上限，但我们无法事先知道上限是多少），协议可以容忍最多 1/3 的拜占庭故障（Byzantine fault）。</li>
<li>在异步（asynchronous）的网络环境中，具有确定性质的协议无法容忍任何错误，但这篇论文并没有提及 <a target="_blank" rel="noopener" href="http://link.springer.com/chapter/10.1007%2F978-3-540-77444-0_7">randomized algorithms</a>，在这种情况下可以容忍最多 1/3 的拜占庭故障。</li>
<li>在同步（synchronous）网络环境中（即网络延迟有上限且上限是已知的），协议可以容忍 100% 的拜占庭故障，但当超过 1/2 的节点为恶意节点时，会有一些限制条件。要注意的是，我们考虑的是 “ 具有认证特性的拜占庭模型（authenticated Byzantine）”，而不是 “ 一般的拜占庭模型 “；具有认证特性指的是将如今已经过大量研究且成本低廉的公私钥加密机制应用在我们的算法中。</li>
</ul>
</li>
</ul>
<p>当然，还有一个著名的“8 条荒谬的分布式假设（<a target="_blank" rel="noopener" href="http://en.wikipedia.org/wiki/Fallacies_of_distributed_computing">Fallacies of Distributed Computing</a>）”。</p>
<ol>
<li>网络是稳定的。</li>
<li>网络传输的延迟是零。</li>
<li>网络的带宽是无穷大。</li>
<li>网络是安全的。</li>
<li>网络的拓扑不会改变。</li>
<li>只有一个系统管理员。</li>
<li>传输数据的成本为零。</li>
<li>整个网络是同构的。</li>
</ol>
<p>阿尔农·罗特姆 - 盖尔 - 奥兹（Arnon Rotem-Gal-Oz）写了一篇长文 <a target="_blank" rel="noopener" href="http://www.rgoarchitects.com/Files/fallacies.pdf">Fallacies of Distributed Computing Explained</a> 来解释为什么这些观点是错误的。另外，<a target="_blank" rel="noopener" href="http://blog.fogcreek.com/eight-fallacies-of-distributed-computing-tech-talk/">加勒思·威尔逊（Gareth Wilson）的文章</a> 则用日常生活中的例子，对这些点做了通俗的解释。为什么我们深刻地认识到这 8 个错误？是因为，这要我们清楚地认识到——在分布式系统中错误是不可能避免的，我们在分布式系统中，能做的不是避免错误，而是要把错误的处理当成功能写在代码中。</p>
<p>下面分享几篇一致性方面的论文。</p>
<ul>
<li><p>当然，关于经典的 CAP 理论，也存在一些误导的地方，这个问题在 2012 年有一篇论文 <a target="_blank" rel="noopener" href="https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed">CAP Twelve Years Later: How the Rules Have Changed</a> （<a target="_blank" rel="noopener" href="http://www.infoq.com/cn/articles/cap-twelve-years-later-how-the-rules-have-changed">中译版</a>）中做了一些讨论，主要是说，在 CAP 中最大的问题就是分区，也就是 P，在 P 发生的情况下，非常难以保证 C 和 A。然而，这是强一致性的情况。</p>
<p>其实，在很多时候，我们并不需要强一致性的系统，所以后来，人们争论关于数据一致性和可用性时，主要是集中在强一致性的 ACID 或最终一致性的 BASE。当时，BASE 还不怎么为世人所接受，主要是大家都觉得 ACID 是最完美的模型，大家很难接受不完美的 BASE。在 CAP 理论中，大家总是觉得需要 “ 三选二 “，也就是说，P 是必选项，那 “ 三选二 “ 的选择题不就变成数据一致性 (consistency)、服务可用性 (availability) 间的 “ 二选一 “ ？</p>
<p>然而，现实却是，P 很少遇到，而 C 和 A 这两个事，工程实践中一致性有不同程度，可用性也有不同等级，在保证分区容错性的前提下，放宽约束后可以兼顾一致性和可用性，两者不是非此即彼。其实，在一个时间可能允许的范围内是可以取舍并交替选择的。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://pdfs.semanticscholar.org/5015/8bc1a8a67295ab7bce0550886a9859000dc2.pdf">Harvest, Yield, and Scalable Tolerant Systems</a> ，这篇论文是基于上面那篇 “CAP 12 年后 “ 的论文写的，它主要提出了 Harvest 和 Yield 概念，并把上面那篇论文中所讨论的东西讲得更为仔细了一些。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://queue.acm.org/detail.cfm?id=1394128">Base: An Acid Alternative</a> （<a target="_blank" rel="noopener" href="http://www.cnblogs.com/savorboard/p/base-an-acid-alternative.html">中译版</a>），本文是 eBay 的架构师在 2008 年发表给 ACM 的文章，是一篇解释 BASE 原则，或者说最终一致性的经典文章。文中讨论了 BASE 与 ACID 原则的基本差异, 以及如何设计大型网站以满足不断增长的可伸缩性需求，其中有如何对业务做调整和折中，以及一些具体的折中技术的介绍。一个比较经典的话是——“在对数据库进行分区后, 为了可用性（Availability）牺牲部分一致性（Consistency）可以显著地提升系统的可伸缩性 (Scalability)”。</p>
</li>
<li><p><a target="_blank" rel="noopener" href="https://www.allthingsdistributed.com/2008/12/eventually_consistent.html">Eventually Consistent</a> ，这篇文章是 AWS 的 CTO 维尔纳·沃格尔（Werner Vogels）在 2008 年发布在 ACM Queue 上的一篇数据库方面的重要文章，阐述了 NoSQL 数据库的理论基石——最终一致性，对传统的关系型数据库（ACID，Transaction）做了较好的补充。</p>
</li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Chinese/" rel="tag">Chinese</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
    <article
  id="post-how-database-work"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2020/02/07/how-database-work/"
    >How database work</a> 
</h2>
 

    </header>
     
    <div class="article-meta">
      <a href="/2020/02/07/how-database-work/" class="article-date">
  <time datetime="2020-02-08T01:31:19.000Z" itemprop="datePublished">2020-02-07</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>When it comes to relational databases, I can’t help thinking that something is missing. They’re used everywhere. There are many different databases: from the small and useful SQLite to the powerful Teradata. But, there are only a few articles that explain how a database works. You can google by yourself “how does a relational database work” to see how few results there are. Moreover, those articles are short. Now, if you look for the last trendy technologies (Big Data, NoSQL or JavaScript), you’ll find more in-depth articles explaining how they work.</p>
<p>Are relational databases too old and too boring to be explained outside of university courses, research papers and books?</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/main_databases.jpg"><img src="../../public/images/main_databases-20200207203251326.jpg" alt="logos of main databases"></a></p>
<p>As a developer, I HATE using something I don’t understand. And, if databases have been used for 40 years, there must be a reason. Over the years, I’ve spent hundreds of hours to really understand these weird black boxes I use every day. <strong>Relational Databases</strong> <strong>are</strong> very interesting because they’re <strong>based on useful and reusable concepts</strong>. If understanding a database interests you but you’ve never had the time or the will to dig into this wide subject, you should like this article.</p>
<p>Though the title of this article is explicit, <strong>the aim of this article is NOT to understand how to use a database</strong>. Therefore, <strong>you should already know how to write a simple join query and basic CRUD queries</strong>; otherwise you might not understand this article. This is the only thing you need to know, I’ll explain everything else.</p>
<p>I’ll start with some computer science stuff like time complexity. I know that some of you hate this concept but, without it, you can’t understand the cleverness inside a database. Since it’s a huge topic, <strong>I’ll focus on</strong> what I think is essential: <strong>the way a database handles an SQL query</strong>. I’ll only present <strong>the basic concepts behind a database</strong> so that at the end of the article you’ll have a good idea of what’s happening under the hood.</p>
<p>Since it’s a long and technical article that involves many algorithms and data structures, take your time to read it. Some concepts are more difficult to understand; you can skip them and still get the overall idea.</p>
<p>For the more knowledgeable of you, this article is more or less divided into 3 parts:</p>
<ul>
<li>An overview of low-level and high-level database components</li>
<li>An overview of the query optimization process</li>
<li>An overview of the transaction and buffer pool management</li>
</ul>
<p>Contents [<a target="_blank" rel="noopener" href="http://coding-geek.com/how-databases-work/#">show</a>]</p>
<h1 id="Back-to-basics"><a href="#Back-to-basics" class="headerlink" title="Back to basics"></a>Back to basics</h1><p>A long time ago (in a galaxy far, far away….), developers had to know exactly the number of operations they were coding. They knew by heart their algorithms and data structures because they couldn’t afford to waste the CPU and memory of their slow computers.</p>
<p>In this part, I’ll remind you about some of these concepts because they are essential to understand a database. I’ll also introduce the notion of <strong>database index</strong>.</p>
<h2 id="O-1-vs-O-n2"><a href="#O-1-vs-O-n2" class="headerlink" title="O(1) vs O(n2)"></a>O(1) vs O(n2)</h2><p>Nowadays, many developers don’t care about time complexity … and they’re right!</p>
<p>But when you deal with a large amount of data (I’m not talking about thousands) or if you’re fighting for milliseconds, it becomes critical to understand this concept. And guess what, databases have to deal with both situations! I won’t bore you a long time, just the time to get the idea. This will help us later to understand the concept of <strong>cost based optimization</strong>.</p>
<h3 id="The-concept"><a href="#The-concept" class="headerlink" title="The concept"></a>The concept</h3><p>The <strong>time complexity is used to see how long an algorithm will take for a given amount of data</strong>. To describe this complexity, computer scientists use the mathematical big O notation. This notation is used with a function that describes how many operations an algorithm needs for a given amount of input data.</p>
<p>For example, when I say “this algorithm is in O( some_function() )”, it means that for a certain amount of data the algorithm needs some_function(a_certain_amount_of_data) operations to do its job.</p>
<p><strong>What’s important is</strong> not the amount of data but <strong>the way the number of operations increases when the amount of data increases</strong>. The time complexity doesn’t give the exact number of operations but a good idea.</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/TimeComplexity.png"><img src="../../public/images/TimeComplexity-20200207203251326.png" alt="time complexity analysis"></a></p>
<p>In this figure, you can see the evolution of different types of complexities. I used a logarithmic scale to plot it. In other words, the number of data is quickly increasing from 1 to 1 billion. We can see that:</p>
<ul>
<li>The O(1) or constant complexity stays constant (otherwise it wouldn’t be called constant complexity).</li>
<li>The <strong>O(log(n)) stays low even with billions of data</strong>.</li>
<li>The worst complexity is the <strong>O(n2) where the number of operations quickly explodes</strong>.</li>
<li>The two other complexities are quickly increasing.</li>
</ul>
<h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples"></a>Examples</h3><p>With a low amount of data, the difference between O(1) and O(n2) is negligible. For example, let’s say you have an algorithm that needs to process 2000 elements.</p>
<ul>
<li>An O(1) algorithm will cost you 1 operation</li>
<li>An O(log(n)) algorithm will cost you 7 operations</li>
<li>An O(n) algorithm will cost you 2 000 operations</li>
<li>An O(n*log(n)) algorithm will cost you 14 000 operations</li>
<li>An O(n2) algorithm will cost you 4 000 000 operations</li>
</ul>
<p>The difference between O(1) and O(n2) seems a lot (4 million) but you’ll lose at max 2 ms, just the time to blink your eyes. Indeed, current processors can handle <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Instructions_per_second">hundreds of millions of operations per second</a>. This is why performance and optimization are not an issue in many IT projects.</p>
<p>As I said, it’s still important to know this concept when facing a huge number of data. If this time the algorithm needs to process 1 000 000 elements (which is not that big for a database):</p>
<ul>
<li>An O(1) algorithm will cost you 1 operation</li>
<li>An O(log(n)) algorithm will cost you 14 operations</li>
<li>An O(n) algorithm will cost you 1 000 000 operations</li>
<li>An O(n*log(n)) algorithm will cost you 14 000 000 operations</li>
<li>An O(n2) algorithm will cost you 1 000 000 000 000 operations</li>
</ul>
<p>I didn’t do the math but I’d say with the O(n2) algorithm you have the time to take a coffee (even a second one!). If you put another 0 on the amount of data, you’ll have the time to take a long nap.</p>
<h3 id="Going-deeper"><a href="#Going-deeper" class="headerlink" title="Going deeper"></a>Going deeper</h3><p>To give you an idea:</p>
<ul>
<li>A search in a good hash table gives an element in O(1)</li>
<li>A search in a well-balanced tree gives a result in O(log(n))</li>
<li>A search in an array gives a result in O(n)</li>
<li>The best sorting algorithms have an O(n*log(n)) complexity.</li>
<li>A bad sorting algorithm has an O(n2) complexity</li>
</ul>
<p>Note: In the next parts, we’ll see these algorithms and data structures.</p>
<p>There are multiple types of time complexity:</p>
<ul>
<li>the average case scenario</li>
<li>the best case scenario</li>
<li>and the worst case scenario</li>
</ul>
<p>The time complexity is often the worst case scenario.</p>
<p>I only talked about time complexity but complexity also works for:</p>
<ul>
<li>the memory consumption of an algorithm</li>
<li>the disk I/O consumption of an algorithm</li>
</ul>
<p>Of course there are worse complexities than n2, like:</p>
<ul>
<li>n4: that sucks! Some of the algorithms I’ll mention have this complexity.</li>
<li>3n: that sucks even more! One of the algorithms we’re going to see in the middle of this article has this complexity (and it’s really used in many databases).</li>
<li>factorial n : you’ll never get your results, even with a low amount of data.</li>
<li>nn: if you end-up with this complexity, you should ask yourself if IT is really your field…</li>
</ul>
<p>Note: I didn’t give you the real definition of the big O notation but just the idea. You can read this article on <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Big_O_notation">Wikipedia</a> for the real (asymptotic) definition.</p>
<h2 id="Merge-Sort"><a href="#Merge-Sort" class="headerlink" title="Merge Sort"></a>Merge Sort</h2><p>What do you do when you need to sort a collection? What? You call the sort() function … ok, good answer… But for a database you have to understand how this sort() function works.</p>
<p>There are several good sorting algorithms so I’ll focus on the most important one: <strong>the merge sort</strong>. You might not understand right now why sorting data is useful but you should after the part on query optimization. Moreover, understanding the merge sort will help us later to understand a common database join operation called the <strong>merge join</strong>.</p>
<h3 id="Merge"><a href="#Merge" class="headerlink" title="Merge"></a>Merge</h3><p>Like many useful algorithms, the merge sort is based on a trick: merging 2 sorted arrays of size N/2 into a N-element sorted array only costs N operations. This operation is called a <strong>merge</strong>.</p>
<p>Let’s see what this means with a simple example:</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/merge_sort_3.png"><img src="../../public/images/merge_sort_3-20200207203251311.png" alt="merge operation during merge sort algorithm"></a></p>
<p>You can see on this figure that to construct the final sorted array of 8 elements, you only need to iterate one time in the 2 4-element arrays. Since both 4-element arrays are already sorted:</p>
<ul>
<li>1) you compare both current elements in the 2 arrays (current=first for the first time)</li>
<li>2) then take the lowest one to put it in the 8-element array</li>
<li>3) and go to the next element in the array you took the lowest element</li>
<li>and repeat 1,2,3 until you reach the last element of one of the arrays.</li>
<li>Then you take the rest of the elements of the other array to put them in the 8-element array.</li>
</ul>
<p>This works because both 4-element arrays are sorted and therefore you don’t need to “go back” in these arrays.</p>
<p>Now that we’ve understood this trick, here is my pseudocode of the merge sort.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array mergeSort(array a)&#96;&#96;  &#96;&#96;if&#96;&#96;(length(a)&#x3D;&#x3D;&#96;&#96;1&#96;&#96;)&#96;&#96;   &#96;&#96;return&#96; &#96;a[&#96;&#96;0&#96;&#96;];&#96;&#96;  &#96;&#96;end &#96;&#96;if&#96; &#96;  &#96;&#96;&#x2F;&#x2F;recursive calls&#96;&#96;  &#96;&#96;[left_array right_array] :&#x3D; split_into_2_equally_sized_arrays(a);&#96;&#96;  &#96;&#96;array new_left_array :&#x3D; mergeSort(left_array);&#96;&#96;  &#96;&#96;array new_right_array :&#x3D; mergeSort(right_array);&#96; &#96;  &#96;&#96;&#x2F;&#x2F;merging the 2 small ordered arrays into a big one&#96;&#96;  &#96;&#96;array result :&#x3D; merge(new_left_array,new_right_array);&#96;&#96;  &#96;&#96;return&#96; &#96;result;</span><br></pre></td></tr></table></figure>

<p>The merge sort breaks the problem into smaller problems then finds the results of the smaller problems to get the result of the initial problem (note: this kind of algorithms is called divide and conquer). If you don’t understand this algorithm, don’t worry; I didn’t understand it the first time I saw it. If it can help you, I see this algorithm as a two-phase algorithm:</p>
<ul>
<li>The division phase where the array is divided into smaller arrays</li>
<li>The sorting phase where the small arrays are put together (using the merge) to form a bigger array.</li>
</ul>
<h3 id="Division-phase"><a href="#Division-phase" class="headerlink" title="Division phase"></a>Division phase</h3><p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/merge_sort_1.png"><img src="../../public/images/merge_sort_1-20200207203251327.png" alt="division phaseduring merge sort algorithm"></a></p>
<p>During the division phase, the array is divided into unitary arrays using 3 steps. The formal number of steps is log(N) (since N=8, log(N) = 3).</p>
<p>How do I know that?</p>
<p>I’m a genius! In one word: mathematics. The idea is that each step divides the size of the initial array by 2. The number of steps is the number of times you can divide the initial array by two. This is the exact definition of logarithm (in base 2).</p>
<h3 id="Sorting-phase"><a href="#Sorting-phase" class="headerlink" title="Sorting phase"></a>Sorting phase</h3><p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/merge_sort_2.png"><img src="../../public/images/merge_sort_2-20200207203251328.png" alt="sort phaseduring merge sort algorithm"></a></p>
<p>In the sorting phase, you start with the unitary arrays. During each step, you apply multiple merges and the overall cost is N=8 operations:</p>
<ul>
<li>In the first step you have 4 merges that cost 2 operations each</li>
<li>In the second step you have 2 merges that cost 4 operations each</li>
<li>In the third step you have 1 merge that costs 8 operations</li>
</ul>
<p>Since there are log(N) steps, <strong>the overall costs N * log(N) operations</strong>.</p>
<h3 id="The-power-of-the-merge-sort"><a href="#The-power-of-the-merge-sort" class="headerlink" title="The power of the merge sort"></a>The power of the merge sort</h3><p>Why this algorithm is so powerful?</p>
<p>Because:</p>
<ul>
<li>You can modify it in order to reduce the memory footprint, in a way that you don’t create new arrays but you directly modify the input array.</li>
</ul>
<p>Note: this kind of algorithms is called <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/In-place_algorithm">in-place</a>.</p>
<ul>
<li>You can modify it in order to use disk space and a small amount of memory at the same time without a huge disk I/O penalty. The idea is to load in memory only the parts that are currently processed. This is important when you need to sort a multi-gigabyte table with only a memory buffer of 100 megabytes.</li>
</ul>
<p>Note: this kind of algorithms is called <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/External_sorting">external sorting</a>.</p>
<ul>
<li>You can modify it to run on multiple processes/threads/servers.</li>
</ul>
<p>For example, the distributed merge sort is one of the key components of <a target="_blank" rel="noopener" href="https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapreduce/Reducer.html">Hadoop </a>(which is THE framework in Big Data).</p>
<ul>
<li>This algorithm can turn lead into gold (true fact!).</li>
</ul>
<p>This sorting algorithm is used in most (if not all) databases but it’s not the only one. If you want to know more, you can read this <a target="_blank" rel="noopener" href="http://wwwlgis.informatik.uni-kl.de/archiv/wwwdvs.informatik.uni-kl.de/courses/DBSREAL/SS2005/Vorlesungsunterlagen/Implementing_Sorting.pdf">research paper</a> that discusses the pros and cons of the common sorting algorithms in a database.</p>
<h2 id="Array-Tree-and-Hash-table"><a href="#Array-Tree-and-Hash-table" class="headerlink" title="Array, Tree and Hash table"></a>Array, Tree and Hash table</h2><p>Now that we understand the idea behind time complexity and sorting, I have to tell you about 3 data structures. It’s important because they’re <strong>the backbone of modern databases</strong>. I’ll also introduce the notion of <strong>database index</strong>.</p>
<h3 id="Array"><a href="#Array" class="headerlink" title="Array"></a>Array</h3><p>The two-dimensional array is the simplest data structure. A table can be seen as an array. For example:</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/array.png"><img src="../../public/images/array-20200207203251327.png" alt="array table in databases"></a></p>
<p>This 2-dimensional array is a table with rows and columns:</p>
<ul>
<li>Each row represents a subject</li>
<li>The columns the features that describe the subjects.</li>
<li>Each column stores a certain type of data (integer, string, date …).</li>
</ul>
<p>Though it’s great to store and visualize data, when you need to look for a specific value it sucks.</p>
<p>For example, <strong>if you want to find all the guys who work in the UK</strong>, you’ll have to look at each row to find if the row belongs to the UK. <strong>This will cost you N operations</strong> (N being the number of rows) which is not bad but could there be a faster way? This is where trees come into play.</p>
<p>Note: Most modern databases provide advanced arrays to store tables efficiently like heap-organized tables or index-organized tables. But it doesn’t change the problem of fast searching for a specific condition on a group of columns.</p>
<h3 id="Tree-and-database-index"><a href="#Tree-and-database-index" class="headerlink" title="Tree and database index"></a>Tree and database index</h3><p>A binary search tree is a binary tree with a special property, the key in each node must be:</p>
<ul>
<li>greater than all keys stored in the left sub-tree</li>
<li>smaller than all keys stored in the right sub-tree</li>
</ul>
<p>Let’s see what it means visually</p>
<h4 id="The-idea"><a href="#The-idea" class="headerlink" title="The idea"></a>The idea</h4><p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/BST.png"><img src="../../public/images/BST-20200207203251343.png" alt="binary search tree"></a></p>
<p>This tree has N=15 elements. Let’s say I’m looking for 208:</p>
<ul>
<li>I start with the root whose key is 136. Since 136&lt;208, I look at the right sub-tree of the node 136.</li>
<li>398&gt;208 so, I look at the left sub-tree of the node 398</li>
<li>250&gt;208 so, I look at the left sub-tree of the node 250</li>
<li>200&lt;208 so, I look at the right sub-tree of the node 200. But 200 doesn’t have a right subtree, <strong>the value doesn’t exist</strong> (because if it did exist it would be in the right subtree of 200)</li>
</ul>
<p>Now let’s say I’m looking for 40</p>
<ul>
<li>I start with the root whose key is 136. Since 136&gt;40, I look at the left sub-tree of the node 136.</li>
<li>80&gt;40 so, I look at the left sub-tree of the node 80</li>
<li>40= 40, <strong>the node exists</strong>. I extract the id of the row inside the node (it’s not in the figure) and look at the table for the given row id.</li>
<li>Knowing the row id let me know where the data is precisely on the table and therefore I can get it instantly.</li>
</ul>
<p>In the end, both searches cost me the number of levels inside the tree. If you read carefully the part on the merge sort you should see that there are log(N) levels. So the <strong>cost of the search is log(N)</strong>, not bad!</p>
<h4 id="Back-to-our-problem"><a href="#Back-to-our-problem" class="headerlink" title="Back to our problem"></a>Back to our problem</h4><p>But this stuff is very abstract so let’s go back to our problem. Instead of a stupid integer, imagine the string that represents the country of someone in the previous table. Suppose you have a tree that contains the column “country” of the table:</p>
<ul>
<li>If you want to know who is working in the UK</li>
<li>you look at the tree to get the node that represents the UK</li>
<li>inside the “UK node” you’ll find the locations of the rows of the UK workers.</li>
</ul>
<p>This search only costs you log(N) operations instead of N operations if you directly use the array. What you’ve just imagined was a <strong>database index</strong>.</p>
<p>You can build a tree index for any group of columns (a string, an integer, 2 strings, an integer and a string, a date …) as long as you have a function to compare the keys (i.e. the group of columns) so that you can establish an <strong>order</strong> <strong>among the keys</strong> (which is the case for any basic types in a database).</p>
<h4 id="B-Tree-Index"><a href="#B-Tree-Index" class="headerlink" title="B+Tree Index"></a>B+Tree Index</h4><p>Although this tree works well to get a specific value, there is a BIG problem when you need to <strong>get multiple elements</strong> <strong>between two values</strong>. It will cost O(N) because you’ll have to look at each node in the tree and check if it’s between these 2 values (for example, with an in-order traversal of the tree). Moreover this operation is not disk I/O friendly since you’ll have to read the full tree. We need to find a way to efficiently do a <strong>range query</strong>. To answer this problem, modern databases use a modified version of the previous tree called B+Tree. In a B+Tree:</p>
<ul>
<li>only the lowest nodes (the leaves) <strong>store information</strong> (the location of the rows in the associated table)</li>
<li>the other nodes are just here <strong>to route</strong> to the right node <strong>during the search</strong>.</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/database_index.png"><img src="../../public/images/database_index-20200207203251357.png" alt="B+Tree index in databases"></a></p>
<p>As you can see, there are more nodes (twice more). Indeed, you have additional nodes, the “decision nodes” that will help you to find the right node (that stores the location of the rows in the associated table). But the search complexity is still in O(log(N)) (there is just one more level). The big difference is that <strong>the lowest nodes are linked to their successors</strong>.</p>
<p>With this B+Tree, if you’re looking for values between 40 and 100:</p>
<ul>
<li>You just have to look for 40 (or the closest value after 40 if 40 doesn’t exist) like you did with the previous tree.</li>
<li>Then gather the successors of 40 using the direct links to the successors until you reach 100.</li>
</ul>
<p>Let’s say you found M successors and the tree has N nodes. The search for a specific node costs log(N) like the previous tree. But, once you have this node, you get the M successors in M operations with the links to their successors. <strong>This search only costs M + log(N)</strong> operations vs N operations with the previous tree. Moreover, you don’t need to read the full tree (just M + log(N) nodes), which means less disk usage. If M is low (like 200 rows) and N large (1 000 000 rows) it makes a BIG difference.</p>
<p>But there are new problems (again!). If you add or remove a row in a database (and therefore in the associated B+Tree index):</p>
<ul>
<li>you have to keep the order between nodes inside the B+Tree otherwise you won’t be able to find nodes inside the mess.</li>
<li>you have to keep the lowest possible number of levels in the B+Tree otherwise the time complexity in O(log(N)) will become O(N).</li>
</ul>
<p>I other words, the B+Tree needs to be self-ordered and self-balanced. Thankfully, this is possible with smart deletion and insertion operations. But this comes with a cost: the insertion and deletion in a B+Tree are in O(log(N)). This is why some of you have heard that <strong>using too many indexes is not a good idea.</strong> Indeed, <strong>you’re slowing down the fast insertion/update/deletion of a row</strong> in a table since the database needs to update the indexes of the table with a costly O(log(N)) operation per index. Moreover, adding indexes means more workload for the <strong>transaction manager</strong> (we will see this manager at the end of the article).</p>
<p>For more details, you can look at the Wikipedia <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/B%2B_tree">article about B+Tree</a>. If you want an example of a B+Tree implementation in a database, look at <a target="_blank" rel="noopener" href="http://blog.jcole.us/2013/01/07/the-physical-structure-of-innodb-index-pages/">this article</a> and <a target="_blank" rel="noopener" href="http://blog.jcole.us/2013/01/10/btree-index-structures-in-innodb/">this article</a> from a core developer of MySQL. They both focus on how innoDB (the engine of MySQL) handles indexes.</p>
<p>Note: I was told by a reader that, because of low-level optimizations, the B+Tree needs to be fully balanced.</p>
<h3 id="Hash-table"><a href="#Hash-table" class="headerlink" title="Hash table"></a>Hash table</h3><p>Our last important data structure is the hash table. It’s very useful when you want to quickly look for values.  Moreover, understanding the hash table will help us later to understand a common database join operation called the <strong>hash join</strong>. This data structure is also used by a database to store some internal stuff (like the <strong>lock table</strong> or the <strong>buffer pool</strong>, we’ll see both concepts later)</p>
<p>The hash table is a data structure that quickly finds an element with its key. To build a hash table you need to define:</p>
<ul>
<li><strong>a key</strong> for your elements</li>
<li><strong>a hash function</strong> for the keys. The computed hashes of the keys give the locations of the elements (called <strong>buckets</strong>).</li>
<li><strong>a function to compare the keys</strong>. Once you found the right bucket you have to find the element you’re looking for inside the bucket using this comparison.</li>
</ul>
<h4 id="A-simple-example"><a href="#A-simple-example" class="headerlink" title="A simple example"></a>A simple example</h4><p>Let’s have a visual example:</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/hash_table.png"><img src="../../public/images/hash_table-20200207203251356.png" alt="hash table"></a></p>
<p>This hash table has 10 buckets. Since I’m lazy I only drew 5 buckets but I know you’re smart so I let you imagine the 5 others. The Hash function I used is the modulo 10 of the key. In other words I only keep the last digit of the key of an element to find its bucket:</p>
<ul>
<li>if the last digit is 0 the element ends up in the bucket 0,</li>
<li>if the last digit is 1 the element ends up in the bucket 1,</li>
<li>if the last digit is 2 the element ends up in the bucket 2,</li>
<li>…</li>
</ul>
<p>The compare function I used is simply the equality between 2 integers.</p>
<p>Let’s say you want to get the element 78:</p>
<ul>
<li>The hash table computes the hash code for 78 which is 8.</li>
<li>It looks in the bucket 8, and the first element it finds is 78.</li>
<li>It gives you back the element 78</li>
<li><strong>The</strong> <strong>search only costs 2 operations</strong> (1 for computing the hash value and the other for finding the element inside the bucket).</li>
</ul>
<p>Now, let’s say you want to get the element 59:</p>
<ul>
<li>The hash table computes the hash code for 59 which is 9.</li>
<li>It looks in the bucket 9, and the first element it finds is 99. Since 99!=59, element 99 is not the right element.</li>
<li>Using the same logic, it looks at the second element (9), the third (79), … , and the last (29).</li>
<li>The element doesn’t exist.</li>
<li><strong>The search costs 7 operations</strong>.</li>
</ul>
<h4 id="A-good-hash-function"><a href="#A-good-hash-function" class="headerlink" title="A good hash function"></a>A good hash function</h4><p>As you can see, depending on the value you’re looking for, the cost is not the same!</p>
<p>If I now change the hash function with the modulo 1 000 000 of the key (i.e. taking the last 6 digits), the second search only costs 1 operation because there are no elements in the bucket 000059. <strong>The real challenge is to find a good hash function that will create buckets that contain a very small amount of elements</strong>.</p>
<p>In my example, finding a good hash function is easy. But this is a simple example, finding a good hash function is more difficult when the key is:</p>
<ul>
<li>a string (for example the last name of a person)</li>
<li>2 strings (for example the last name and the first name of a person)</li>
<li>2 strings and a date (for example the last name, the first name and the birth date of a person)</li>
<li>…</li>
</ul>
<p><strong>With a good hash function,</strong> <strong>the search in a hash table is in O(1)</strong>.</p>
<h4 id="Array-vs-hash-table"><a href="#Array-vs-hash-table" class="headerlink" title="Array vs hash table"></a>Array vs hash table</h4><p>Why not using an array?</p>
<p>Hum, you’re asking a good question.</p>
<ul>
<li>A hash table can be <strong>half loaded in memory</strong> and the other buckets can stay on disk.</li>
<li>With an array you have to use a contiguous space in memory. If you’re loading a large table it’s <strong>very difficult to have enough contiguous space</strong>.</li>
<li>With a hash table you can <strong>choose the key you want</strong> (for example the country AND the last name of a person).</li>
</ul>
<p>For more information, you can read my article on the <a target="_blank" rel="noopener" href="http://coding-geek.com/how-does-a-hashmap-work-in-java/">Java HashMap</a> which is an efficient hash table implementation; you don’t need to understand Java to understand the concepts inside this article.</p>
<h1 id="Global-overview"><a href="#Global-overview" class="headerlink" title="Global overview"></a>Global overview</h1><p>We’ve just seen the basic components inside a database. We now need to step back to see the big picture.</p>
<p>A database is a collection of information that can easily be accessed and modified. But a simple bunch of files could do the same. In fact, the simplest databases like SQLite are nothing more than a bunch of files. But SQLite is a well-crafted bunch of files because it allows you to:</p>
<ul>
<li>use transactions that ensure data are safe and coherent</li>
<li>quickly process data even when you’re dealing with millions of data</li>
</ul>
<p>More generally, a database can be seen as the following figure:</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/global_overview.png"><img src="../../public/images/global_overview-20200207203251364.png" alt="global overview of a database"></a></p>
<p>Before writing this part, I’ve read multiple books/papers and every source had its on way to represent a database. So, don’t focus too much on how I organized this database or how I named the processes because I made some choices to fit the plan of this article. What matters are the different components; the overall idea is that <strong>a database is divided into multiple components that interact with each other</strong>.</p>
<p>The core components:</p>
<ul>
<li><strong>The process manager</strong>: Many databases have a <strong>pool of processes/threads</strong> that needs to be managed. Moreover, in order to gain nanoseconds, some modern databases use their own threads instead of the Operating System threads.</li>
<li><strong>The network manager</strong>: Network I/O is a big issue, especially for distributed databases. That’s why some databases have their own manager.</li>
<li><strong>File system manager</strong>: <strong>Disk I/O is the first bottleneck of a database</strong>. Having a manager that will perfectly handle the Operating System file system or even replace it is important.</li>
<li><strong>The memory manager</strong>: To avoid the disk I/O penalty a large quantity of ram is required. But if you handle a large amount of memory, you need an efficient memory manager. Especially when you have many queries using memory at the same time.</li>
<li><strong>Security Manager</strong>: for managing the authentication and the authorizations of the users</li>
<li><strong>Client manager</strong>: for managing the client connections</li>
<li>…</li>
</ul>
<p>The tools:</p>
<ul>
<li><strong>Backup manager</strong>: for saving and restoring a database.</li>
<li><strong>Recovery manager</strong>: for restarting the database in a <strong>coherent state</strong> after a crash</li>
<li><strong>Monitor manager</strong>: for logging the activity of the database and providing tools to monitor a database</li>
<li><strong>Administration manager</strong>: for storing metadata (like the names and the structures of the tables) and providing tools to manage databases, schemas, tablespaces, …</li>
<li>…</li>
</ul>
<p>The query Manager:</p>
<ul>
<li><strong>Query parser</strong>: to check if a query is valid</li>
<li><strong>Query rewriter</strong>: to pre-optimize a query</li>
<li><strong>Query optimizer</strong>: to optimize a query</li>
<li><strong>Query executor</strong>: to compile and execute a query</li>
</ul>
<p>The data manager:</p>
<ul>
<li><strong>Transaction manager</strong>: to handle transactions</li>
<li><strong>Cache manager</strong>: to put data in memory before using them and put data in memory before writing them on disk</li>
<li><strong>Data access manager</strong>: to access data on disk</li>
</ul>
<p>For the rest of this article, I’ll focus on how a database manages an SQL query through the following processes:</p>
<ul>
<li>the client manager</li>
<li>the query manager</li>
<li>the data manager (I’ll also include the recovery manager in this part)</li>
</ul>
<h1 id="Client-manager"><a href="#Client-manager" class="headerlink" title="Client manager"></a>Client manager</h1><p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/client_manager.png"><img src="../../public/images/client_manager-20200207203251358.png" alt="client manager in databases"></a></p>
<p>The client manager is the part that handles the communications with the client. The client can be a (web) server or an end-user/end-application. The client manager provides different ways to access the database through a set of well-known APIs: JDBC, ODBC, OLE-DB …</p>
<p>It can also provide proprietary database access APIs.</p>
<p>When you connect to a database:</p>
<ul>
<li>The manager first checks your <strong>authentication</strong> (your login and password) and then checks if you have the <strong>authorizations</strong> to use the database. These access rights are set by your DBA.</li>
<li>Then, it checks if there is a process (or a thread) available to manage your query.</li>
<li>It also checks if the database if not under heavy load.</li>
<li>It can wait a moment to get the required resources. If this wait reaches a timeout, it closes the connection and gives a readable error message.</li>
<li>Then it <strong>sends your query to the query manager</strong> and your query is processed</li>
<li>Since the query processing is not an “all or nothing” thing, as soon as it gets data from the query manager, it <strong>stores</strong> <strong>the partial results in a buffer and start sending</strong> them to you.</li>
<li>In case of problem, it stops the connection, gives you a <strong>readable explanation</strong> and releases the resources.</li>
</ul>
<h1 id="Query-manager"><a href="#Query-manager" class="headerlink" title="Query manager"></a>Query manager</h1><p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/query_manager.png"><img src="../../public/images/query_manager-20200207203251359.png" alt="query manager in databases"></a></p>
<p><strong>This part is where the power of a database lies</strong>. During this part, an ill-written query is transformed into a <strong>fast</strong> executable code. The code is then executed and the results are returned to the client manager. It’s a multiple-step operation:</p>
<ul>
<li>the query is first <strong>parsed</strong> to see if it’s valid</li>
<li>it’s then <strong>rewritten</strong> to remove useless operations and add some pre-optimizations</li>
<li>it’s then <strong>optimized</strong> to improve the performances and transformed into an execution and data access plan.</li>
<li>then the plan is <strong>compiled</strong></li>
<li>at last, it’s <strong>executed</strong></li>
</ul>
<p>In this part, I won’t talk a lot about the last 2 points because they’re less important.</p>
<p>After reading this part, if you want a better understanding I recommend reading:</p>
<ul>
<li>The initial research paper (1979) on cost based optimization: <a target="_blank" rel="noopener" href="http://www.cs.berkeley.edu/~brewer/cs262/3-selinger79.pdf">Access Path Selection in a Relational Database Management System</a>. This article is only 12 pages and understandable with an average level in computer science.</li>
<li>A very good and in-depth presentation on how DB2 9.X optimizes queries <a target="_blank" rel="noopener" href="http://infolab.stanford.edu/~hyunjung/cs346/db2-talk.pdf">here</a></li>
<li>A very good presentation on how PostgreSQL optimizes queries <a target="_blank" rel="noopener" href="http://momjian.us/main/writings/pgsql/optimizer.pdf">here</a>. It’s the most accessible document since it’s more a presentation on “let’s see what query plans PostgreSQL gives in these situations“ than a “let’s see the algorithms used by PostgreSQL”.</li>
<li>The official <a target="_blank" rel="noopener" href="https://www.sqlite.org/optoverview.html">SQLite documentation</a> about optimization. It’s “easy” to read because SQLite uses simple rules. Moreover, it’s the only official documentation that really explains how it works.</li>
<li>A good presentation on how SQL Server 2005 optimizes queries <a target="_blank" rel="noopener" href="https://blogs.msdn.com/cfs-filesystemfile.ashx/__key/communityserver-components-postattachments/00-08-50-84-93/QPTalk.pdf">here</a></li>
<li>A white paper about optimization in Oracle 12c <a target="_blank" rel="noopener" href="http://www.oracle.com/technetwork/database/bi-datawarehousing/twp-optimizer-with-oracledb-12c-1963236.pdf">here</a></li>
<li>2 theoretical courses on query optimization from the authors of the book “<em>DATABASE SYSTEM CONCEPTS”</em> <a target="_blank" rel="noopener" href="http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch12.ppt">here</a> and t<a target="_blank" rel="noopener" href="http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch13.ppt">here</a>. A good read that focuses on disk I/O cost but a good level in CS is required.</li>
<li>Another <a target="_blank" rel="noopener" href="https://www.informatik.hu-berlin.de/de/forschung/gebiete/wbi/teaching/archive/sose05/dbs2/slides/09_joins.pdf">theoretical course</a> that I find more accessible but that only focuses on join operators and disk I/O.</li>
</ul>
<h2 id="Query-parser"><a href="#Query-parser" class="headerlink" title="Query parser"></a>Query parser</h2><p>Each SQL statement is sent to the parser where it is checked for correct syntax. If you made a mistake in your query the parser will reject the query. For example, if you wrote “SLECT …” instead of “SELECT …”,  the story ends here.</p>
<p>But this goes deeper. It also checks that the keywords are used in the right order. For example a WHERE before a SELECT will be rejected.</p>
<p>Then, the tables and the fields inside the query are analyzed. The parser uses the metadata of the database to check:</p>
<ul>
<li>If the <strong>tables exist</strong></li>
<li>If the <strong>fields</strong> of the tables exist</li>
<li>If the <strong>operations</strong> for the types of the fields <strong>are possible</strong> (for example you can’t compare an integer with a string, you can’t use a substring() function on an integer)</li>
</ul>
<p>Then it checks if you have the <strong>authorizations</strong> to read (or write) the tables in the query. Again, these access rights on tables are set by your DBA.</p>
<p>During this parsing, the SQL query is transformed into an internal representation (often a tree)</p>
<p>If everything is ok then the internal representation is sent to the query rewriter.</p>
<h2 id="Query-rewriter"><a href="#Query-rewriter" class="headerlink" title="Query rewriter"></a>Query rewriter</h2><p>At this step, we have an internal representation of a query. The aim of the rewriter is:</p>
<ul>
<li>to pre-optimize the query</li>
<li>to avoid unnecessary operations</li>
<li>to help the optimizer to find the best possible solution</li>
</ul>
<p>The rewriter executes a list of known rules on the query. If the query fits a pattern of a rule, the rule is applied and the query is rewritten. Here is a non-exhaustive list of (optional) rules:</p>
<ul>
<li><strong>View merging:</strong> If you’re using a view in your query, the view is transformed with the SQL code of the view.</li>
<li><strong>Subquery flattening</strong>: Having subqueries is very difficult to optimize so the rewriter will try to modify a query with a subquery to remove the subquery.</li>
</ul>
<p>For example</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;PERSON.*&#96;&#96;FROM&#96; &#96;PERSON&#96;&#96;WHERE&#96; &#96;PERSON.person_key &#96;&#96;IN&#96;&#96;(&#96;&#96;SELECT&#96; &#96;MAILS.person_key&#96;&#96;FROM&#96; &#96;MAILS&#96;&#96;WHERE&#96; &#96;MAILS.mail &#96;&#96;LIKE&#96; &#96;&#39;christophe%&#39;&#96;&#96;);</span><br></pre></td></tr></table></figure>

<p>Will be replaced by</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;PERSON.*&#96;&#96;FROM&#96; &#96;PERSON, MAILS&#96;&#96;WHERE&#96; &#96;PERSON.person_key &#x3D; MAILS.person_key&#96;&#96;and&#96; &#96;MAILS.mail &#96;&#96;LIKE&#96; &#96;&#39;christophe%&#39;&#96;&#96;;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong>Removal of unnecessary operators</strong>: For example if you use a DISTINCT whereas you have a UNIQUE constraint that prevents the data from being non-unique, the DISTINCT keyword is removed.</li>
<li><strong>Redundant join elimination:</strong> If you have twice the same join condition because one join condition is hidden in a view or if by transitivity there is a useless join, it’s removed.</li>
<li><strong>Constant arithmetic evaluation:</strong> If you write something that requires a calculus, then it’s computed once during the rewriting. For example WHERE AGE &gt; 10+2 is transformed into WHERE AGE &gt; 12 and TODATE(“some date”) is transformed into the date in the datetime format</li>
<li><strong>(**</strong>Advanced) Partition Pruning:** If you’re using a partitioned table, the rewriter is able to find what partitions to use.</li>
<li><strong>(Advanced) Materialized view rewrite</strong>: If you have a materialized view that matches a subset of the predicates in your query, the rewriter checks if the view is up to date and modifies the query to use the materialized view instead of the raw tables.</li>
<li><strong>(Advanced) Custom rules:</strong> If you have custom rules to modify a query (like Oracle policies), then the rewriter executes these rules</li>
<li><strong>(Advanced) Olap transformations</strong>: analytical/windowing functions, star joins, rollup … are also transformed (but I’m not sure if it’s done by the rewriter or the optimizer, since both processes are very close it must depends on the database).</li>
</ul>
<p>This rewritten query is then sent to the query optimizer where the fun begins!</p>
<h2 id="Statistics"><a href="#Statistics" class="headerlink" title="Statistics"></a>Statistics</h2><p>Before we see how a database optimizes a query we need to speak about <strong>statistics</strong> because <strong>without them</strong> <strong>a database is stupid</strong>. If you don’t tell the database to analyze its own data, it will not do it and it will make (very) bad assumptions.</p>
<p>But what kind of information does a database need?</p>
<p>I have to (briefly) talk about how databases and Operating systems store data. They’re using a minimum unit called <strong>a</strong> <strong>page</strong> or a block (4 or 8 kilobytes by default). This means that if you only need 1 Kbytes it will cost you one page anyway. If the page takes 8 Kbytes then you’ll waste 7 Kbytes.</p>
<p>Back to the statistics! When you ask a database to gather statistics, it computes values like:</p>
<ul>
<li>The number of rows/pages in a table</li>
<li>For each column in a table:<ul>
<li>distinct data values</li>
<li>the length of data values (min, max, average)</li>
<li>data range information (min, max, average)</li>
</ul>
</li>
<li>Information on the indexes of the table.</li>
</ul>
<p><strong>These statistics will help the optimizer to estimate the</strong> <strong>disk I/O, CPU and memory usages of the query.</strong></p>
<p>The statistics for each column are very important. For example if a table PERSON needs to be joined on 2 columns: LAST_NAME, FIRST_NAME. With the statistics, the database knows that there are only 1 000 different values on FIRST_NAME and 1 000 000 different values on LAST_NAME. Therefore, the database will join the data on LAST_NAME, FIRST_NAME instead of FIRST_NAME,LAST_NAME because it produces way less comparisons since the LAST_NAME are unlikely to be the same so most of the time a comparison on the 2 (or 3) first characters of the LAST_NAME is enough.</p>
<p>But these are basic statistics. You can ask a database to compute advanced statistics called <strong>histograms</strong>. Histograms are statistics that inform about the distribution of the values inside the columns. For example</p>
<ul>
<li>the most frequent values</li>
<li>the quantiles</li>
<li>…</li>
</ul>
<p>These extra statistics will help the database to find an even better query plan. Especially for equality predicate (ex: WHERE AGE = 18 ) or range predicates (ex: WHERE AGE &gt; 10 and AGE &lt;40 ) because the database will have a better idea of the number rows concerned by these predicates (note: the technical word for this concept is selectivity).</p>
<p>The statistics are stored in the metadata of the database. For example you can see the statistics for the (non-partitioned) tables:</p>
<ul>
<li>in USER/ALL/DBA_TABLES and USER/ALL/DBA_TAB_COLUMNS for Oracle</li>
<li>in SYSCAT.<em>TABLES</em> and <em>SYSCAT.COLUMNS for DB2</em>.</li>
</ul>
<p>The <strong>statistics have to be up to date</strong>. There is nothing worse than a database thinking a table has only 500 rows whereas it has 1 000 000 rows. The only drawback of the statistics is that <strong>it takes time to compute them</strong>. This is why they’re not automatically computed by default in most databases. It becomes difficult with millions of data to compute them. In this case, you can choose to compute only the basics statistics or to compute the stats on a sample of the database.</p>
<p>For example, when I was working on a project dealing with hundreds of millions rows in each tables, I chose to compute the statistics on only 10%, which led to a huge gain in time. For the story it turned out to be a bad decision because occasionally the 10% chosen by Oracle 10G for a specific column of a specific table were very different from the overall 100% (which is very unlikely to happen for a table with 100M rows). This wrong statistic led to a query taking occasionally 8 hours instead of 30 seconds; a nightmare to find the root cause. This example shows how important the statistics are.</p>
<p>Note: Of course, there are more advanced statistics specific for each database. If you want to know more, read the documentations of the databases. That being said, I’ve tried to understand how the statistics are used and the best official documentation I found was the <a target="_blank" rel="noopener" href="http://www.postgresql.org/docs/9.4/static/row-estimation-examples.html">one from PostgreSQL</a>.</p>
<h2 id="Query-optimizer"><a href="#Query-optimizer" class="headerlink" title="Query optimizer"></a>Query optimizer</h2><p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/15-McDonalds_CBO.jpg"><img src="../../public/images/15-McDonalds_CBO-20200207203251382.jpg" alt="CBO"></a></p>
<p>All modern databases are using a <strong>Cost Based Optimization</strong> (or <strong>CBO</strong>) to optimize queries. The idea is to put a cost an every operation and find the best way to reduce the cost of the query by using the cheapest chain of operations to get the result.</p>
<p>To understand how a cost optimizer works I think it’s good to have an example to “feel” the complexity behind this task. In this part I’ll present you the 3 common ways to join 2 tables and we will quickly see that even a simple join query is a nightmare to optimize. After that, we’ll see how real optimizers do this job.</p>
<p>For these joins, I’ll focus on their time complexity but <strong>a</strong> <strong>database optimizer computes</strong> their <strong>CPU cost, disk I/O cost and memory requirement</strong>. The difference between time complexity and CPU cost is that time cost is very approximate (it’s for lazy guys like me). For the CPU cost, I should count every operation like an addition, an “if statement”, a multiplication, an iteration … Moreover:</p>
<ul>
<li>Each high level code operation has a specific number of low level CPU operations.</li>
<li>The cost of a CPU operation is not the same (in terms of CPU cycles) whether you’re using an Intel Core i7, an Intel Pentium 4, an AMD Opteron…. In other words it depends on the CPU architecture.</li>
</ul>
<p>Using the time complexity is easier (at least for me) and with it we can still get the concept of CBO. I’ll sometimes speak about disk I/O since it’s an important concept. Keep in mind that <strong>the bottleneck is most of the time the disk I/O and not the CPU usage</strong>.</p>
<h3 id="Indexes"><a href="#Indexes" class="headerlink" title="Indexes"></a>Indexes</h3><p>We talked about indexes when we saw the B+Trees. Just remember that these <strong>indexes are already sorted</strong>.</p>
<p>FYI, there are other types of indexes like <strong>bitmap indexes</strong>. They don’t offer the same cost in terms of CPU, disk I/O and memory than B+Tree indexes.</p>
<p>Moreover, many modern databases can <strong>dynamically create temporary indexes</strong> just for the current query if it can improve the cost of the execution plan.</p>
<h3 id="Access-Path"><a href="#Access-Path" class="headerlink" title="Access Path"></a>Access Path</h3><p>Before applying your join operators, you first need to get your data. Here is how you can get your data.</p>
<p>Note: Since the real problem with all the access paths is the disk I/O, I won’t talk a lot about time complexity.</p>
<h4 id="Full-scan"><a href="#Full-scan" class="headerlink" title="Full scan"></a>Full scan</h4><p>If you’ve ever read an execution plan you must have seen the word <strong>full scan</strong> (or just scan). A full scan is simply the database reading a table or an index entirely. <strong>In terms of disk I/O, a table full scan is obviously more expensive than an index full scan</strong>.</p>
<h4 id="Range-Scan"><a href="#Range-Scan" class="headerlink" title="Range Scan"></a>Range Scan</h4><p>There are other types of scan like <strong>index range scan</strong>. It is used for example when you use a predicate like “WHERE AGE &gt; 20 AND AGE &lt;40”.</p>
<p>Of course you need have an index on the field AGE to use this index range scan.</p>
<p>We already saw in the first part that the time cost of a range query is something like log(N) +M, where N is the number of data in this index and M an estimation of the number of rows inside this range. <strong>Both N and M values are known thanks to the statistics</strong> (Note: M is the selectivity for the predicate AGE &gt;20 AND AGE&lt;40). Moreover, for a range scan you don’t need to read the full index so it’s <strong>less expensive in terms of disk I/O than a full scan</strong>.</p>
<h4 id="Unique-scan"><a href="#Unique-scan" class="headerlink" title="Unique scan"></a>Unique scan</h4><p>If you only need one value from an index you can use the <strong>unique scan</strong>.</p>
<h4 id="Access-by-row-id"><a href="#Access-by-row-id" class="headerlink" title="Access by row id"></a>Access by row id</h4><p>Most of the time, if the database uses an index, it will have to look for the rows associated to the index. To do so it will use an access by row id.</p>
<p>For example, if you do something like</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;LASTNAME, FIRSTNAME &#96;&#96;from&#96; &#96;PERSON &#96;&#96;WHERE&#96; &#96;AGE &#x3D; 28</span><br></pre></td></tr></table></figure>

<p>If you have an index for person on column age, the optimizer will use the index to find all the persons who are 28 then it will ask for the associate rows in the table because the index only has information about the age and you want to know the lastname and the firstname.</p>
<p>But, if now you do something like</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;TYPE_PERSON.CATEGORY &#96;&#96;from&#96; &#96;PERSON ,TYPE_PERSON&#96;&#96;WHERE&#96; &#96;PERSON.AGE &#x3D; TYPE_PERSON.AGE</span><br></pre></td></tr></table></figure>

<p>The index on PERSON will be used to join with TYPE_PERSON but the table PERSON will not be accessed by row id since you’re not asking information on this table.</p>
<p>Though it works great for a few accesses, the real issue with this operation is the disk I/O. If you need too many accesses by row id the database might choose a full scan.</p>
<h4 id="Others-paths"><a href="#Others-paths" class="headerlink" title="Others paths"></a>Others paths</h4><p>I didn’t present all the access paths. If you want to know more, you can read the <a target="_blank" rel="noopener" href="https://docs.oracle.com/database/121/TGSQL/tgsql_optop.htm">Oracle documentation</a>. The names might not be the same for the other databases but the concepts behind are the same.</p>
<h3 id="Join-operators"><a href="#Join-operators" class="headerlink" title="Join operators"></a>Join operators</h3><p>So, we know how to get our data, let’s join them!</p>
<p>I’ll present the 3 common join operators: Merge Join, Hash Join and Nested Loop Join. But before that, I need to introduce new vocabulary: <strong>inner relation</strong> and <strong>outer relation</strong>. A relation can be:</p>
<ul>
<li>a table</li>
<li>an index</li>
<li>an intermediate result from a previous operation (for example the result of a previous join)</li>
</ul>
<p>When you’re joining two relations, the join algorithms manage the two relations differently. In the rest of the article, I’ll assume that:</p>
<ul>
<li>the outer relation is the left data set</li>
<li>the inner relation is the right data set</li>
</ul>
<p>For example, A JOIN B is the join between A and B where A is the outer relation and B the inner relation.</p>
<p>Most of the time, <strong>the cost of A JOIN B is not the same as the cost of B JOIN A.</strong></p>
<p><strong>In this part, I’ll also assume that the outer relation has N elements</strong> <strong>and the inner relation M elements</strong>. Keep in mind that a real optimizer knows the values of N and M with the statistics.</p>
<p>Note: N and M are the cardinalities of the relations.</p>
<h4 id="Nested-loop-join"><a href="#Nested-loop-join" class="headerlink" title="Nested loop join"></a>Nested loop join</h4><p>The nested loop join is the easiest one.</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/nested_loop_join.png"><img src="../../public/images/nested_loop_join-20200207203251391.png" alt="nested loop join in databases"></a></p>
<p>Here is the idea:</p>
<ul>
<li>for each row in the outer relation</li>
<li>you look at all the rows in the inner relation to see if there are rows that match</li>
</ul>
<p>Here is a pseudo code:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nested_loop_join(array outer, array inner)&#96;&#96; &#96;&#96;for&#96; &#96;each row a in outer&#96;&#96;  &#96;&#96;for&#96; &#96;each row b in inner&#96;&#96;   &#96;&#96;if&#96; &#96;(match_join_condition(a,b))&#96;&#96;    &#96;&#96;write_result_in_output(a,b)&#96;&#96;   &#96;&#96;end &#96;&#96;if&#96;&#96;  &#96;&#96;end &#96;&#96;for&#96;&#96;  &#96;&#96;end &#96;&#96;for</span><br></pre></td></tr></table></figure>

<p>Since it’s a double iteration, the <strong>time complexity is O(N*M)</strong></p>
<p>In term of disk I/O, for each of the N rows in the outer relation, the inner loop needs to read M rows from the inner relation. This algorithm needs to read N + N<em>M rows from disk. But, if the inner relation is small enough, you can put the relation in memory and just have M +N reads. With this modification, *</em>the inner relation must be the smallest one** since it has more chance to fit in memory.</p>
<p>In terms of time complexity it makes no difference but in terms of disk I/O it’s way better to read only once both relations.   </p>
<p>Of course, the inner relation can be replaced by an index, it will be better for the disk I/O.</p>
<p>Since this algorithm is very simple, here is another version that is more disk I/O friendly if the inner relation is too big to fit in memory. Here is the idea:</p>
<ul>
<li>instead of reading both relation row by row,</li>
<li>you read them bunch by bunch and keep 2 bunches of rows (from each relation) in memory,</li>
<li>you compare the rows inside the two bunches and keep the rows that match,</li>
<li>then you load new bunches from disk and compare them</li>
<li>and so on until there are no bunches to load.</li>
</ul>
<p>Here is a possible algorithm:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; improved version to reduce the disk I&#x2F;O.&#96;&#96;nested_loop_join_v2(file outer, file inner)&#96;&#96; &#96;&#96;for&#96; &#96;each bunch ba in outer&#96;&#96; &#96;&#96;&#x2F;&#x2F; ba is now in memory&#96;&#96;  &#96;&#96;for&#96; &#96;each bunch bb in inner&#96;&#96;    &#96;&#96;&#x2F;&#x2F; bb is now in memory&#96;&#96;    &#96;&#96;for&#96; &#96;each row a in ba&#96;&#96;     &#96;&#96;for&#96; &#96;each row b in bb&#96;&#96;      &#96;&#96;if&#96; &#96;(match_join_condition(a,b))&#96;&#96;       &#96;&#96;write_result_in_output(a,b)&#96;&#96;      &#96;&#96;end &#96;&#96;if&#96;&#96;     &#96;&#96;end &#96;&#96;for&#96;&#96;    &#96;&#96;end &#96;&#96;for&#96;&#96;  &#96;&#96;end &#96;&#96;for&#96;&#96;  &#96;&#96;end &#96;&#96;for</span><br></pre></td></tr></table></figure>



<p><strong>With this version, the time complexity remains the same, but the number of disk access decreases</strong>:</p>
<ul>
<li>With the previous version, the algorithm needs N + N*M accesses (each access gets one row).</li>
<li>With this new version, the number of disk accesses becomes number_of_bunches_for(outer)+ number_of_ bunches_for(outer)* number_of_ bunches_for(inner).</li>
<li>If you increase the size of the bunch you reduce the number of disk accesses.</li>
</ul>
<p>Note: Each disk access gathers more data than the previous algorithm but it doesn’t matter since they’re sequential accesses (the real issue with mechanical disks is the time to get the first data).</p>
<h4 id="Hash-join"><a href="#Hash-join" class="headerlink" title="Hash join"></a>Hash join</h4><p>The hash join is more complicated but gives a better cost than a nested loop join in many situations.</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/hash_join.png"><img src="../../public/images/hash_join-20200207203251391.png" alt="hash join in a database"></a></p>
<p>The idea of the hash join is to:</p>
<ul>
<li>1) Get all elements from the inner relation</li>
<li>2) Build an in-memory hash table</li>
<li>3) Get all elements of the outer relation one by one</li>
<li>4) Compute the hash of each element (with the hash function of the hash table) to find the associated bucket of the inner relation</li>
<li>5) find if there is a match between the elements in the bucket and the element of the outer table</li>
</ul>
<p>In terms of time complexity I need to make some assumptions to simplify the problem:</p>
<ul>
<li>The inner relation is divided into X buckets</li>
<li>The hash function distributes hash values almost uniformly for both relations. In other words the buckets are equally sized.</li>
<li>The matching between an element of the outer relation and all elements inside a bucket costs the number of elements inside the buckets.</li>
</ul>
<p>The time complexity is (M/X) * N + cost_to_create_hash_table(M) + cost_of_hash_function*N</p>
<p>If the Hash function creates enough small-sized buckets then <strong>the time complexity is O(M+N)</strong></p>
<p>Here is another version of the hash join which is more memory friendly but less disk I/O friendly. This time:</p>
<ul>
<li>1) you compute the hash tables for both the inner and outer relations</li>
<li>2) then you put them on disk</li>
<li>3) then you compare the 2 relations bucket by bucket (with one loaded in-memory and the other read row by row)</li>
</ul>
<h4 id="Merge-join"><a href="#Merge-join" class="headerlink" title="Merge join"></a>Merge join</h4><p><strong>The merge join is the only join that produces a sorted result.</strong></p>
<p>Note: In this simplified merge join, there are no inner or outer tables; they both play the same role. But real implementations make a difference, for example, when dealing with duplicates.</p>
<p>The merge join can be divided into of two steps:</p>
<ol>
<li>(Optional) Sort join operations: Both the inputs are sorted on the join key(s).</li>
<li>Merge join operation: The sorted inputs are merged together.</li>
</ol>
<p>Sort</p>
<p>We already spoke about the merge sort, in this case a merge sort in a good algorithm (but not the best if memory is not an issue).</p>
<p>But sometimes the data sets are already sorted, for example:</p>
<ul>
<li>If the table is natively ordered, for example an index-organized table on the join condition</li>
<li>If the relation is an index on the join condition</li>
<li>If this join is applied on an intermediate result already sorted during the process of the query</li>
</ul>
<p>Merge join</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/merge_join.png"><img src="../../public/images/merge_join-20200207203251391.png" alt="merge join in a database"></a></p>
<p>This part is very similar to the merge operation of the merge sort we saw. But this time, instead of picking every element from both relations, we only pick the elements from both relations that are equals. Here is the idea:</p>
<ul>
<li>1) you compare both current elements in the 2 relations (current=first for the first time)</li>
<li>2) if they’re equal, then you put both elements in the result and you go to the next element for both relations</li>
<li>3) if not, you go to the next element for the relation with the lowest element (because the next element might match)</li>
<li>4) and repeat 1,2,3 until you reach the last element of one of the relation.</li>
</ul>
<p>This works because both relations are sorted and therefore you don’t need to “go back” in these relations.</p>
<p>This algorithm is a simplified version because it doesn’t handle the case where the same data appears multiple times in both arrays (in other words a multiple matches). The real version is more complicated “just” for this case; this is why I chose a simplified version.</p>
<p>If both relations are already sorted then <strong>the time complexity is O(N+M)</strong></p>
<p>If both relations need to be sorted then the time complexity is the cost to sort both relations: <strong>O(N*Log(N) + M*Log(M))</strong></p>
<p>For the CS geeks, here is a possible algorithm that handles the multiple matches (note: I’m not 100% sure about my algorithm):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mergeJoin(relation a, relation b)&#96;&#96; &#96;&#96;relation output&#96;&#96; &#96;&#96;integer a_key:&#x3D;&#96;&#96;0&#96;&#96;;&#96;&#96; &#96;&#96;integer b_key:&#x3D;&#96;&#96;0&#96;&#96;;&#96;&#96; &#96; &#96; &#96;&#96;while&#96; &#96;(a[a_key]!&#x3D;&#96;&#96;null&#96; &#96;or b[b_key]!&#x3D;&#96;&#96;null&#96;&#96;)&#96;&#96;  &#96;&#96;if&#96; &#96;(a[a_key] &lt; b[b_key])&#96;&#96;   &#96;&#96;a_key++;&#96;&#96;  &#96;&#96;else&#96; &#96;if&#96; &#96;(a[a_key] &gt; b[b_key])&#96;&#96;   &#96;&#96;b_key++;&#96;&#96;  &#96;&#96;else&#96; &#96;&#x2F;&#x2F;Join predicate satisfied&#96;&#96;  &#96;&#96;&#x2F;&#x2F;i.e. a[a_key] &#x3D;&#x3D; b[b_key]&#96; &#96;   &#96;&#96;&#x2F;&#x2F;count the number of duplicates in relation a&#96;&#96;   &#96;&#96;integer nb_dup_in_a &#x3D; &#96;&#96;1&#96;&#96;:&#96;&#96;   &#96;&#96;while&#96; &#96;(a[a_key]&#x3D;&#x3D;a[a_key+nb_dup_in_a])&#96;&#96;    &#96;&#96;nb_dup_in_a++;&#96;&#96;    &#96; &#96;   &#96;&#96;&#x2F;&#x2F;count the number of duplicates in relation b&#96;&#96;   &#96;&#96;integer dup_in_b &#x3D; &#96;&#96;1&#96;&#96;:&#96;&#96;   &#96;&#96;while&#96; &#96;(b[b_key]&#x3D;&#x3D;b[b_key+nb_dup_in_b])&#96;&#96;    &#96;&#96;nb_dup_in_b++;&#96;&#96;    &#96; &#96;   &#96;&#96;&#x2F;&#x2F;write the duplicates in output&#96;&#96;    &#96;&#96;for&#96; &#96;(&#96;&#96;int&#96; &#96;i &#x3D; &#96;&#96;0&#96; &#96;; i&lt; nb_dup_in_a ; i++)&#96;&#96;     &#96;&#96;for&#96; &#96;(&#96;&#96;int&#96; &#96;j &#x3D; &#96;&#96;0&#96; &#96;; i&lt; nb_dup_in_b ; i++)   &#96;&#96;      &#96;&#96;write_result_in_output(a[a_key+i],b[b_key+j])&#96;&#96;      &#96; &#96;   &#96;&#96;a_key&#x3D;a_key + nb_dup_in_a-&#96;&#96;1&#96;&#96;;&#96;&#96;   &#96;&#96;b_key&#x3D;b_key + nb_dup_in_b-&#96;&#96;1&#96;&#96;;&#96; &#96;  &#96;&#96;end &#96;&#96;if&#96;&#96; &#96;&#96;end &#96;&#96;while</span><br></pre></td></tr></table></figure>



<h4 id="Which-one-is-the-best"><a href="#Which-one-is-the-best" class="headerlink" title="Which one is the best?"></a>Which one is the best?</h4><p>If there was a best type of joins, there wouldn’t be multiple types. This question is very difficult because many factors come into play like:</p>
<ul>
<li>The <strong>amount of free memory</strong>: without enough memory you can say goodbye to the powerful hash join (at least the full in-memory hash join)</li>
<li>The <strong>size of the 2 data sets</strong>. For example if you have a big table with a very small one, a nested loop join will be faster than a hash join because the hash join has an expensive creation of hashes. If you have 2 very large tables the nested loop join will be very CPU expensive.</li>
<li>The <strong>presence</strong> <strong>of</strong> <strong>indexes</strong>. With 2 B+Tree indexes the smart choice seems to be the merge join</li>
<li>If <strong>the result need to be sorted</strong>: Even if you’re working with unsorted data sets, you might want to use a costly merge join (with the sorts) because at the end the result will be sorted and you’ll be able to chain the result with another merge join (or maybe because the query asks implicitly/explicitly for a sorted result with an ORDER BY/GROUP BY/DISTINCT operation)</li>
<li>If <strong>the relations are already sorted</strong>: In this case the merge join is the best candidate</li>
<li>The type of joins you’re doing: is it an <strong>equijoin</strong> (i.e.: tableA.col1 = tableB.col2)? Is it an <strong>inner join</strong>, an <strong>outer join,</strong> a <strong>cartesian product</strong> or a <strong>self-join</strong>? Some joins can’t work in certain situations.</li>
<li>The <strong>distribution of data</strong>. If the data on the join condition are <strong>skewed</strong> (For example you’re joining people on their last name but many people have the same), using a hash join will be a disaster because the hash function will create ill-distributed buckets.</li>
<li>If you want the join to be executed by <strong>multiple threads/process</strong></li>
</ul>
<p>For more information, you can read the <a target="_blank" rel="noopener" href="https://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.admin.perf.doc/doc/c0005311.html">DB2</a>, <a target="_blank" rel="noopener" href="http://docs.oracle.com/cd/B28359_01/server.111/b28274/optimops.htm#i76330">ORACLE</a> or <a target="_blank" rel="noopener" href="https://technet.microsoft.com/en-us/library/ms191426(v=sql.105).aspx">SQL Server</a> documentations.</p>
<h3 id="Simplified-example"><a href="#Simplified-example" class="headerlink" title="Simplified example"></a>Simplified example</h3><p>We’ve just seen 3 types of join operations.</p>
<p>Now let’s say we need to join 5 tables to have a full view of a person. A PERSON can have:</p>
<ul>
<li>multiple MOBILES</li>
<li>multiple MAILS</li>
<li>multiple ADRESSES</li>
<li>multiple BANK_ACCOUNTS</li>
</ul>
<p>In other words we need a quick answer for the following query:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">SELECT&#96; &#96;* &#96;&#96;from&#96; &#96;PERSON, MOBILES, MAILS,ADRESSES, BANK_ACCOUNTS&#96;&#96;WHERE&#96;&#96;PERSON.PERSON_ID &#x3D; MOBILES.PERSON_ID&#96;&#96;AND&#96; &#96;PERSON.PERSON_ID &#x3D; MAILS.PERSON_ID&#96;&#96;AND&#96; &#96;PERSON.PERSON_ID &#x3D; ADRESSES.PERSON_ID&#96;&#96;AND&#96; &#96;PERSON.PERSON_ID &#x3D; BANK_ACCOUNTS.PERSON_ID</span><br></pre></td></tr></table></figure>

<p>As a query optimizer, I have to find the best way to process the data. But there are 2 problems:</p>
<ul>
<li>What kind of join should I use for each join?</li>
</ul>
<p>I have 3 possible joins (Hash Join, Merge Join, Nested Join) with the possibility to use 0,1 or 2 indexes (not to mention that there are different types of indexes).</p>
<ul>
<li>What order should I choose to compute the join?</li>
</ul>
<p>For example, the following figure shows different possible plans for only 3 joins on 4 tables</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/join_ordering_problem.png"><img src="../../public/images/join_ordering_problem-20200207203251393.png" alt="join ordering optimization problem in a database"></a></p>
<p>So here are my possibilities:</p>
<ul>
<li>1) I use a brute force approach</li>
</ul>
<p>Using the database statistics, I <strong>compute the cost for every possible plan</strong> and I keep the best one. But there are many possibilities. For a given order of joins, each join has 3 possibilities: HashJoin, MergeJoin, NestedJoin. So, for a given order of joins there are 34 possibilities. The join ordering is a <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Catalan_number">permutation problem on a binary tree</a> and there are (2<em>4)!/(4+1)! possible orders. For this very simplified problem, I end up with 34</em>(2*4)!/(4+1)! possibilities.</p>
<p>In non-geek terms, it means 27 216 possible plans. If I now add the possibility for the merge join to take 0,1 or 2 B+Tree indexes, the number of possible plans becomes 210 000. Did I forget to mention that this query is VERY SIMPLE?</p>
<ul>
<li>2) I cry and quit this job</li>
</ul>
<p>It’s very tempting but you wouldn’t get your result and I need money to pay the bills.</p>
<ul>
<li>3) I only try a few plans and take the one with the lowest cost.</li>
</ul>
<p>Since I’m not superman, I can’t compute the cost of every plan. Instead, I can <strong>arbitrary choose a subset of all the possible plans</strong>, compute their costs and give you the best plan of this subset.</p>
<ul>
<li>4) I apply smart <strong>rules to reduce the number of possible plans</strong>.</li>
</ul>
<p>There are 2 types of rules:</p>
<p>I can use “logical” rules that will remove useless possibilities but they won’t filter a lot of possible plans. For example: “the inner relation of the nested loop join must be the smallest data set”</p>
<p>I accept not finding the best solution and apply more aggressive rules to reduce a lot the number of possibilities. For example “If a relation is small, use a nested loop join and never use a merge join or a hash join”</p>
<p>In this simple example, I end up with many possibilities. But <strong>a real query can have other relational operators</strong> like OUTER JOIN, CROSS JOIN, GROUP BY, ORDER BY, PROJECTION, UNION, INTERSECT, DISTINCT … <strong>which means even more possibilities</strong>.</p>
<p>So, how a database does it?</p>
<h3 id="Dynamic-programming-greedy-algorithm-and-heuristic"><a href="#Dynamic-programming-greedy-algorithm-and-heuristic" class="headerlink" title="Dynamic programming, greedy algorithm and heuristic"></a>Dynamic programming, greedy algorithm and heuristic</h3><p>A relational database tries the multiple approaches I’ve just said. The real job of an optimizer is to find a good solution on a limited amount of time.</p>
<p><strong>Most of the time an optimizer doesn’t find the best solution but a “good” one</strong>.</p>
<p>For small queries, doing a brute force approach is possible. But there is a way to avoid unnecessary computations so that even medium queries can use the brute force approach. This is called dynamic programming.</p>
<h4 id="Dynamic-Programming"><a href="#Dynamic-Programming" class="headerlink" title="Dynamic Programming"></a>Dynamic Programming</h4><p>The idea behind these 2 words is that many executions plan are very similar. If you look at the following plans:</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/overlapping_trees.png"><img src="../../public/images/overlapping_trees-20200207203251396.png" alt="overlapping trees optimization dynamic programming"></a></p>
<p>They share the same (A JOIN B) subtree. So, instead of computing the cost of this subtree in every plan, we can compute it once, save the computed cost and reuse it when we see this subtree again. More formally, we’re facing an overlapping problem. To avoid the extra-computation of the partial results we’re using memoization.</p>
<p>Using this technique, instead of having a (2<em>N)!/(N+1)! time complexity, we “just” have 3N. In our previous example with 4 joins, it means passing from 336 ordering to 81. If you take a bigger *</em>query with 8 joins** (which is not big)<strong>, it means passing from 57 657 600 to 6561</strong>.</p>
<p>For the CS geeks, here is an algorithm I found on the <a target="_blank" rel="noopener" href="http://codex.cs.yale.edu/avi/db-book/db6/slide-dir/PPT-dir/ch13.ppt">formal course I already gave you</a>. I won’t explain this algorithm so read it only if you already know dynamic programming or if you’re good with algorithms (you’ve been warned!):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">procedure findbestplan(S)&#96;&#96;if&#96; &#96;(bestplan[S].cost infinite)&#96;&#96;  &#96;&#96;return&#96; &#96;bestplan[S]&#96;&#96;&#x2F;&#x2F; else bestplan[S] has not been computed earlier, compute it now&#96;&#96;if&#96; &#96;(S contains only &#96;&#96;1&#96; &#96;relation)&#96;&#96;     &#96;&#96;set bestplan[S].plan and bestplan[S].cost based on the best way&#96;&#96;     &#96;&#96;of accessing S &#96;&#96;&#x2F;* Using selections on S and indices on S *&#x2F;&#96;&#96;   &#96;&#96;else&#96; &#96;for&#96; &#96;each non-empty subset S1 of S such that S1 !&#x3D; S&#96;&#96;  &#96;&#96;P1&#x3D; findbestplan(S1)&#96;&#96;  &#96;&#96;P2&#x3D; findbestplan(S - S1)&#96;&#96;  &#96;&#96;A &#x3D; best algorithm &#96;&#96;for&#96; &#96;joining results of P1 and P2&#96;&#96;  &#96;&#96;cost &#x3D; P1.cost + P2.cost + cost of A&#96;&#96;  &#96;&#96;if&#96; &#96;cost &lt; bestplan[S].cost&#96;&#96;    &#96;&#96;bestplan[S].cost &#x3D; cost&#96;&#96;   &#96;&#96;bestplan[S].plan &#x3D; “execute P1.plan; execute P2.plan;&#96;&#96;         &#96;&#96;join results of P1 and P2 using A”&#96;&#96;return&#96; &#96;bestplan[S]</span><br></pre></td></tr></table></figure>



<p>For bigger queries you can still do a dynamic programming approach but with extra rules (or <strong>heuristics</strong>) to remove possibilities:</p>
<ul>
<li>If we analyze only a certain type of plan (for example: the left-deep trees) we end up with n*2n instead of 3n</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/left-deep-tree.png"><img src="../../public/images/left-deep-tree-20200207203251418.png" alt="left deep tree example"></a></p>
<ul>
<li>If we add logical rules to avoid plans for some patterns (like “if a table as an index for the given predicate, don’t try a merge join on the table but only on the index”) it will reduce the number of possibilities without hurting to much the best possible solution.</li>
<li>If we add rules on the flow (like “perform the join operations BEFORE all the other relational operations”) it also reduces a lot of possibilities.</li>
<li>…</li>
</ul>
<h4 id="Greedy-algorithms"><a href="#Greedy-algorithms" class="headerlink" title="Greedy algorithms"></a>Greedy algorithms</h4><p>But for a very big query or to have a very fast answer (but not a very fast query), another type of algorithms is used, the greedy algorithms.</p>
<p>The idea is to follow a rule (or <strong>heuristic</strong>) to build a query plan in an incremental way. With this rule, a greedy algorithm finds the best solution to a problem one step at a time. The algorithm starts the query plan with one JOIN. Then, at each step, the algorithm adds a new JOIN to the query plan using the same rule.</p>
<p>Let’s take a simple example. Let’s say we have a query with 4 joins on 5 tables (A, B, C, D and E). To simplify the problem we just take the nested join as a possible join. Let’s use the rule “use the join with the lowest cost”</p>
<ul>
<li>we arbitrary start on one of the 5 tables (let’s choose A)</li>
<li>we compute the cost of every join with A (A being the inner or outer relation).</li>
<li>we find that A JOIN B gives the lowest cost.</li>
<li>we then compute the cost of every join with the result of A JOIN B (A JOIN B being the inner or outer relation).</li>
<li>we find that (A JOIN B) JOIN C gives the best cost.</li>
<li>we then compute the cost of every join with the result of the (A JOIN B) JOIN C …</li>
<li>….</li>
<li>At the end we find the plan (((A JOIN B) JOIN C) JOIN D) JOIN E)</li>
</ul>
<p>Since we arbitrary started with A, we can apply the same algorithm for B, then C then D then E. We then keep the plan with the lowest cost.</p>
<p>By the way, this algorithm has a name: it’s called the <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm">Nearest neighbor algorithm</a>.</p>
<p>I won’t go into details, but with a good modeling and a sort in N<em>log(N) this problem can <a target="_blank" rel="noopener" href="http://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&cad=rja&uact=8&ved=0CE0QFjAEahUKEwjR8OLUmv3GAhUJuxQKHdU-DAA&url=http%3A%2F%2Fwww.cs.bu.edu%2F~steng%2Fteaching%2FSpring2004%2Flectures%2Flecture3.ppt&ei=hyK3VZGRAYn2UtX9MA&usg=AFQjCNGL41kMNkG5cH">easily be solved</a>. The *</em>cost of this algorithm is in O(N*log(N)) vs O(3N) for the full dynamic programming version**. If you have a big query with 20 joins, it means 26 vs 3 486 784 401, a BIG difference!</p>
<p>The problem with this algorithm is that we assume that finding the best join between 2 tables will give us the best cost if we keep this join and add a new join. But:</p>
<ul>
<li>even if A JOIN B gives the best cost between A, B and C</li>
<li>(A JOIN C) JOIN B might give a better result than (A JOIN B) JOIN C.</li>
</ul>
<p>To improve the result, you can run multiple greedy algorithms using different rules and keep the best plan.</p>
<h4 id="Other-algorithms"><a href="#Other-algorithms" class="headerlink" title="Other algorithms"></a>Other algorithms</h4><p>[If you’re already fed up with algorithms, skip to the next part, what I’m going to say is not important for the rest of the article]</p>
<p>The problem of finding the best possible plan is an active research topic for many CS researchers. They often try to find better solutions for more precise problems/patterns. For example,</p>
<ul>
<li>if the query is a star join (it’s a certain type of multiple-join query), some databases will use a specific algorithm.</li>
<li>if the query is a parallel query, some databases will use a specific algorithm</li>
<li>…</li>
</ul>
<p>Other algorithms are also studied to replace dynamic programming for large queries. Greedy algorithms belong to larger family called <strong>heuristic algorithms</strong>. A greedy algorithm follows a rule (or heuristic), keeps the solution it found at the previous step and “appends” it to find the solution for the current step. Some algorithms follow a rule and apply it in a step-by-step way but don’t always keep the best solution found in the previous step. They are called heuristic algorithms.</p>
<p>For example, <strong>genetic algorithms</strong> follow a rule but the best solution of the last step is not often kept:</p>
<ul>
<li>A solution represents a possible full query plan</li>
<li>Instead of one solution (i.e. plan) there are P solutions (i.e. plans) kept at each step.</li>
<li>0) P query plans are randomly created</li>
<li>1) Only the plans with the best costs are kept</li>
<li>2) These best plans are mixed up to produce P news plans</li>
<li>3) Some of the P new plans are randomly modified</li>
<li>4) The step 1,2,3 are repeated T times</li>
<li>5) Then you keep the best plan from the P plans of the last loop.</li>
</ul>
<p>The more loops you do the better the plan will be.</p>
<p>Is it magic? No, it’s the laws of nature: only the fittest survives!</p>
<p>FYI, genetic algorithms are implemented in <a target="_blank" rel="noopener" href="http://www.postgresql.org/docs/9.4/static/geqo-intro.html">PostgreSQL</a> but I wasn’t able to find if they’re used by default.</p>
<p>There are other heuristic algorithms used in databases like Simulated Annealing, Iterative Improvement, Two-Phase Optimization… But I don’t know if they’re currently used in enterprise databases or if they’re only used in research databases.</p>
<p>For more information, you can read the following research article that presents more possible algorithms: <a target="_blank" rel="noopener" href="http://www.acad.bg/rismim/itc/sub/archiv/Paper6_1_2009.PDF">Review of Algorithms for the Join Ordering Problem in Database Query Optimization</a></p>
<h3 id="Real-optimizers"><a href="#Real-optimizers" class="headerlink" title="Real optimizers"></a>Real optimizers</h3><p>[You can skip to the next part, what I’m going to say is not important]</p>
<p>But, all this blabla is very theoretical. Since I’m a developer and not a researcher, I like <strong>concrete examples</strong>.</p>
<p>Let’s see how the <a target="_blank" rel="noopener" href="https://www.sqlite.org/optoverview.html">SQLite optimizer</a> works. It’s a light database so it uses a simple optimization based on a greedy algorithm with extra-rules to limit the number of possibilities:</p>
<ul>
<li>SQLite chooses to never reorder tables in a CROSS JOIN operator</li>
<li><strong>joins are implemented as nested joins</strong></li>
<li>outer joins are always evaluated in the order in which they occur</li>
<li>…</li>
<li>Prior to version 3.8.0, <strong>SQLite uses the “Nearest Neighbor” greedy algorithm when searching for the best query plan</strong></li>
</ul>
<p>Wait a minute … we’ve already seen this algorithm! What a coincidence!</p>
<ul>
<li>Since version 3.8.0 (released in 2015), SQLite uses the “<a target="_blank" rel="noopener" href="https://www.sqlite.org/queryplanner-ng.html">N Nearest Neighbors</a>” <strong>greedy algorithm</strong> when searching for the best query plan</li>
</ul>
<p>Let’s see how another optimizer does his job. IBM DB2 is like all the enterprise databases but I’ll focus on this one since it’s the last one I’ve really used before switching to Big Data.</p>
<p>If we look at the <a target="_blank" rel="noopener" href="https://www-01.ibm.com/support/knowledgecenter/SSEPGG_9.7.0/com.ibm.db2.luw.admin.perf.doc/doc/r0005278.html">official documentation</a>, we learn that the DB2 optimizer let you use 7 different levels of optimization:</p>
<ul>
<li>Use greedy algorithms for the joins<ul>
<li>0 – minimal optimization, use index scan and nested-loop join and avoid some Query Rewrite</li>
<li>1 – low optimization</li>
<li>2 – full optimization</li>
</ul>
</li>
<li>Use dynamic programming for the joins<ul>
<li>3 – moderate optimization and rough approximation</li>
<li>5 – full optimization, uses all techniques with heuristics</li>
<li>7 – full optimization similar to 5, without heuristics</li>
<li>9 – maximal optimization spare no effort/expense <strong>considers all possible join orders, including Cartesian products</strong></li>
</ul>
</li>
</ul>
<p>We can see that <strong>DB2 uses greedy algorithms and dynamic programming</strong>. Of course, they don’t share the heuristics they use since the query optimizer is the main power of a database.</p>
<p>FYI, <strong>the default level is 5.</strong> By default the optimizer uses the following characteristics:</p>
<ul>
<li><p><strong>All available statistics</strong>, including frequent-value and quantile statistics, are used.</p>
</li>
<li><p><strong>All query rewrite rules</strong> (including materialized query table routing) are applied, except computationally intensive rules that are applicable only in very rare cases.</p>
</li>
<li><p>Dynamic programming join enumeration</p>
</li>
</ul>
<p>  is used, with:</p>
<ul>
<li>Limited use of composite inner relation</li>
<li>Limited use of Cartesian products for star schemas involving lookup tables</li>
</ul>
<ul>
<li>A wide range of access methods is considered, including list prefetch (note: will see what is means), index ANDing (note: a special operation with indexes), and materialized query table routing.</li>
</ul>
<p>By default, <strong>DB2 uses dynamic programming limited by heuristics for the join ordering</strong>.</p>
<p>The others conditions (GROUP BY, DISTINCT…) are handled by simple rules.</p>
<h3 id="Query-Plan-Cache"><a href="#Query-Plan-Cache" class="headerlink" title="Query Plan Cache"></a>Query Plan Cache</h3><p>Since the creation of a plan takes time, most databases store the plan into a <strong>query plan cache</strong> to avoid useless re-computations of the same query plan. It’s kind of a big topic since the database needs to know when to update the outdated plans. The idea is to put a threshold and if the statistics of a table have changed above this threshold then the query plan involving this table is purged from the cache.</p>
<h2 id="Query-executor"><a href="#Query-executor" class="headerlink" title="Query executor"></a>Query executor</h2><p>At this stage we have an optimized execution plan. This plan is compiled to become an executable code. Then, if there are enough resources (memory, CPU) it is executed by the query executor. The operators in the plan (JOIN, SORT BY …) can be executed in a sequential or parallel way; it’s up to the executor. To get and write its data, the query executor interacts with the data manager, which is the next part of the article.</p>
<h1 id="Data-manager"><a href="#Data-manager" class="headerlink" title="Data manager"></a>Data manager</h1><p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/data_manager.png"><img src="../../public/images/data_manager-20200207203251432.png" alt="data manager in databases"></a></p>
<p>At this step, the query manager is executing the query and needs the data from the tables and indexes. It asks the data manager to get the data, but there are 2 problems:</p>
<ul>
<li>Relational databases use a transactional model. So, you can’t get any data at any time because someone else might be using/modifying the data at the same time.</li>
<li><strong>Data retrieval is the slowest operation in a database</strong>, therefore the data manager needs to be smart enough to get and keep data in memory buffers.</li>
</ul>
<p>In this part, we’ll see how relational databases handle these 2 problems. I won’t talk about the way the data manager gets its data because it’s not the most important (and this article is long enough!).</p>
<h2 id="Cache-manager"><a href="#Cache-manager" class="headerlink" title="Cache manager"></a>Cache manager</h2><p>As I already said, the main bottleneck of databases is disk I/O. To improve performance, modern databases use a cache manager.</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/cache_manager.png"><img src="../../public/images/cache_manager-20200207203251433.png" alt="cache manager in databases"></a></p>
<p>Instead of directly getting the data from the file system, the query executor asks for the data to the cache manager. The cache manager has an in-memory cache called <strong>buffer pool</strong>. <strong>Getting data from memory dramatically speeds up a database</strong>. It’s difficult to give an order of magnitude because it depends on the operation you need to do:</p>
<ul>
<li>sequential access (ex: full scan) vs random access (ex: access by row id),</li>
<li>read vs write</li>
</ul>
<p>and the type of disks used by the database:</p>
<ul>
<li>7.2k/10k/15k rpm HDD</li>
<li>SSD</li>
<li>RAID 1/5/…</li>
</ul>
<p>but I’d say <strong>memory is 100 to 100k times faster than disk</strong>.</p>
<p>But, this leads to another problem (as always with databases…). The cache manager needs to get the data in memory BEFORE the query executor uses them; otherwise the query manager has to wait for the data from the slow disks.</p>
<h3 id="Prefetching"><a href="#Prefetching" class="headerlink" title="Prefetching"></a>Prefetching</h3><p>This problem is called prefetching. A query executor knows the data it’ll need because it knows the full flow of the query and has knowledge of the data on disk with the statistics. Here is the idea:</p>
<ul>
<li>When the query executor is processing its first bunch of data</li>
<li>It asks the cache manager to pre-load the second bunch of data</li>
<li>When it starts processing the second bunch of data</li>
<li>It asks the CM to pre-load the third bunch and informs the CM that the first bunch can be purged from cache.</li>
<li>…</li>
</ul>
<p>The CM stores all these data in its buffer pool. In order to know if a data is still needed, the cache manager adds an extra-information about the cached data (called a <strong>latch</strong>).</p>
<p>Sometimes the query executor doesn’t know what data it’ll need and some databases don’t provide this functionality. Instead, they use a speculative prefetching (for example: if the query executor asked for data 1,3,5 it’ll likely ask for 7,9,11 in a near future) or a sequential prefetching (in this case the CM simply loads from disks the next contiguous data after the ones asked).</p>
<p>To monitor how well the prefetching is working, modern databases provide a metric called <strong>buffer/cache hit ratio</strong>. The hit ratio shows how often a requested data has been found in the buffer cache without requiring disk access.</p>
<p>Note: a poor cache hit ratio doesn’t always mean that the cache is ill-working. For more information, you can read the <a target="_blank" rel="noopener" href="http://docs.oracle.com/database/121/TGDBA/tune_buffer_cache.htm">Oracle documentation</a>.</p>
<p>But, a buffer is a <strong>limited</strong> amount of memory. Therefore, it needs to remove some data to be able to load new ones. Loading and purging the cache has a cost in terms of disk and network I/O. If you have a query that is often executed, it wouldn’t be efficient to always load then purge the data used by this query. To handle this problem, modern databases use a buffer replacement strategy.</p>
<h3 id="Buffer-Replacement-strategies"><a href="#Buffer-Replacement-strategies" class="headerlink" title="Buffer-Replacement strategies"></a>Buffer-Replacement strategies</h3><p>Most modern databases (at least SQL Server, MySQL, Oracle and DB2) use an LRU algorithm.</p>
<h4 id="LRU"><a href="#LRU" class="headerlink" title="LRU"></a>LRU</h4><p><strong>LRU</strong> stands for <strong>L</strong>east <strong>R</strong>ecently <strong>U</strong>sed. The idea behind this algorithm is to keep in the cache the data that have been recently used and, therefore, are more likely to be used again.</p>
<p>Here is a visual example:</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/LRU.png"><img src="../../public/images/LRU-20200207203251433.png" alt="LRU algorithm in a database"></a></p>
<p>For the sake of comprehension, I’ll assume that the data in the buffer are not locked by latches (and therefore can be removed). In this simple example the buffer can store 3 elements:</p>
<ul>
<li>1: the cache manager uses the data 1 and puts the data into the empty buffer</li>
<li>2: the CM uses the data 4 and puts the data into the half-loaded buffer</li>
<li>3: the CM uses the data 3 and puts the data into the half-loaded buffer</li>
<li>4: the CM uses the data 9. The buffer is full so <strong>data 1 is removed</strong> <strong>since it’s the last recently used data</strong>. Data 9 is added into the buffer</li>
<li>5: the CM uses the data 4. <strong>Data 4 is already in the buffer therefore it becomes the first recently used data again</strong>.</li>
<li>6: the CM uses the data 1. The buffer is full so <strong>data 9 is removed</strong> <strong>since it’s the last recently used data</strong>. Data 1 is added into the buffer</li>
<li>…</li>
</ul>
<p>This algorithm works well but there are some limitations. What if there is a full scan on a large table? In other words, what happens when the size of the table/index is above the size of the buffer? Using this algorithm will remove all the previous values in the cache whereas the data from the full scan are likely to be used only once.</p>
<h4 id="Improvements"><a href="#Improvements" class="headerlink" title="Improvements"></a>Improvements</h4><p>To prevent this to happen, some databases add specific rules. For example according to <a target="_blank" rel="noopener" href="http://docs.oracle.com/database/121/CNCPT/memory.htm#i10221">Oracle documentation</a>:</p>
<blockquote>
<p>“For very large tables, the database typically uses a direct path read, which loads blocks directly […], to avoid populating the buffer cache. For medium size tables, the database may use a direct read or a cache read. If it decides to use a cache read, then the database places the blocks at the end of the LRU list to prevent the scan from effectively cleaning out the buffer cache.”</p>
</blockquote>
<p>There are other possibilities like using an advanced version of LRU called LRU-K. For example SQL Server uses LRU-K for K =2.</p>
<p>This idea behind this algorithm is to take into account more history. With the simple LRU (which is also LRU-K for K=1), the algorithm only takes into account the last time the data was used. With the LRU-K:</p>
<ul>
<li>It takes into account the <strong>K last times the data was used</strong>.</li>
<li><strong>A weight is put</strong> on the number of times the data was used</li>
<li>If a bunch of new data is loaded into the cache, the old but often used data are not removed (because their weights are higher).</li>
<li>But the algorithm can’t keep old data in the cache if they aren’t used anymore.</li>
<li>So the <strong>weights decrease</strong> <strong>over time if the data is not used</strong>.</li>
</ul>
<p>The computation of the weight is costly and this is why SQL Server only uses K=2. This value performs well for an acceptable overhead.</p>
<p>For a more in-depth knowledge of LRU-K, you can read the original research paper (1993): <a target="_blank" rel="noopener" href="http://www.cs.cmu.edu/~christos/courses/721-resources/p297-o_neil.pdf">The LRU-K page replacement algorithm for database disk buffering</a>.</p>
<h4 id="Other-algorithms-1"><a href="#Other-algorithms-1" class="headerlink" title="Other algorithms"></a>Other algorithms</h4><p>Of course there are other algorithms to manage cache like</p>
<ul>
<li>2Q (a LRU-K like algorithm)</li>
<li>CLOCK (a LRU-K like algorithm)</li>
<li>MRU (most recently used, uses the same logic than LRU but with another rule)</li>
<li>LRFU (Least Recently and Frequently Used)</li>
<li>…</li>
</ul>
<p>Some databases let the possibility to use another algorithm than the default one.</p>
<h3 id="Write-buffer"><a href="#Write-buffer" class="headerlink" title="Write buffer"></a>Write buffer</h3><p>I only talked about read buffers that load data before using them. But in a database you also have write buffers that store data and flush them on disk by bunches instead of writing data one by one and producing many single disk accesses.</p>
<p>Keep in mind that buffers store <strong>pages</strong> (the smallest unit of data) and not rows (which is a logical/human way to see data). A page in a buffer pool is <strong>dirty</strong> if the page has been modified and not written on disk. There are multiple algorithms to decide the best time to write the dirty pages on disk but it’s highly linked to the notion of transaction, which is the next part of the article.</p>
<h2 id="Transaction-manager"><a href="#Transaction-manager" class="headerlink" title="Transaction manager"></a>Transaction manager</h2><p>Last but not least, this part is about the transaction manager. We’ll see how this process ensures that each query is executed in its own transaction. But before that, we need to understand the concept of ACID transactions.</p>
<h3 id="I’m-on-acid"><a href="#I’m-on-acid" class="headerlink" title="I’m on acid"></a>I’m on acid</h3><p>An ACID transaction is a <strong>unit of work</strong> that ensures 4 things:</p>
<ul>
<li><strong>Atomicity</strong>: the transaction is “all or nothing”, even if it lasts 10 hours. If the transaction crashes, the state goes back to before the transaction (the transaction is <strong>rolled back</strong>).</li>
<li><strong>Isolation</strong>: if 2 transactions A and B run at the same time, the result of transactions A and B must be the same whether A finishes before/after/during transaction B.</li>
<li><strong>Durability</strong>: once the transaction is <strong>committed</strong> (i.e. ends successfully), the data stay in the database no matter what happens (crash or error).</li>
<li><strong>Consistency</strong>: only valid data (in terms of relational constraints and functional constraints) are written to the database. The consistency is related to atomicity and isolation.</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/dollar_low.jpg"><img src="../../public/images/dollar_low-20200207203251437.jpg" alt="one dollar"></a></p>
<p>During the same transaction, you can run multiple SQL queries to read, create, update and delete data. The mess begins when two transactions are using the same data. The classic example is a money transfer from an account A to an account B. Imagine you have 2 transactions:</p>
<ul>
<li>Transaction 1 that takes 100$ from account A and gives them to account B</li>
<li>Transaction 2 that takes 50$ from account A and gives them to account B</li>
</ul>
<p>If we go back to the <strong>ACID</strong> properties:</p>
<ul>
<li><p><strong>Atomicity</strong> ensures that no matter what happens during T1 (a server crash, a network failure …), you can’t end up in a situation where the 100$ are withdrawn from A and not given to B (this case is an inconsistent state).</p>
</li>
<li><p>I<strong>solation</strong> ensures that if T1 and T2 happen at the same time, in the end A will be taken 150$ and B given 150$ and not, for example, A taken 150$ and B given just $50 because T2 has partially erased the actions of T1 (this case is also an inconsistent state).</p>
</li>
<li><p><strong>Durability</strong> ensures that T1 won’t disappear into thin air if the database crashes just after T1 is committed.</p>
</li>
<li><p><strong>Consistency</strong> ensures that no money is created or destroyed in the system.</p>
</li>
</ul>
<p>[You can skip to the next part if you want, what I’m going to say is not important for the rest of the article]</p>
<p>Many modern databases don’t use a pure isolation as a default behavior because it comes with a huge performance overhead. The SQL norm defines 4 levels of isolation:</p>
<ul>
<li><p><strong>Serializable</strong> (default behaviour in SQLite): The highest level of isolation. Two transactions happening at the same time are 100% isolated. Each transaction has its own “world”.</p>
</li>
<li><p><strong>Repeatable read</strong> (default behavior in MySQL): Each transaction has its own “world” except in one situation. If a transaction ends up successfully and adds new data, these data will be visible in the other and still running transactions. But if A modifies a data and ends up successfully, the modification won’t be visible in the still running transactions. So, this break of isolation between transactions is only about new data, not the existing ones.</p>
</li>
</ul>
<p>For example, if a transaction A does a “SELECT count(1) from TABLE_X” and then a new data is added and committed in TABLE_X by Transaction B, if transaction A does again a count(1) the value won’t be the same.</p>
<p>This is called a <strong>phantom read</strong>.</p>
<ul>
<li><strong>Read committed</strong> (default behavior in Oracle, PostgreSQL and SQL Server): It’s a repeatable read + a new break of isolation. If a transaction A reads a data D and then this data is modified (or deleted) and committed by a transaction B, if A reads data D again it will see the modification (or deletion) made by B on the data.</li>
</ul>
<p>This is called a <strong>non-repeatable read</strong>.</p>
<ul>
<li><strong>Read uncommitted</strong>: the lowest level of isolation. It’s a read committed + a new break of isolation. If a transaction A reads a data D and then this data D is modified by a transaction B (that is not committed and still running), if A reads data D again it will see the modified value. If transaction B is rolled back, then data D read by A the second time doesn’t make no sense since it has been modified by a transaction B that never happened (since it was rolled back).</li>
</ul>
<p>This is called a <strong>dirty read</strong>.</p>
<p>Most databases add their own custom levels of isolation (like the snapshot isolation used by PostgreSQL, Oracle and SQL Server). Moreover, most databases don’t implement all the levels of the SQL norm (especially the read uncommitted level).</p>
<p>The default level of isolation can be overridden by the user/developer at the beginning of the connection (it’s a very simple line of code to add).</p>
<h3 id="Concurrency-Control"><a href="#Concurrency-Control" class="headerlink" title="Concurrency Control"></a>Concurrency Control</h3><p>The real issue to ensure isolation, coherency and atomicity is the <strong>write operations on the same data</strong> (add, update and delete):</p>
<ul>
<li>if all transactions are only reading data, they can work at the same time without modifying the behavior of another transaction.</li>
<li>if (at least) one of the transactions is modifying a data read by other transactions, the database needs to find a way to hide this modification from the other transactions. Moreover, it also needs to ensure that this modification won’t be erased by another transaction that didn’t see the modified data.</li>
</ul>
<p>This problem is a called <strong>concurrency control</strong>.</p>
<p>The easiest way to solve this problem is to run each transaction one by one (i.e. sequentially). But that’s not scalable at all and only one core is working on the multi-processor/core server, not very efficient…</p>
<p>The ideal way to solve this problem is, every time a transaction is created or cancelled:</p>
<ul>
<li>to monitor all the operations of all the transactions</li>
<li>to check if the parts of 2 (or more) transactions are in conflict because they’re reading/modifying the same data.</li>
<li>to reorder the operations inside the conflicting transactions to reduce the size of the conflicting parts</li>
<li>to execute the conflicting parts in a certain order (while the non-conflicting transactions are still running concurrently).</li>
<li>to take into account that a transaction can be cancelled.</li>
</ul>
<p>More formally it’s a scheduling problem with conflicting schedules. More concretely, it’s a very difficult and CPU-expensive optimization problem. Enterprise databases can’t afford to wait hours to find the best schedule for each new transaction event. Therefore, they use less ideal approaches that lead to more time wasted between conflicting transactions.</p>
<h3 id="Lock-manager"><a href="#Lock-manager" class="headerlink" title="Lock manager"></a>Lock manager</h3><p>To handle this problem, most databases are using <strong>locks</strong> and/or <strong>data versioning</strong>. Since it’s a big topic, I’ll focus on the locking part then I’ll speak a little bit about data versioning.</p>
<h4 id="Pessimistic-locking"><a href="#Pessimistic-locking" class="headerlink" title="Pessimistic locking"></a>Pessimistic locking</h4><p>The idea behind locking is:</p>
<ul>
<li>if a transaction needs a data,</li>
<li>it locks the data</li>
<li>if another transaction also needs this data,</li>
<li>it’ll have to wait until the first transaction releases the data.</li>
</ul>
<p>This is called an <strong>exclusive lock</strong>.</p>
<p>But using an exclusive lock for a transaction that only needs to read a data is very expensive since <strong>it forces other transactions that only want to read the same data to wait</strong>. This is why there is another type of lock, the <strong>shared lock</strong>.</p>
<p>With the shared lock:</p>
<ul>
<li>if a transaction needs only to read a data A,</li>
<li>it “shared locks” the data and reads the data</li>
<li>if a second transaction also needs only to read data A,</li>
<li>it “shared locks” the data and reads the data</li>
<li>if a third transaction needs to modify data A,</li>
<li>it “exclusive locks” the data but it has to wait until the 2 other transactions release their shared locks to apply its exclusive lock on data A.</li>
</ul>
<p>Still, if a data as an exclusive lock, a transaction that just needs to read the data will have to wait the end of the exclusive lock to put a shared lock on the data.</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/lock_manager.png"><img src="../../public/images/lock_manager-20200207203251437.png" alt="lock manager in a database"></a></p>
<p>The lock manager is the process that gives and releases locks. Internally, it stores the locks in a hash table (where the key is the data to lock) and knows for each data:</p>
<ul>
<li>which transactions are locking the data</li>
<li>which transactions are waiting for the data</li>
</ul>
<h4 id="Deadlock"><a href="#Deadlock" class="headerlink" title="Deadlock"></a>Deadlock</h4><p>But the use of locks can lead to a situation where 2 transactions are waiting forever for a data:</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/deadlock.png"><img src="../../public/images/deadlock-20200207203251451.png" alt="deadlock with database transactions"></a></p>
<p>In this figure:</p>
<ul>
<li>transaction A has an exclusive lock on data1 and is waiting to get data2</li>
<li>transaction B has an exclusive lock on data2 and is waiting to get data1</li>
</ul>
<p>This is called a <strong>deadlock</strong>.</p>
<p>During a deadlock, the lock manager chooses which transaction to cancel (rollback) in order to remove the deadlock. This decision is not easy:</p>
<ul>
<li>Is it better to kill the transaction that modified the least amount of data (and therefore that will produce the least expensive rollback)?</li>
<li>Is it better to kill the least aged transaction because the user of the other transaction has waited longer?</li>
<li>Is it better to kill the transaction that will take less time to finish (and avoid a possible starvation)?</li>
<li>In case of rollback, how many transactions will be impacted by this rollback?</li>
</ul>
<p>But before making this choice, it needs to check if there are deadlocks.</p>
<p>The hash table can be seen as a graph (like in the previous figures). There is a deadlock if there is a cycle in the graph. Since it’s expensive to check for cycles (because the graph with all the locks is quite big), a simpler approach is often used: using a <strong>timeout</strong>. If a lock is not given within this timeout, the transaction enters a deadlock state.</p>
<p>The lock manager can also check before giving a lock if this lock will create a deadlock. But again it’s computationally expensive to do it perfectly. Therefore, these pre-checks are often a set of basic rules.</p>
<h4 id="Two-phase-locking"><a href="#Two-phase-locking" class="headerlink" title="Two-phase locking"></a>Two-phase locking</h4><p>The <strong>simplest way</strong> to ensure a pure isolation is if a lock is acquired at the beginning of the transaction and released at the end of the transaction. This means that a transaction has to wait for all its locks before it starts and the locks held by a transaction are released when the transaction ends. It works but it <strong>produces a lot of time wasted</strong> to wait for all locks.</p>
<p>A faster way is the <strong>Two-Phase Locking Protocol</strong> (used by DB2 and SQL Server) where a transaction is divided into 2 phases:</p>
<ul>
<li>the <strong>growing phase</strong> where a transaction can obtain locks, but can’t release any lock.</li>
<li>the <strong>shrinking phase</strong> where a transaction can release locks (on the data it has already processed and won’t process again), but can’t obtain new locks.</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/two-phase-locking.png"><img src="../../public/images/two-phase-locking-20200207203251473.png" alt="a problem avoided with two phase locking"></a></p>
<p>The idea behind these 2 simple rules is:</p>
<ul>
<li>to release the locks that aren’t used anymore to reduce the wait time of other transactions waiting for these locks</li>
<li>to prevent from cases where a transaction gets data modified after the transaction started and therefore aren’t coherent with the first data the transaction acquired.</li>
</ul>
<p>This protocol works well except if a transaction that modified a data and released the associated lock is cancelled (rolled back). You could end up in a case where another transaction reads the modified value whereas this value is going to be rolled back. To avoid this problem, <strong>all the exclusive locks must be released at the end of the transaction</strong>.</p>
<h4 id="A-few-words"><a href="#A-few-words" class="headerlink" title="A few words"></a>A few words</h4><p>Of course a real database uses a more sophisticated system involving more types of locks (like intention locks) and more granularities (locks on a row, on a page, on a partition, on a table, on a tablespace) but the idea remains the same.</p>
<p>I only presented the pure lock-based approach. <strong>Data versioning is another way to deal with this problem</strong>.</p>
<p>The idea behind versioning is that:</p>
<ul>
<li>every transaction can modify the same data at the same time</li>
<li>each transaction has its own copy (or version) of the data</li>
<li>if 2 transactions modify the same data, only one modification will be accepted, the other will be refused and the associated transaction will be rolled back (and maybe re-run).</li>
</ul>
<p>It increases the performance since:</p>
<ul>
<li><strong>reader transactions don’t block writer transactions</strong></li>
<li><strong>writer transactions don’t block reader transactions</strong></li>
<li>there is no overhead from the “fat and slow” lock manager</li>
</ul>
<p>Everything is better than locks except when 2 transactions write the same data. Moreover, you can quickly end up with a huge disk space overhead.</p>
<p>Data versioning and locking are two different visions: <strong>optimistic locking vs pessimistic locking</strong>. They both have pros and cons; it really depends on the use case (more reads vs more writes). For a presentation on data versioning, I recommend <a target="_blank" rel="noopener" href="http://momjian.us/main/writings/pgsql/mvcc.pdf">this very good presentation</a> on how PostgreSQL implements multiversion concurrency control.</p>
<p>Some databases like DB2 (until DB2 9.7) and SQL Server (except for snapshot isolation) are only using locks. Other like PostgreSQL, MySQL and Oracle use a mixed approach involving locks and data versioning. I’m not aware of a database using only data versioning (if you know a database based on a pure data versioning, feel free to tell me).</p>
<p>[UPDATE 08/20/2015] I was told by a reader that:</p>
<blockquote>
<p>Firebird and Interbase use versioning without record locking.<br>Versioning has an interesting effect on indexes: sometimes a unique index contains duplicates, the index can have more entries than the table has rows, etc.</p>
</blockquote>
<p>If you read the part on the different levels of isolation, when you increase the isolation level you increase the number of locks and therefore the time wasted by transactions to wait for their locks. This is why most databases don’t use the highest isolation level (Serializable) by default.</p>
<p>As always, you can check by yourself in the documentation of the main databases (for example <a target="_blank" rel="noopener" href="http://dev.mysql.com/doc/refman/5.7/en/innodb-transaction-model.html">MySQL</a>, <a target="_blank" rel="noopener" href="http://www.postgresql.org/docs/9.4/static/mvcc.html">PostgreSQL</a> or <a target="_blank" rel="noopener" href="http://docs.oracle.com/cd/B28359_01/server.111/b28318/consist.htm#i5337">Oracle</a>).</p>
<h3 id="Log-manager"><a href="#Log-manager" class="headerlink" title="Log manager"></a>Log manager</h3><p>We’ve already seen that to increase its performances, a database stores data in memory buffers. But if the server crashes when the transaction is being committed, you’ll lose the data still in memory during the crash, which breaks the Durability of a transaction.</p>
<p>You can write everything on disk but if the server crashes, you’ll end up with the data half written on disk, which breaks the Atomicity of a transaction.</p>
<p><strong>Any modification written by a transaction must be undone or finished</strong>.</p>
<p>To deal with this problem, there are 2 ways:</p>
<ul>
<li><strong>Shadow copies/pages</strong>: Each transaction creates its own copy of the database (or just a part of the database) and works on this copy. In case of error, the copy is removed. In case of success, the database switches instantly the data from the copy with a filesystem trick then it removes the “old” data.</li>
<li><strong>Transaction log</strong>: A transaction log is a storage space. Before each write on disk, the database writes an info on the transaction log so that in case of crash/cancel of a transaction, the database knows how to remove (or finish) the unfinished transaction.</li>
</ul>
<h4 id="WAL"><a href="#WAL" class="headerlink" title="WAL"></a>WAL</h4><p>The shadow copies/pages creates a huge disk overhead when used on large databases involving many transactions. That’s why modern databases use a <strong>transaction log</strong>. The transaction log must be stored on a <strong>stable storage</strong>. I won’t go deeper on storage technologies but using (at least) RAID disks is mandatory to prevent from a disk failure.</p>
<p>Most databases (at least Oracle, <a target="_blank" rel="noopener" href="https://technet.microsoft.com/en-us/library/ms186259(v=sql.105).aspx">SQL Server</a>, <a target="_blank" rel="noopener" href="http://www.ibm.com/developerworks/data/library/techarticle/0301kline/0301kline.html">DB2</a>, <a target="_blank" rel="noopener" href="http://www.postgresql.org/docs/9.4/static/wal.html">PostgreSQL</a>, MySQL and <a target="_blank" rel="noopener" href="https://www.sqlite.org/wal.html">SQLite</a>) deal with the transaction log using the <strong>Write-Ahead Logging protocol</strong> (WAL). The WAL protocol is a set of 3 rules:</p>
<ul>
<li>1) Each modification into the database produces a log record, and <strong>the log record must be written into the transaction log before the data is written on disk</strong>.</li>
<li>2) The log records must be written in order; a log record A that happens before a log record B must but written before B</li>
<li>3) When a transaction is committed, the commit order must be written on the transaction log before the transaction ends up successfully.</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/log_manager.png"><img src="../../public/images/log_manager-20200207203251465.png" alt="log manager in a database"></a></p>
<p>This job is done by a log manager. An easy way to see it is that between the cache manager and the data access manager (that writes data on disk) the log manager writes every update/delete/create/commit/rollback on the transaction log before they’re written on disk. Easy, right?</p>
<p>WRONG ANSWER! After all we’ve been through, you should know that everything related to a database is cursed by the “database effect”. More seriously, the problem is to find a way to write logs while keeping good performances. If the writes on the transaction log are too slow they will slow down everything.</p>
<h4 id="ARIES"><a href="#ARIES" class="headerlink" title="ARIES"></a>ARIES</h4><p>In 1992, IBM researchers “invented” an enhanced version of WAL called ARIES. ARIES is more or less used by most modern databases. The logic might not be the same but the concepts behind ARIES are used everywhere. I put the quotes on invented because, according to this <a target="_blank" rel="noopener" href="http://db.csail.mit.edu/6.830/lectures/lec15-notes.pdf">MIT course</a>, the IBM researchers did “nothing more than writing the good practices of transaction recovery”. Since I was 5 when the ARIES paper was published, I don’t care about this old gossip from bitter researchers. In fact, I only put this info to give you a break before we start this last technical part. I’ve read a huge part of the <a target="_blank" rel="noopener" href="http://www.cs.berkeley.edu/~brewer/cs262/Aries.pdf">research paper on ARIES</a> and I find it very interesting! In this part I’ll only give you an overview of ARIES but I strongly recommend to read the paper if you want a real knowledge.</p>
<p>ARIES stands for <strong>A</strong>lgorithms for <strong>R</strong>ecovery and <strong>I</strong>solation <strong>E</strong>xploiting <strong>S</strong>emantics.</p>
<p>The aim of this technique is double:</p>
<ul>
<li>1) Having <strong>good performances when writing logs</strong></li>
<li>2) Having a fast and <strong>reliable recovery</strong></li>
</ul>
<p>There are multiple reasons a database has to rollback a transaction:</p>
<ul>
<li>Because the user cancelled it</li>
<li>Because of server or network failures</li>
<li>Because the transaction has broken the integrity of the database (for example you have a UNIQUE constraint on a column and the transaction adds a duplicate)</li>
<li>Because of deadlocks</li>
</ul>
<p>Sometimes (for example, in case of network failure), the database can recover the transaction.</p>
<p>How is that possible? To answer this question, we need to understand the information stored in a log record.</p>
<h5 id="The-logs"><a href="#The-logs" class="headerlink" title="The logs"></a>The logs</h5><p>Each <strong>operation (add/remove/modify) during a transaction produces a log</strong>. This log record is composed of:</p>
<ul>
<li><strong>LSN:</strong> A unique <strong>L</strong>og <strong>S</strong>equence <strong>N</strong>umber. This LSN is given in a chronological order*. This means that if an operation A happened before an operation B the LSN of log A will be lower than the LSN of log B.</li>
<li><strong>TransID:</strong> the id of the transaction that produced the operation.</li>
<li><strong>PageID:</strong> the location on disk of the modified data. The minimum amount of data on disk is a page so the location of the data is the location of the page that contains the data.</li>
<li><strong>PrevLSN:</strong> A link to the previous log record produced by the same transaction.</li>
<li><strong>UNDO:</strong> a way to remove the effect of the operation</li>
</ul>
<p>For example, if the operation is an update, the UNDO will store either the value/state of the updated element before the update (physical UNDO) or the reverse operation to go back at the previous state (logical UNDO)**.</p>
<ul>
<li><strong>REDO</strong>: a way replay the operation</li>
</ul>
<p>Likewise, there are 2 ways to do that. Either you store the value/state of the element after the operation or the operation itself to replay it.</p>
<ul>
<li>…: (FYI, an ARIES log has 2 others fields: the UndoNxtLSN and the Type).</li>
</ul>
<p>Moreover, each page on disk (that stores the data, not the log) has id of the log record (LSN) of the last operation that modified the data.</p>
<p>*The way the LSN is given is more complicated because it is linked to the way the logs are stored. But the idea remains the same.</p>
<p>**ARIES uses only logical UNDO because it’s a real mess to deal with physical UNDO.</p>
<p>Note: From my little knowledge, only PostgreSQL is not using an UNDO. It uses instead a garbage collector daemon that removes the old versions of data. This is linked to the implementation of the data versioning in PostgreSQL.</p>
<p>To give you a better idea, here is a visual and simplified example of the log records produced by the query “UPDATE FROM PERSON SET AGE = 18;”. Let’s say this query is executed in transaction 18.</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/ARIES_logs.png"><img src="../../public/images/ARIES_logs-20200207203251473.png" alt="simplified logs of ARIES protocole"></a></p>
<p>Each log has a unique LSN. The logs that are linked belong to the same transaction. The logs are linked in a chronological order (the last log of the linked list is the log of the last operation).</p>
<h5 id="Log-Buffer"><a href="#Log-Buffer" class="headerlink" title="Log Buffer"></a>Log Buffer</h5><p>To avoid that log writing becomes a major bottleneck, a <strong>log buffer</strong> is used.</p>
<p><a target="_blank" rel="noopener" href="http://coding-geek.com/wp-content/uploads/2015/08/ARIES_log_writing.png"><img src="../../public/images/ARIES_log_writing-20200207203251473.png" alt="log writing process in databases"></a></p>
<p>When the query executor asks for a modification:</p>
<ul>
<li>1) The cache manager stores the modification in its buffer.</li>
<li>2) The log manager stores the associated log in its buffer.</li>
<li>3) At this step, the query executor considers the operation is done (and therefore can ask for other modifications)</li>
<li>4) Then (later) the log manager writes the log on the transaction log. The decision when to write the log is done by an algorithm.</li>
<li>5) Then (later) the cache manager writes the modification on disk. The decision when to write data on disk is done by an algorithm.</li>
</ul>
<p><strong>When a transaction is committed, it means that for every operation in the transaction the steps 1, 2, 3,4,5 are done</strong>. Writing in the transaction log is fast since it’s just “adding a log somewhere in the transaction log” whereas writing data on disk is more complicated because it’s “writing the data in a way that it’s fast to read them”.</p>
<h5 id="STEAL-and-FORCE-policies"><a href="#STEAL-and-FORCE-policies" class="headerlink" title="STEAL and FORCE policies"></a>STEAL and FORCE policies</h5><p>For performance reasons the <strong>step 5 might be done after the commit</strong> because in case of crashes it’s still possible to recover the transaction with the REDO logs. This is called a <strong>NO-FORCE policy</strong>.</p>
<p>A database can choose a FORCE policy (i.e. step 5 must be done before the commit) to lower the workload during the recovery.</p>
<p>Another issue is to choose whether <strong>the data are written step-by-step on disk (STEAL policy)</strong> or if the buffer manager needs to wait until the commit order to write everything at once (NO-STEAL). The choice between STEAL and NO-STEAL depends on what you want: fast writing with a long recovery using UNDO logs or fast recovery?</p>
<p>Here is a summary of the impact of these policies on recovery:</p>
<ul>
<li><strong>STEAL/NO-FORCE</strong> <strong>needs UNDO and REDO</strong>: <strong>highest performances</strong> but gives more complex logs and recovery processes (like ARIES). <strong>This is the choice made by most databases</strong>. Note: I read this fact on multiple research papers and courses but I couldn’t find it (explicitly) on the official documentations.</li>
<li>STEAL/ FORCE needs only UNDO.</li>
<li>NO-STEAL/NO-FORCE needs only REDO.</li>
<li>NO-STEAL/FORCE needs nothing: <strong>worst performances</strong> and a huge amount of ram is needed.</li>
</ul>
<h5 id="The-recovery-part"><a href="#The-recovery-part" class="headerlink" title="The recovery part"></a>The recovery part</h5><p>Ok, so we have nice logs, let’s use them!</p>
<p>Let’s say the new intern has crashed the database (rule n°1: it’s always the intern’s fault). You restart the database and the recovery process begins.</p>
<p>ARIES recovers from a crash in three passes:</p>
<ul>
<li><strong>1) The Analysis pass</strong>: The recovery process reads the full transaction log* to recreate the timeline of what was happening during the crash. It determines which transactions to rollback (all the transactions without a commit order are rolled back) and which data needed to be written on disk at the time of the crash.</li>
<li><strong>2) The Redo pass</strong>: This pass starts from a log record determined during analysis, and uses the REDO to update the database to the state it was before the crash.</li>
</ul>
<p>During the redo phase, the REDO logs are processed in a chronological order (using the LSN).</p>
<p>For each log, the recovery process reads the LSN of the page on disk containing the data to modify.</p>
<p>If LSN(page_on_disk)&gt;=LSN(log_record), it means that the data has already been written on disk before the crash (but the value was overwritten by an operation that happened after the log and before the crash) so nothing is done.</p>
<p>If LSN(page_on_disk)&lt;LSN(log_record) then the page on disk is updated.</p>
<p>The redo is done even for the transactions that are going to be rolled back because it simplifies the recovery process (but I’m sure modern databases don’t do that).</p>
<ul>
<li><strong>3) The Undo pass</strong>: This pass rolls back all transactions that were incomplete at the time of the crash. The rollback starts with the last logs of each transaction and processes the UNDO logs in an anti-chronological order (using the PrevLSN of the log records).</li>
</ul>
<p>During the recovery, the transaction log must be warned of the actions made by the recovery process so that the data written on disk are synchronized with what’s written in the transaction log. A solution could be to remove the log records of the transactions that are being undone but that’s very difficult. Instead, ARIES writes compensation logs in the transaction log that delete logically the log records of the transactions being removed.</p>
<p>When a transaction is cancelled “manually” or by the lock manager (to stop a deadlock) or just because of a network failure, then the analysis pass is not needed. Indeed, the information about what to REDO and UNDO is available in 2 in-memory tables:</p>
<ul>
<li>a <strong>transaction table</strong> (stores the state of all current transactions)</li>
<li>a <strong>dirty page table</strong> (stores which data need to be written on disk).</li>
</ul>
<p>These tables are updated by the cache manager and the transaction manager for each new transaction event. Since they are in-memory, they are destroyed when the database crashes.</p>
<p>The job of the analysis phase is to recreate both tables after a crash using the information in the transaction log. <em>To speed up the analysis pass, ARIES provides the notion of *</em>checkpoint**. The idea is to write on disk from time to time the content of the transaction table and the dirty page table and the last LSN at the time of this write so that during the analysis pass, only the logs after this LSN are analyzed.</p>
<h1 id="To-conclude"><a href="#To-conclude" class="headerlink" title="To conclude"></a>To conclude</h1><p>Before writing this article, I knew how big the subject was and I knew it would take time to write an in-depth article about it. It turned out that I was very optimistic and I spent twice more time than expected, but I learned a lot.</p>
<p>If you want a good overview about databases, I recommend reading the research paper “<a target="_blank" rel="noopener" href="http://db.cs.berkeley.edu/papers/fntdb07-architecture.pdf">Architecture of a Database System</a> “. This is a good introduction on databases (110 pages) and for once it’s readable by non-CS guys. This paper helped me a lot to find a plan for this article and it’s not focused on data structures and algorithms like my article but more on the architecture concepts.</p>
<p>If you read this article carefully you should now understand how powerful a database is. Since it was a very long article, let me remind you about what we’ve seen:</p>
<ul>
<li>an overview of the B+Tree indexes</li>
<li>a global overview of a database</li>
<li>an overview of the cost based optimization with a strong focus on join operators</li>
<li>an overview of the buffer pool management</li>
<li>an overview of the transaction management</li>
</ul>
<p>But a database contains even more cleverness. For example, I didn’t speak about some touchy problems like:</p>
<ul>
<li>how to manage clustered databases and global transactions</li>
<li>how to take a snapshot when the database is still running</li>
<li>how to efficiently store (and compress) data</li>
<li>how to manage memory</li>
</ul>
<p>So, think twice when you have to choose between a buggy NoSQL database and a rock-solid relational database. Don’t get me wrong, some NoSQL databases are great. But they’re still young and answering specific problems that concern a few applications.</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Database/" rel="tag">Database</a></li></ul>

    </footer>
  </div>

   
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/19/">prev page</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="page-number" href="/page/19/">19</a><span class="page-number current">20</span><a class="page-number" href="/page/21/">21</a><a class="page-number" href="/page/22/">22</a><span class="space">&hellip;</span><a class="page-number" href="/page/25/">25</a><a class="extend next" rel="next" href="/page/21/">next page</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2020-2021
        <i class="ri-heart-fill heart_icon"></i> Aaron
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></s>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Blog"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">🏡</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">🏛</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">📚</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">🏷</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <script>
      if (window.matchMedia("(max-width: 768px)").matches) {
        document.querySelector('.content').classList.remove('on');
        document.querySelector('.sidebar').classList.remove('on');
      }
    </script>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css">
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script>

<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->


<script src="/js/busuanzi-2.3.pure.min.js"></script>


<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->


<script src="/js/clickBoom2.js"></script>


<!-- CodeCopy -->


<link rel="stylesheet" href="/css/clipboard.css">

<script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>


<!-- CanvasBackground -->


    
  </div>
</body>

</html>